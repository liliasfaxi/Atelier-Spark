{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atelier Apache Spark Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International . Objectif L'objectif de ce cours est d'initier les apprenants aux concepts de base de Apache Spark, et de faire le tours des composants qui le constituent et des cas d'utilisation. Ce cours est renforc\u00e9 par des travaux pratiques. Table des Mati\u00e8res Le cours est divis\u00e9 comme suit: Introduction aux Big Data Pr\u00e9sentation de Apache Spark Installation de Spark Spark Batch Spark SQL Spark Streaming Spark ML-Lib Spark GraphX","title":"Atelier Apache Spark"},{"location":"#atelier-apache-spark","text":"Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Atelier Apache Spark"},{"location":"#objectif","text":"L'objectif de ce cours est d'initier les apprenants aux concepts de base de Apache Spark, et de faire le tours des composants qui le constituent et des cas d'utilisation. Ce cours est renforc\u00e9 par des travaux pratiques.","title":"Objectif"},{"location":"#table-des-matieres","text":"Le cours est divis\u00e9 comme suit: Introduction aux Big Data Pr\u00e9sentation de Apache Spark Installation de Spark Spark Batch Spark SQL Spark Streaming Spark ML-Lib Spark GraphX","title":"Table des Mati\u00e8res"},{"location":"p1-big-data/","text":"Partie 1 - Introduction au Big Data Les \"Big Data\", Pourquoi? L'\u00eatre humain, \u00e0 travers l'humanit\u00e9, a toujours cherch\u00e9 trois choses : Savoir (qu'est-ce qui s'est pass\u00e9?), Comprendre (pourquoi cela s'est-il pass\u00e9?) et Pr\u00e9dire (qu'est-ce que qui se passera?). Plusieurs cultures ont clam\u00e9 l'omniscience en ayant recours \u00e0 des subterfuges, tels que les oracles, l'astrologie, le tarot, ou les boules de cristal. Cela dit, ces moyens ne sont gu\u00e8res satisfaisants \u00e0 l'esprit m\u00e9ticuleux du scientifique, qui cherche toujours une explication logique et rationnelle \u00e0 tout \u00e9v\u00e8nement, et une justification convainquante \u00e0 tout comportement. Le scientifique se base sur des faits. Il veut arriver \u00e0 faire de la magie gr\u00e2ce \u00e0 la technologie. Pour arriver \u00e0 ces fins, le scientifique a besoin de donn\u00e9es. L'int\u00e9r\u00eat de collecter des donn\u00e9es et de les exploiter a longtemps \u00e9t\u00e9 n\u00e9glig\u00e9, et a \u00e9t\u00e9 limit\u00e9 au peu de donn\u00e9es, jug\u00e9es \"utiles\", qui semblaient suffisantes pour atteindre un objectif imm\u00e9diat. Cependant, adopter le chemin \u00e9vident et peu risqu\u00e9 n'aurait jamais permis de r\u00e9aliser les miracles auxquelles on s'attendait. Il fallait trouver un autre moyen.. Le terme Big Data est apparu peu de temps apr\u00e8s l'apparition du terme Web 2.0, qui montre la transition de l'internet d'une \u00e8re o\u00f9 l'ajout des donn\u00e9es \u00e9tait exclusivement r\u00e9serv\u00e9 \u00e0 une \u00e9lite experte, o\u00f9 le volume des donn\u00e9es disponible \u00e9tait petit mais o\u00f9 les donn\u00e9es \u00e9taient pr\u00e9cieuses et pertinentes, vers une \u00e8re o\u00f9 tout un chacun \u00e9tait capable d'introduire des connaissances, v\u00e9ridiques ou pas, qui seraient sauvegard\u00e9es dans une m\u00e9moire collective jusqu'\u00e0 la fin des temps. Ce changement de paradigme a entrain\u00e9 le besoin d'infrastructures nouvelles, qui seraient capables, non seulement de stocker ces donn\u00e9es, mais \u00e9galement d'en extraire de la valeur. Ces infrastructures auront la capacit\u00e9 de g\u00e9rer toute la cha\u00eene logistique des donn\u00e9es, de la collecte vers l'affichage. Cela semble \u00e9vident, me direz-vous, car les syst\u00e8mes classiques sont capables de faire cela. Qui stocke mieux les donn\u00e9es qu'une bonne vieille base de donn\u00e9es relationnelle? Le probl\u00e8me est que les donn\u00e9es dites \"Big Data\" sont caract\u00e9ris\u00e9es par des propri\u00e9t\u00e9s telles que, les syst\u00e8mes classiques de stockage et de traitement auraient du mal \u00e0 les exploiter \u00e0 leur juste valeur. Caract\u00e9ristiques des Donn\u00e9es Massives Le terme \"donn\u00e9es massives\", ou \"Big Data\", ne donne \u00e0 mon avis pas enti\u00e8rement justice aux donn\u00e9es de notre domaine. En effet, il repr\u00e9sente une seule caract\u00e9ristique parmis plusieurs, le Volume, qui, m\u00eame si elle semble \u00eatre la plus importante, est loin d'\u00eatre la plus critique. En effet, les donn\u00e9es massives sont caract\u00e9ris\u00e9es par les fameux *V . Il en existe plusieurs (10 jusqu'\u00e0 ce jour si je ne m'abuse, certains en citent m\u00eame 42!!!), mais pourraient \u00e0 mon avis \u00eatre r\u00e9sum\u00e9s en trois caract\u00e9ristiques primordiales, autours de la combinaison desquelles tournent toutes les d\u00e9cisions prises dans ce domaine. Volume C'est \u00e9videmment le V le plus manifeste, qui caract\u00e9rise le fait que les donn\u00e9es ont un volume \u00e9norme qui peut atteindre des valeurs de l'ordre de Exa-, Zetta- ou Yottaoctet (allant jusqu'\u00e0 2^{80} 2^{80} octets!). Mais ceci n'est pas tout. Un volume \u00e9norme, s'il reste constant, est g\u00e9rable : il suffit de trouver une machine suffisamment puissante pour le sauvegarder. Le probl\u00e8me avec la propri\u00e9t\u00e9 du volume, c'est qu'il augmente de fa\u00e7on continue, ce qui rend sa gestion beaucoup plus ardue. Une citation bien connue, et qui se re-confirme chaque ann\u00e9e, atteste que \"Over the last two years alone 90 percent of the data in the world was generated.\" Il est donc primordial de trouver un moyen de g\u00e9rer ce volume toujours croissant des donn\u00e9es. V\u00e9locit\u00e9 Cette propri\u00e9t\u00e9 est, \u00e0 mon avis, la plus probl\u00e9matique des trois, car, coupl\u00e9e avec le volume, elle rend les syst\u00e8me actuels obsol\u00e8tes. En effet, la v\u00e9locit\u00e9 est, litt\u00e9ralement, \"La vitesse avec laquelle quelque chose se d\u00e9place dans une direction particuli\u00e8re\". Dans notre cas, la v\u00e9locit\u00e9 des donn\u00e9es est la responsable directe du volume croissant des donn\u00e9es dans le syst\u00e8me. Elle est provoqu\u00e9e par une arriv\u00e9e des donn\u00e9es dans le syst\u00e8me sous la forme d'un flux constant qui demande \u00e0 \u00eatre stock\u00e9 et trait\u00e9 imm\u00e9diatement, ainsi que le besoin croissant des utilisateurs d'avoir une repr\u00e9sentation r\u00e9cente et fid\u00e8le de l'\u00e9tat des donn\u00e9es. D'ailleurs, cette propri\u00e9t\u00e9 a engendr\u00e9 une autre pr\u00e9occupation des analystes des donn\u00e9es, qui est de fournir une introspection en temps r\u00e9el sur les donn\u00e9es, les qualifiant ainsi de \" Fast Data \". Vari\u00e9t\u00e9 Ce qui distingue vraiment les donn\u00e9es massives des donn\u00e9es g\u00e9r\u00e9es classiquement dans des bases de donn\u00e9es op\u00e9rationnelles, c'est le support des donn\u00e9es semi- et non structur\u00e9es. En effet, les donn\u00e9es non structur\u00e9es sont des donn\u00e9es qu'on stocke dans un format qui n'est pas d\u00e9fini \u00e0 la cr\u00e9ation, telles que les donn\u00e9es textuelles, images ou sons. Les donn\u00e9es semi-structur\u00e9es sont des donn\u00e9es qui contiennent une structure, mais une structure qui n'est pas rigide, et dont on ne d\u00e9finit pas les contraintes \u00e0 l'insertion de la donn\u00e9e, contrairement aux donn\u00e9es structur\u00e9es (se trouvant typiquement dans des bases de donn\u00e9es relationnelles) qui, si elles ne respectent pas la structure d\u00e9finie, sont consid\u00e9r\u00e9es fausses et ne sont pas autoris\u00e9es \u00e0 \u00eatre enregistr\u00e9es. On estime que seules 15% des donn\u00e9es dans une entreprise sont des donn\u00e9es structur\u00e9es, contre 85% qui ne le sont pas! Dans une optique centr\u00e9e sur les donn\u00e9es, dont le but est de gagner le maximum de vision \u00e0 partir des donn\u00e9es, perdre autant de sources d'information est un vrai probl\u00e8me. Il est donc important que les syst\u00e8mes Big Data sachent interpr\u00e9ter ces donn\u00e9es et en extraire le maximum de valeur. Toutes les d\u00e9cisions, choix et propri\u00e9t\u00e9s prises au niveau des architectures et infrastructures Big Data sont r\u00e9gies par ces trois caract\u00e9ristiques, ce qui va compl\u00e8tement changer la vision \"relationnelle\" que tout informaticien qui se respecte a acquis tout au long de ses ann\u00e9es d'\u00e9tude et de travail. Cela dit, ce ne sont pas les seules propri\u00e9t\u00e9s. D'autres V ont vu le jour, mais sans jamais avoir autant d'impact sur l'infrastructre, plut\u00f4t dans la fa\u00e7on de d\u00e9finir les processus, la gouvernance et les approches m\u00e9tier \u00e0 adopter. Nous citons par exemple : V\u00e9racit\u00e9 : c'est la confiance que nous devons avoir en nos donn\u00e9es. Cette propri\u00e9t\u00e9 est inversement proportionnelle au volume et \u00e0 la vari\u00e9t\u00e9 : plus nos donn\u00e9es sont fiables, moins elles sont diversifi\u00e9es et volumineuses! Valeur : c'est la capacit\u00e9 d'extraire de la valeur m\u00e9tier \u00e0 partir des donn\u00e9es. Variabilit\u00e9 : une extension de la vari\u00e9t\u00e9, qui indique \u00e0 quel point nos donn\u00e9es peuvent avoir des dimensions diff\u00e9rentes \u00e0 partir des sources de donn\u00e9es disparates. Visualisation : c'est la capacit\u00e9 de nos donn\u00e9es \u00e0 \u00eatre repr\u00e9sent\u00e9es par les outils de visualisation classiques. etc. Infrastructure Big Data : Besoins Les caract\u00e9ristiques des donn\u00e9es Big Data cit\u00e9es ci-dessus, entra\u00eenent des besoins particuliers en termes d'infrastructure et d'architecture. Volume La caract\u00e9ristique de volume, qui implique que la taille des donn\u00e9es augmente de fa\u00e7on r\u00e9guli\u00e8re, fait qu'on ne peut plus se contenter d'un syst\u00e8me centralis\u00e9 classique. Car dans un syst\u00e8me centralis\u00e9 (donc bas\u00e9 sur une seule machine), augmenter les ressources de stockage au besoin implique ce que nous appelons une scalabilit\u00e9 verticale ou un scale up , qui veut dire une augmentation des capacit\u00e9s du serveur de stockage en rajoutant des processeurs, de la RAM ou des disques. Cependant, cette solution, bien qu'elle soit intuitive, rapide et ne requiert pas de changement architecturaux cons\u00e9quents, implique en g\u00e9n\u00e9ral un temps d'arr\u00eat pendant l'installation, ainsi qu'une d\u00e9pense assez cons\u00e9quente pour faire l'acquisition d'un serveur puissant. De plus, une machine unique atteindra rapidement une limite mat\u00e9rielle, car il vous est impossible d'augmenter ses ressources ind\u00e9finiment. En contrepartie, il est possible de penser que, face \u00e0 un volume de donn\u00e9es toujours en augmentation, il serait plus judicieux de rajouter des machines au besoin, cr\u00e9ant ainsi un cluster de machines interconnect\u00e9es, ou syst\u00e8me r\u00e9parti , dont la taille et la capacit\u00e9 sont virtuellement illimit\u00e9es. Nous sommes donc face \u00e0 un autre type de scalabilit\u00e9 : la scalabilit\u00e9 horizontale ou le scale out . Donc Volume => Scalabilit\u00e9 Horizontale V\u00e9locit\u00e9 La v\u00e9locit\u00e9 est une propri\u00e9t\u00e9 qui, coupl\u00e9e au volume, rend la gestion de l'infrastructure un vrai cauchemar. En effet, g\u00e9rer des donn\u00e9es en continuelle arriv\u00e9e implique qu'il y'a un risque \u00e9norme de perte de donn\u00e9es, si elles ne sont pas manipul\u00e9es \u00e0 temps. C'est pour cette raison qu'un syst\u00e8me Big Data se doit d'\u00eatre continuellement disponible : toute requ\u00eate de lecture ou d'\u00e9criture doit \u00eatre trait\u00e9e en un temps raisonnable, et le syst\u00e8me doit \u00eatre continuellement alerte pour saisir toutes les donn\u00e9es, sans risquer de les perdre. Ainsi V\u00e9locit\u00e9 => Disponibilit\u00e9 Vari\u00e9t\u00e9 La vari\u00e9t\u00e9 de donn\u00e9es implique non seulement que nous sommes en pr\u00e9sence de donn\u00e9es structur\u00e9es, semi-structur\u00e9es et non structur\u00e9es, mais \u00e9galement que ces donn\u00e9es peuvent parvenir de sources diff\u00e9rentes, avec des formats diff\u00e9rents, et que m\u00eame \u00e0 partir d'une m\u00eame source, ce format peut changer d'un moment \u00e0 un autre. Dans les syst\u00e8mes classiques, tout ce qui est variable doit passer par une couche d'homog\u00e9n\u00e9isation qui transformera chaque entr\u00e9e ou enregistrement dans la forme souhait\u00e9e, en remplissant par des valeurs NULL les donn\u00e9es manquantes. Rajouter cette couche d'homog\u00e9n\u00e9isation aura un double impact n\u00e9gatif sur notre syst\u00e8me : (1) \u00e0 cause de la v\u00e9locit\u00e9, cette op\u00e9ration risquera de ralentir la collecte et saisie des donn\u00e9es entrantes, et (2) on pourra subir une perte de donn\u00e9es suite \u00e0 ces transformations. C'est pour ces raisons qu'un syst\u00e8me Big Data se doit de supporter des types de donn\u00e9es changeants, sans pour autant requ\u00e9rir \u00e0 des subterfuges qui alourdissent ou contournent le syst\u00e8me de stockage. D'o\u00f9 Vari\u00e9t\u00e9 => Flexibilit\u00e9 Th\u00e9or\u00e8me CAP Les besoins de scalabilit\u00e9, disponibilit\u00e9 et flexibilit\u00e9, obligatoires pour avoir un syst\u00e8me Big Data en bonne et due forme, se trouvent confront\u00e9s \u00e0 une contrainte de taille... et qu'en est-il de la coh\u00e9rence (commun\u00e9ment appel\u00e9e aussi consistence, par anglicisme)? La coh\u00e9rence repr\u00e9sente en effet un must pour les syst\u00e8mes relationnels classiques, et une base sur laquelle sont prises toutes les d\u00e9cisions conceptuelles et techniques. Elle repr\u00e9sente le fait que les donn\u00e9es stables doivent respecter toutes les contraintes d'int\u00e9grit\u00e9 d\u00e9finies \u00e0 la cr\u00e9ation de la base de donn\u00e9e. Par exemple, si un champ est d\u00e9cr\u00e9t\u00e9 \"Not Null\", il doit le rester quelque soit la situation, et \u00e0 aucun moment une requ\u00eate ne doit surprendre ce champs avec une valeur nulle, m\u00eame si c'est juste une valeur interm\u00e9diaire. La coh\u00e9rence est un principe tr\u00e8s rigide dans les bases de donn\u00e9es relationnelles, et repr\u00e9sente le crit\u00e8re de base pour la gestion des transactions : le C de ACID . Cela dit, dans les syst\u00e8mes Big Data, nous nous trouvons confront\u00e9s \u00e0 un probl\u00e8me de taille : nous devons \u00eatre en pr\u00e9sence d'une infrastructure r\u00e9partie et hautement disponible. Or, il existe un th\u00e9or\u00e8me appel\u00e9 CAP pour Consistency / Availability / Partition tolerance , qui stipule que ces trois propri\u00e9t\u00e9s (notamment la coh\u00e9rence, la disponibilit\u00e9 et la tol\u00e9rance au partitionnement), ne peuvent jamais avoir lieu en m\u00eame temps. Seules deux d'entre elles peuvent \u00eatre respect\u00e9es \u00e0 la fois. Essayons d'expliquer pourquoi. Un syst\u00e8me r\u00e9parti est dit coh\u00e9rent si tous ses noeuds voient les m\u00eames donn\u00e9es en m\u00eame temps. C'est \u00e0 dire que, si nous r\u00e9alisons une op\u00e9ration de lecture sur un syst\u00e8me consistant, il devrait toujours retourner la valeur la plus r\u00e9cente qui ait \u00e9t\u00e9 \u00e9crite, quel que soit l'endroit \u00e0 partir duquel la lecture est effectu\u00e9e. Ainsi, si une donn\u00e9e est modifi\u00e9e sur un noeud particulier, pour conserver la coh\u00e9rence demand\u00e9e, aucune op\u00e9ration de lecture ne doit \u00eatre permise avant d'avoir mis \u00e0 jour toutes les r\u00e9pliques (copies) de cette donn\u00e9es. Or, les diff\u00e9rents noeuds d'un cluster sont en g\u00e9n\u00e9ral distants, parfois m\u00eame g\u00e9ographiquement, il est donc n\u00e9cessaire d'attendre que la propagation de la modification se fasse sur le r\u00e9seau, pour effectuer n'importe quelle op\u00e9ration, m\u00eame une lecture. Ceci va rendre nos donn\u00e9es indisponibles \u00e0 la lecture pendant tout le temps que durera l'op\u00e9ration de synchronisation, qui est un temps incertain puisque... r\u00e9seau. Assurer donc une coh\u00e9rence forte dans un syst\u00e8me distribu\u00e9 est en contradiction avec le besoin de disponibilit\u00e9 du syst\u00e8me et des donn\u00e9es. D'ailleurs, c'est ce que font les bases de donn\u00e9es relationnelles r\u00e9parties, qui conservent les propri\u00e9t\u00e9s ACID tout en distribuant les donn\u00e9es, mais qui souffrent d'un manque notoire de performance. Les syst\u00e8mes Big Data, subissant les contraintes des V pr\u00e9c\u00e9demment cit\u00e9s, doivent donc faire un choix. Or ce choix est loin d'\u00eatre facile : qui voudra acheter un syst\u00e8me qui pr\u00f4ne haut et fort qu'il est incoh\u00e9rent ? L'id\u00e9e serait donc de partir sur le principe de coh\u00e9rence \u00e9ventuelle ou parfois de coh\u00e9rence ajustable . Ainsi, un syst\u00e8me Big Data est un syst\u00e8me principalement disponible, fondamentalement r\u00e9parti, et qui assure une coh\u00e9rence \u00e9ventuelle au bout d'un temps g\u00e9n\u00e9ralement n\u00e9gligeable, avec la possibilit\u00e9 de configurer les niveau de coh\u00e9rence parfois m\u00eame dynamiquement. Les experts les appellent donc les syst\u00e8mes BASE (admirez le jeux de mot.. ACID, BASE \ud83d\ude0e): B asically A vailable S oft-state E ventual consistency La propri\u00e9t\u00e9 de Soft State ou d'\u00e9tat \"mou\" veut dire que l'\u00e9tat du syst\u00e8me peut changer dans le temps, m\u00eame sans qu'il y ait une nouvelle entr\u00e9e, \u00e0 cause du principe de coh\u00e9rence \u00e9ventuelle expliqu\u00e9 pr\u00e9c\u00e9demment. Maintenant que vous \u00eates plus familiaris\u00e9s avec les caract\u00e9ristiques d'un syst\u00e8me Big Data, listons quelques principes, appel\u00e9s ici MOTTOS , qui vont r\u00e9gir nos futures d\u00e9cisions dans ce domaine. Principes de base du Domaine des Big Data Il est important, avant d'entamer n'importe quel travail sur les syst\u00e8mes Big Data, de consid\u00e9rer certains principes, qui sont parfois en enti\u00e8re contradiction avec les principes classiques de d\u00e9veloppement d'application. Ce n'est pas si \u00e9tonnant : le domaine des Big Data n'est pas cens\u00e9 prendre la place des domaines relationnel et d\u00e9cisionnel, mais plut\u00f4t les enrichir et les agr\u00e9menter. MOTTO 1 : Stocker d'abord, r\u00e9fl\u00e9chir ensuite \u00c0 cause de la v\u00e9locit\u00e9, il est important de consid\u00e9rer qu'il nous sera parfois difficile, voire impossible, de nettoyer les donn\u00e9es ou de faire un traitement quelconque dessus, avant de les stocker. Cela risque dans bien des cas de nous faire perdre des donn\u00e9es, le cauchemar de tout scientifique des donn\u00e9es! Nous devons donc envisager la possibilit\u00e9 de d\u00e9finir des syst\u00e8mes de stockage qui contiennent des donn\u00e9es non nettoy\u00e9es, en vrac (appel\u00e9es raw data ), pour ensuite lancer des traitements dessus.. l'horreur pour un gestionnaire de bases des donn\u00e9es! \ud83d\ude31 Bien entendu, ces \"bases\" ne sont pas con\u00e7ues pour \u00eatre directement exploit\u00e9es par des applications externes, mais plut\u00f4t pour conserver le plus longtemps possibles les donn\u00e9es brutes, sans perte, qui pourraient eventuellement \u00eatre r\u00e9utilis\u00e9es pour d'autres fins. MOTTO 2 : Absolument TOUTES les donn\u00e9es sont importantes! D'o\u00f9 l'int\u00e9r\u00eat du MOTTO 1 . Il nous est parfois difficile, au tout d\u00e9but de la conception des syst\u00e8mes Big Data, de cerner toutes les possibilit\u00e9s offertes par ces syst\u00e8mes et par les donn\u00e9es que nous avons \u00e0 notre disposition. Nous sommes donc en g\u00e9n\u00e9ral tent\u00e9s de supprimer les donn\u00e9es dont nous n'avons pas besoin une fois extraite l'information imm\u00e9diatement utile. Cela dit, gr\u00e2ce \u00e0 l'accessibilit\u00e9 des syst\u00e8mes de stockage magn\u00e9tiques et leur prix de plus en plus bas, nous consid\u00e9rons qu'il est largement plus b\u00e9n\u00e9fique de stocker des donn\u00e9es qu'on n'utilisera peut-\u00eatre jamais, plut\u00f4t que de gagner de la place et perdre un potentiel pouvoir concurrentiel. MOTTO 3 : Ce sont les donn\u00e9es qui pilotent le traitement Dans un syst\u00e8me op\u00e9rationnel classique, ainsi que dans la plupart des syst\u00e8mes d\u00e9cisionnels, ce sont les besoins m\u00e9tier qui pr\u00e9valoient : le responsable m\u00e9tier commence par d\u00e9finir les besoins (ou les KPIs : Key Performance Indicators dans le cas d'un syst\u00e8me d\u00e9cisionnel), puis le responsable technique con\u00e7oit les structures de donn\u00e9es pour r\u00e9pondre \u00e0 ces besoins. Par essence, un syst\u00e8me Big Data fonctionne diff\u00e9remment : les donn\u00e9es sont collect\u00e9es tout d'abord \u00e0 partir de toutes les sources possibles; des traitements de fouille et d'exploration de ces donn\u00e9es sont lanc\u00e9s ensuite, pour extraire de la valeur \u00e0 partir de ces donn\u00e9es. L'objectif est toujours le m\u00eame : chercher l'effet WOW! D'o\u00f9 l'int\u00e9r\u00eat de ce MOTTO : d\u00e9finir le traitement \u00e0 r\u00e9aliser d\u00e9pend des donn\u00e9es que nous avons r\u00e9ussi \u00e0 collecter, et pas le contraire. Cela implique donc l'utilisation d'autres types de syst\u00e8mes de traitement et d'algorithmes d'analyse. MOTTO 4 : Co-localisation des donn\u00e9es et du traitement Un syst\u00e8me classique \u00e0 plusieurs couches, tel que le syst\u00e8me trois tiers par exemple, se base sur le principe de s\u00e9paration des donn\u00e9es et du traitement. On trouve en g\u00e9n\u00e9ral des donn\u00e9es sur un serveur de bases de donn\u00e9es s\u00e9par\u00e9, et les traitement complexes sur un serveur d'application qui se charge de l'aggr\u00e9gation et de l'affichage de ces donn\u00e9es. Ceci est agr\u00e9ment\u00e9 d'un langage de requ\u00eatage d\u00e9claratif (typiquement SQL) pour r\u00e9aliser des op\u00e9rations de filtrage, parfois assez lourdes et complexes, au niveau de la base de donn\u00e9es. Cela dit, dans un contexte Big Data, le volume des donn\u00e9es peut s'av\u00e9rer assez cons\u00e9quent, trop m\u00eame pour envisager de le d\u00e9placer \u00e0 chaque fois vers un autre syst\u00e8me pour en extraire une vraie valeur. De plus, compter sur un langage comme SQL pour diminuer le volume ou faire de simples agr\u00e9gations au niveau de la base de donn\u00e9es pourra la rendre indisponible pendant un moment (car n'oublions pas que nous parlons d'un syst\u00e8me r\u00e9parti), ce qui va \u00e0 l'encontre du principe de v\u00e9locit\u00e9, qui exige une disponibilit\u00e9 \u00e0 toute \u00e9preuve du syst\u00e8me de stockage. C'est pour cette raison que, pour r\u00e9aliser les traitements voulus en un temps raisonnable et sans avoir \u00e0 trimballer les donn\u00e9es sur le r\u00e9seau, il est question dans les syst\u00e8mes Big Data de d\u00e9placer le traitement vers les donn\u00e9es massives, au lieu de d\u00e9placer les donn\u00e9es vers le traitement. MOTTO 5 : La redondance, c'est bien Dans les bases de donn\u00e9es relationnelles, le plus grand ennemi \u00e0 combattre dans la conception de la structure de donn\u00e9es est la redondance, et ce pour deux raisons. La premi\u00e8re, \u00e9vidente, est le gain d'espace : notre espace de stockage est pr\u00e9cieux, et nous devons \u00e9viter de le gaspiller sans raison pr\u00e9cise. La deuxi\u00e8me est un besoin de coh\u00e9rence : si nous dupliquons une m\u00eame information \u00e0 plusieurs endroits dans la base, nous devrons par la suite faire attention, parfois par des m\u00e9canismes compliqu\u00e9s et co\u00fbteux, \u00e0 ce que cette information soit mise \u00e0 jour instantan\u00e9ment sur la totalit\u00e9 de ses copies. Ce besoin d'\u00e9viter la redondance a cr\u00e9\u00e9 la n\u00e9cessit\u00e9 d'utiliser plusieurs techniques, telles que les jointures et clefs \u00e9trang\u00e8res, et entra\u00eene parfois la cr\u00e9ation d'un tr\u00e8s grand nombre de tables. Ceci rajoute une complexit\u00e9 pour le requ\u00eatage, et une lourdeur d'ex\u00e9cution des t\u00e2ches sur la base. Un syst\u00e8me Big Data qui, non seulement est caract\u00e9ris\u00e9 par un gros volume de donn\u00e9es, mais \u00e9galement une grande v\u00e9locit\u00e9, et qui doit donc \u00eatre imm\u00e9diatement disponible, ne peut pas se permettre de gaspiller ses ressources en requ\u00eates inutiles. On tol\u00e8re donc \u00e0 un certain point les risques dus \u00e0 la redondance, pour gagner en disponibilit\u00e9, primordiale dans ce type de syst\u00e8mes. D'autre part, un syst\u00e8me Big Data est un syst\u00e8me r\u00e9parti par excellence, et dans un syst\u00e8me r\u00e9parti, il est primordial d'assurer une bonne tol\u00e9rance aux fautes en cr\u00e9ant des r\u00e9pliques des donn\u00e9es, diss\u00e9min\u00e9es partout sur le cluster. Ces r\u00e9pliques assurent qu'aucune machine n'est compl\u00e8tement indispensable, et diminue le risque d'indisponibilit\u00e9 des donn\u00e9es. Un autre signe de redondance. MOTTO 6 : Vive le Polyglottisme! \u00catre polyglotte, c'est \u00eatre capable de parler plusieurs langues. Et les syst\u00e8mes Big Data encouragent le polyglottisme. En effet, ce sont des syst\u00e8mes complexes qui impliquent en g\u00e9n\u00e9ral plusieurs traitements et plusieurs types de donn\u00e9es diff\u00e9rentes (donn\u00e9es brutes, donn\u00e9es nettoy\u00e9es, donn\u00e9es trait\u00e9es), ce qui fait qu'il existe deux principes importants \u00e0 encourager : Polyglot Programming : Une application peut comporter plusieurs langages et paradigmes de programmation, chacun assurant un besoin particulier, de fa\u00e7on \u00e0 profiter des avantages de chacun \u00e0 sa juste valeur. Polyglot Persistence : Dans une m\u00eame application, il est possible d'utiliser plusieurs syst\u00e8mes de stockage diff\u00e9rents (relationnels, NOSQL, syst\u00e8mes de fichiers, etc.). Gr\u00e2ce \u00e0 ces deux principes, on pourra cr\u00e9er des applications complexes mais compl\u00e8tes, qui permettent d'assurer tous les besoins en terme de stockage et de traitement. Technologies et Paradigmes Les op\u00e9rations \u00e0 r\u00e9aliser sur les syst\u00e8mes Big Data consistent principalement en : Ingestion des donn\u00e9es : repr\u00e9sente les phases de collecte et d'importation des donn\u00e9es pour \u00eatre stock\u00e9es ou trait\u00e9es \u00e0 la vol\u00e9e. Cela peut se faire en \"temps r\u00e9el\", c'est \u00e0 dire que les donn\u00e9es sont import\u00e9es au moment o\u00f9 elles sont \u00e9mises par leur source, ou bien \"par lots\", ce qui veut dire que les donn\u00e9es sont import\u00e9es par portions \u00e0 intervalles r\u00e9gulier. Exemples de technologies Apache Kafka , Amazon Kinesis , Apache Flume , Sqoop , etc. Stockage des donn\u00e9es : Les syst\u00e8mes de stockage de donn\u00e9es respectant les propri\u00e9t\u00e9s le Big Data se distinguent principalement en syst\u00e8mes de fichiers distribu\u00e9s, tel que Hadoop HDFS ou Google GFS , ou bases de donn\u00e9es NOSQL, tel que MongoDB , Cassandra , Redis ou Neo4J . Traitement des donn\u00e9es : Plusieurs types de traitement de donn\u00e9es sont possibles, nous citons : Traitement par lot (Batch Processing) : c'est le traitement des donn\u00e9es au repos (data at rest) qui se fait sur l'ensemble des donn\u00e9es stock\u00e9es, sans avoir besoin d'une interaction avec l'utilisateur. Le traitement par lot est adapt\u00e9 principalement aux op\u00e9rations ayant lieu \u00e0 la fin d'un cycle, permettant d'avoir une vision globale sur la totalit\u00e9 des donn\u00e9es, par exemple pour avoir un rapport global ou une analyse mensuelle. Les op\u00e9rations de traitement par lots sont en g\u00e9n\u00e9ral lanc\u00e9es \u00e0 des p\u00e9riodes r\u00e9guli\u00e8res, car elles sont connues pour avoir une grande latence (temps total de traitement). Exemples de technologies Hadoop Map Reduce et Spark Batch . Traitement en Streaming (Stream Processing) : c'est le traitement des donn\u00e9es en transit (data in motion) , ou en d'autres termes, le traitement des donn\u00e9es pendant qu'elles sont produites ou re\u00e7ues. Les donn\u00e9es \u00e9tant en g\u00e9n\u00e9ral cr\u00e9\u00e9es en tant que flux continu (\u00e9v\u00e8nements de capteurs, activit\u00e9 des utilisateurs sur un site web, flux vid\u00e9o, etc.), elles sont captur\u00e9es comme une s\u00e9rie d'\u00e9v\u00e8nements continus dans le temps. Avant la cr\u00e9ation des traitements en streaming, ces donn\u00e9es \u00e9taient stock\u00e9es dans une base de donn\u00e9es, un syst\u00e8me de fichier ou tout autre forme de stockage en masse. Les applications appelleront ensuite les donn\u00e9es au besoin. Gr\u00e2ce \u00e0 ce nouveau paradigme, les donn\u00e9es peuvent maintenant \u00eatre trait\u00e9es \u00e0 la vol\u00e9e, ce qui permet \u00e0 la couche applicative d'\u00eatre toujours sur \u00e9coute et \u00e0 jour. Exemples de technologies Apache Flink et Apache Storm . Traitement par Micro-Lot (Micro-Batch Processing) : c'est la pratique de collecter les donn\u00e9es en petits groupes (appel\u00e9s des micro-lots ou des micro-batchs ) pour les traiter. Contrairement au traditionnel traitement par lot, cette variante fait en sorte que le traitement des donn\u00e9es soit plus fr\u00e9quent, et que les r\u00e9sultats soient produits avec une latence beaucoup plus petite. Les donn\u00e9es sont collect\u00e9es par intervalles selon un seuil pr\u00e9d\u00e9fini, limit\u00e9 par un temps (par exemple toutes les secondes), ou par un nombre (tous les 20 \u00e9l\u00e9ments). Ce traitement est en g\u00e9n\u00e9ral une alternative au traitement en streaming, o\u00f9 les donn\u00e9es sont trait\u00e9es \u00e0 la vol\u00e9e, mais risquent d'\u00eatre perdues si le temps de traitement est sup\u00e9rieur \u00e0 la fr\u00e9quence de g\u00e9n\u00e9ration des donn\u00e9es. Le micro-batching permet, par contraste, de sauvegarder les donn\u00e9es dans un buffer, ralentissant ainsi le flux g\u00e9n\u00e9r\u00e9. D'autre part, les donn\u00e9es \u00e9tant trait\u00e9es par micro-lots, il est possible d'avoir une visibilit\u00e9 sur ce petit lot de donn\u00e9es, contrairement au traitement en streaming qui n'a de visibilit\u00e9 que sur la derni\u00e8re donn\u00e9e g\u00e9n\u00e9r\u00e9e, \u00e0 moins de proc\u00e9der \u00e0 des m\u00e9canismes parfois co\u00fbteux. En contrepartie, le traitement en micro-batch donne des r\u00e9sultats moins r\u00e9cents que le \"vrai\" streaming, et s'ex\u00e9cute sous forme de bursts r\u00e9guliers, qui peuvent parfois \u00eatre g\u00eanants pour le syst\u00e8me sous-jacent. Exemples de technologies Spark Streaming et Logstash . Traitement Interactif (Interactive Processing) : Dans les syst\u00e8mes Big Data, la notion de transaction n'est plus exactement la m\u00eame que pour les syst\u00e8mes classiques: finies les sacro-saintes propri\u00e9t\u00e9s ACID dont le premier objectif est d'avoir des donn\u00e9es correctes et coh\u00e9rentes, et bonjour les propri\u00e9t\u00e9s BASE, qui favorisent un acc\u00e8s moins rigide aux donn\u00e9es. On parle donc rarement de traitement transactionnel en Big Data, mais de traitements plut\u00f4t interactifs : une requ\u00eate est envoy\u00e9e par le client, trait\u00e9e imm\u00e9diatement par le syst\u00e8me qui renverra un r\u00e9sultat dans un temps raisonnable. On parle alors d' interaction entre l'utilisateur et le syst\u00e8me. Les traitements en batch et en streaming ne sont pas cens\u00e9s communiquer avec un utilisateur de l'autre c\u00f4t\u00e9. En g\u00e9n\u00e9ral, les r\u00e9sultats de ces traitements sont enregistr\u00e9s dans un syst\u00e8me de stockage, qui sera, lui, par la suite interrog\u00e9 par l'utilisateur. Le traitement interactif est donc le r\u00e9sultat d'une requ\u00eate de l'utilisateur, faite en g\u00e9n\u00e9ral sur une base de donn\u00e9es (relationnelle ou NOSQL). Exemples de technologies Apache Drill , Cloudera Impala ou Apache Zeppelin .","title":"P1 - Introduction au Big Data"},{"location":"p1-big-data/#partie-1-introduction-au-big-data","text":"","title":"Partie 1 - Introduction au Big Data"},{"location":"p1-big-data/#les-big-data-pourquoi","text":"L'\u00eatre humain, \u00e0 travers l'humanit\u00e9, a toujours cherch\u00e9 trois choses : Savoir (qu'est-ce qui s'est pass\u00e9?), Comprendre (pourquoi cela s'est-il pass\u00e9?) et Pr\u00e9dire (qu'est-ce que qui se passera?). Plusieurs cultures ont clam\u00e9 l'omniscience en ayant recours \u00e0 des subterfuges, tels que les oracles, l'astrologie, le tarot, ou les boules de cristal. Cela dit, ces moyens ne sont gu\u00e8res satisfaisants \u00e0 l'esprit m\u00e9ticuleux du scientifique, qui cherche toujours une explication logique et rationnelle \u00e0 tout \u00e9v\u00e8nement, et une justification convainquante \u00e0 tout comportement. Le scientifique se base sur des faits. Il veut arriver \u00e0 faire de la magie gr\u00e2ce \u00e0 la technologie. Pour arriver \u00e0 ces fins, le scientifique a besoin de donn\u00e9es. L'int\u00e9r\u00eat de collecter des donn\u00e9es et de les exploiter a longtemps \u00e9t\u00e9 n\u00e9glig\u00e9, et a \u00e9t\u00e9 limit\u00e9 au peu de donn\u00e9es, jug\u00e9es \"utiles\", qui semblaient suffisantes pour atteindre un objectif imm\u00e9diat. Cependant, adopter le chemin \u00e9vident et peu risqu\u00e9 n'aurait jamais permis de r\u00e9aliser les miracles auxquelles on s'attendait. Il fallait trouver un autre moyen.. Le terme Big Data est apparu peu de temps apr\u00e8s l'apparition du terme Web 2.0, qui montre la transition de l'internet d'une \u00e8re o\u00f9 l'ajout des donn\u00e9es \u00e9tait exclusivement r\u00e9serv\u00e9 \u00e0 une \u00e9lite experte, o\u00f9 le volume des donn\u00e9es disponible \u00e9tait petit mais o\u00f9 les donn\u00e9es \u00e9taient pr\u00e9cieuses et pertinentes, vers une \u00e8re o\u00f9 tout un chacun \u00e9tait capable d'introduire des connaissances, v\u00e9ridiques ou pas, qui seraient sauvegard\u00e9es dans une m\u00e9moire collective jusqu'\u00e0 la fin des temps. Ce changement de paradigme a entrain\u00e9 le besoin d'infrastructures nouvelles, qui seraient capables, non seulement de stocker ces donn\u00e9es, mais \u00e9galement d'en extraire de la valeur. Ces infrastructures auront la capacit\u00e9 de g\u00e9rer toute la cha\u00eene logistique des donn\u00e9es, de la collecte vers l'affichage. Cela semble \u00e9vident, me direz-vous, car les syst\u00e8mes classiques sont capables de faire cela. Qui stocke mieux les donn\u00e9es qu'une bonne vieille base de donn\u00e9es relationnelle? Le probl\u00e8me est que les donn\u00e9es dites \"Big Data\" sont caract\u00e9ris\u00e9es par des propri\u00e9t\u00e9s telles que, les syst\u00e8mes classiques de stockage et de traitement auraient du mal \u00e0 les exploiter \u00e0 leur juste valeur.","title":"Les \"Big Data\", Pourquoi?"},{"location":"p1-big-data/#caracteristiques-des-donnees-massives","text":"Le terme \"donn\u00e9es massives\", ou \"Big Data\", ne donne \u00e0 mon avis pas enti\u00e8rement justice aux donn\u00e9es de notre domaine. En effet, il repr\u00e9sente une seule caract\u00e9ristique parmis plusieurs, le Volume, qui, m\u00eame si elle semble \u00eatre la plus importante, est loin d'\u00eatre la plus critique. En effet, les donn\u00e9es massives sont caract\u00e9ris\u00e9es par les fameux *V . Il en existe plusieurs (10 jusqu'\u00e0 ce jour si je ne m'abuse, certains en citent m\u00eame 42!!!), mais pourraient \u00e0 mon avis \u00eatre r\u00e9sum\u00e9s en trois caract\u00e9ristiques primordiales, autours de la combinaison desquelles tournent toutes les d\u00e9cisions prises dans ce domaine. Volume C'est \u00e9videmment le V le plus manifeste, qui caract\u00e9rise le fait que les donn\u00e9es ont un volume \u00e9norme qui peut atteindre des valeurs de l'ordre de Exa-, Zetta- ou Yottaoctet (allant jusqu'\u00e0 2^{80} 2^{80} octets!). Mais ceci n'est pas tout. Un volume \u00e9norme, s'il reste constant, est g\u00e9rable : il suffit de trouver une machine suffisamment puissante pour le sauvegarder. Le probl\u00e8me avec la propri\u00e9t\u00e9 du volume, c'est qu'il augmente de fa\u00e7on continue, ce qui rend sa gestion beaucoup plus ardue. Une citation bien connue, et qui se re-confirme chaque ann\u00e9e, atteste que \"Over the last two years alone 90 percent of the data in the world was generated.\" Il est donc primordial de trouver un moyen de g\u00e9rer ce volume toujours croissant des donn\u00e9es. V\u00e9locit\u00e9 Cette propri\u00e9t\u00e9 est, \u00e0 mon avis, la plus probl\u00e9matique des trois, car, coupl\u00e9e avec le volume, elle rend les syst\u00e8me actuels obsol\u00e8tes. En effet, la v\u00e9locit\u00e9 est, litt\u00e9ralement, \"La vitesse avec laquelle quelque chose se d\u00e9place dans une direction particuli\u00e8re\". Dans notre cas, la v\u00e9locit\u00e9 des donn\u00e9es est la responsable directe du volume croissant des donn\u00e9es dans le syst\u00e8me. Elle est provoqu\u00e9e par une arriv\u00e9e des donn\u00e9es dans le syst\u00e8me sous la forme d'un flux constant qui demande \u00e0 \u00eatre stock\u00e9 et trait\u00e9 imm\u00e9diatement, ainsi que le besoin croissant des utilisateurs d'avoir une repr\u00e9sentation r\u00e9cente et fid\u00e8le de l'\u00e9tat des donn\u00e9es. D'ailleurs, cette propri\u00e9t\u00e9 a engendr\u00e9 une autre pr\u00e9occupation des analystes des donn\u00e9es, qui est de fournir une introspection en temps r\u00e9el sur les donn\u00e9es, les qualifiant ainsi de \" Fast Data \". Vari\u00e9t\u00e9 Ce qui distingue vraiment les donn\u00e9es massives des donn\u00e9es g\u00e9r\u00e9es classiquement dans des bases de donn\u00e9es op\u00e9rationnelles, c'est le support des donn\u00e9es semi- et non structur\u00e9es. En effet, les donn\u00e9es non structur\u00e9es sont des donn\u00e9es qu'on stocke dans un format qui n'est pas d\u00e9fini \u00e0 la cr\u00e9ation, telles que les donn\u00e9es textuelles, images ou sons. Les donn\u00e9es semi-structur\u00e9es sont des donn\u00e9es qui contiennent une structure, mais une structure qui n'est pas rigide, et dont on ne d\u00e9finit pas les contraintes \u00e0 l'insertion de la donn\u00e9e, contrairement aux donn\u00e9es structur\u00e9es (se trouvant typiquement dans des bases de donn\u00e9es relationnelles) qui, si elles ne respectent pas la structure d\u00e9finie, sont consid\u00e9r\u00e9es fausses et ne sont pas autoris\u00e9es \u00e0 \u00eatre enregistr\u00e9es. On estime que seules 15% des donn\u00e9es dans une entreprise sont des donn\u00e9es structur\u00e9es, contre 85% qui ne le sont pas! Dans une optique centr\u00e9e sur les donn\u00e9es, dont le but est de gagner le maximum de vision \u00e0 partir des donn\u00e9es, perdre autant de sources d'information est un vrai probl\u00e8me. Il est donc important que les syst\u00e8mes Big Data sachent interpr\u00e9ter ces donn\u00e9es et en extraire le maximum de valeur. Toutes les d\u00e9cisions, choix et propri\u00e9t\u00e9s prises au niveau des architectures et infrastructures Big Data sont r\u00e9gies par ces trois caract\u00e9ristiques, ce qui va compl\u00e8tement changer la vision \"relationnelle\" que tout informaticien qui se respecte a acquis tout au long de ses ann\u00e9es d'\u00e9tude et de travail. Cela dit, ce ne sont pas les seules propri\u00e9t\u00e9s. D'autres V ont vu le jour, mais sans jamais avoir autant d'impact sur l'infrastructre, plut\u00f4t dans la fa\u00e7on de d\u00e9finir les processus, la gouvernance et les approches m\u00e9tier \u00e0 adopter. Nous citons par exemple : V\u00e9racit\u00e9 : c'est la confiance que nous devons avoir en nos donn\u00e9es. Cette propri\u00e9t\u00e9 est inversement proportionnelle au volume et \u00e0 la vari\u00e9t\u00e9 : plus nos donn\u00e9es sont fiables, moins elles sont diversifi\u00e9es et volumineuses! Valeur : c'est la capacit\u00e9 d'extraire de la valeur m\u00e9tier \u00e0 partir des donn\u00e9es. Variabilit\u00e9 : une extension de la vari\u00e9t\u00e9, qui indique \u00e0 quel point nos donn\u00e9es peuvent avoir des dimensions diff\u00e9rentes \u00e0 partir des sources de donn\u00e9es disparates. Visualisation : c'est la capacit\u00e9 de nos donn\u00e9es \u00e0 \u00eatre repr\u00e9sent\u00e9es par les outils de visualisation classiques. etc.","title":"Caract\u00e9ristiques des Donn\u00e9es Massives"},{"location":"p1-big-data/#infrastructure-big-data-besoins","text":"Les caract\u00e9ristiques des donn\u00e9es Big Data cit\u00e9es ci-dessus, entra\u00eenent des besoins particuliers en termes d'infrastructure et d'architecture. Volume La caract\u00e9ristique de volume, qui implique que la taille des donn\u00e9es augmente de fa\u00e7on r\u00e9guli\u00e8re, fait qu'on ne peut plus se contenter d'un syst\u00e8me centralis\u00e9 classique. Car dans un syst\u00e8me centralis\u00e9 (donc bas\u00e9 sur une seule machine), augmenter les ressources de stockage au besoin implique ce que nous appelons une scalabilit\u00e9 verticale ou un scale up , qui veut dire une augmentation des capacit\u00e9s du serveur de stockage en rajoutant des processeurs, de la RAM ou des disques. Cependant, cette solution, bien qu'elle soit intuitive, rapide et ne requiert pas de changement architecturaux cons\u00e9quents, implique en g\u00e9n\u00e9ral un temps d'arr\u00eat pendant l'installation, ainsi qu'une d\u00e9pense assez cons\u00e9quente pour faire l'acquisition d'un serveur puissant. De plus, une machine unique atteindra rapidement une limite mat\u00e9rielle, car il vous est impossible d'augmenter ses ressources ind\u00e9finiment. En contrepartie, il est possible de penser que, face \u00e0 un volume de donn\u00e9es toujours en augmentation, il serait plus judicieux de rajouter des machines au besoin, cr\u00e9ant ainsi un cluster de machines interconnect\u00e9es, ou syst\u00e8me r\u00e9parti , dont la taille et la capacit\u00e9 sont virtuellement illimit\u00e9es. Nous sommes donc face \u00e0 un autre type de scalabilit\u00e9 : la scalabilit\u00e9 horizontale ou le scale out . Donc Volume => Scalabilit\u00e9 Horizontale V\u00e9locit\u00e9 La v\u00e9locit\u00e9 est une propri\u00e9t\u00e9 qui, coupl\u00e9e au volume, rend la gestion de l'infrastructure un vrai cauchemar. En effet, g\u00e9rer des donn\u00e9es en continuelle arriv\u00e9e implique qu'il y'a un risque \u00e9norme de perte de donn\u00e9es, si elles ne sont pas manipul\u00e9es \u00e0 temps. C'est pour cette raison qu'un syst\u00e8me Big Data se doit d'\u00eatre continuellement disponible : toute requ\u00eate de lecture ou d'\u00e9criture doit \u00eatre trait\u00e9e en un temps raisonnable, et le syst\u00e8me doit \u00eatre continuellement alerte pour saisir toutes les donn\u00e9es, sans risquer de les perdre. Ainsi V\u00e9locit\u00e9 => Disponibilit\u00e9 Vari\u00e9t\u00e9 La vari\u00e9t\u00e9 de donn\u00e9es implique non seulement que nous sommes en pr\u00e9sence de donn\u00e9es structur\u00e9es, semi-structur\u00e9es et non structur\u00e9es, mais \u00e9galement que ces donn\u00e9es peuvent parvenir de sources diff\u00e9rentes, avec des formats diff\u00e9rents, et que m\u00eame \u00e0 partir d'une m\u00eame source, ce format peut changer d'un moment \u00e0 un autre. Dans les syst\u00e8mes classiques, tout ce qui est variable doit passer par une couche d'homog\u00e9n\u00e9isation qui transformera chaque entr\u00e9e ou enregistrement dans la forme souhait\u00e9e, en remplissant par des valeurs NULL les donn\u00e9es manquantes. Rajouter cette couche d'homog\u00e9n\u00e9isation aura un double impact n\u00e9gatif sur notre syst\u00e8me : (1) \u00e0 cause de la v\u00e9locit\u00e9, cette op\u00e9ration risquera de ralentir la collecte et saisie des donn\u00e9es entrantes, et (2) on pourra subir une perte de donn\u00e9es suite \u00e0 ces transformations. C'est pour ces raisons qu'un syst\u00e8me Big Data se doit de supporter des types de donn\u00e9es changeants, sans pour autant requ\u00e9rir \u00e0 des subterfuges qui alourdissent ou contournent le syst\u00e8me de stockage. D'o\u00f9 Vari\u00e9t\u00e9 => Flexibilit\u00e9","title":"Infrastructure Big Data : Besoins"},{"location":"p1-big-data/#theoreme-cap","text":"Les besoins de scalabilit\u00e9, disponibilit\u00e9 et flexibilit\u00e9, obligatoires pour avoir un syst\u00e8me Big Data en bonne et due forme, se trouvent confront\u00e9s \u00e0 une contrainte de taille... et qu'en est-il de la coh\u00e9rence (commun\u00e9ment appel\u00e9e aussi consistence, par anglicisme)? La coh\u00e9rence repr\u00e9sente en effet un must pour les syst\u00e8mes relationnels classiques, et une base sur laquelle sont prises toutes les d\u00e9cisions conceptuelles et techniques. Elle repr\u00e9sente le fait que les donn\u00e9es stables doivent respecter toutes les contraintes d'int\u00e9grit\u00e9 d\u00e9finies \u00e0 la cr\u00e9ation de la base de donn\u00e9e. Par exemple, si un champ est d\u00e9cr\u00e9t\u00e9 \"Not Null\", il doit le rester quelque soit la situation, et \u00e0 aucun moment une requ\u00eate ne doit surprendre ce champs avec une valeur nulle, m\u00eame si c'est juste une valeur interm\u00e9diaire. La coh\u00e9rence est un principe tr\u00e8s rigide dans les bases de donn\u00e9es relationnelles, et repr\u00e9sente le crit\u00e8re de base pour la gestion des transactions : le C de ACID . Cela dit, dans les syst\u00e8mes Big Data, nous nous trouvons confront\u00e9s \u00e0 un probl\u00e8me de taille : nous devons \u00eatre en pr\u00e9sence d'une infrastructure r\u00e9partie et hautement disponible. Or, il existe un th\u00e9or\u00e8me appel\u00e9 CAP pour Consistency / Availability / Partition tolerance , qui stipule que ces trois propri\u00e9t\u00e9s (notamment la coh\u00e9rence, la disponibilit\u00e9 et la tol\u00e9rance au partitionnement), ne peuvent jamais avoir lieu en m\u00eame temps. Seules deux d'entre elles peuvent \u00eatre respect\u00e9es \u00e0 la fois. Essayons d'expliquer pourquoi. Un syst\u00e8me r\u00e9parti est dit coh\u00e9rent si tous ses noeuds voient les m\u00eames donn\u00e9es en m\u00eame temps. C'est \u00e0 dire que, si nous r\u00e9alisons une op\u00e9ration de lecture sur un syst\u00e8me consistant, il devrait toujours retourner la valeur la plus r\u00e9cente qui ait \u00e9t\u00e9 \u00e9crite, quel que soit l'endroit \u00e0 partir duquel la lecture est effectu\u00e9e. Ainsi, si une donn\u00e9e est modifi\u00e9e sur un noeud particulier, pour conserver la coh\u00e9rence demand\u00e9e, aucune op\u00e9ration de lecture ne doit \u00eatre permise avant d'avoir mis \u00e0 jour toutes les r\u00e9pliques (copies) de cette donn\u00e9es. Or, les diff\u00e9rents noeuds d'un cluster sont en g\u00e9n\u00e9ral distants, parfois m\u00eame g\u00e9ographiquement, il est donc n\u00e9cessaire d'attendre que la propagation de la modification se fasse sur le r\u00e9seau, pour effectuer n'importe quelle op\u00e9ration, m\u00eame une lecture. Ceci va rendre nos donn\u00e9es indisponibles \u00e0 la lecture pendant tout le temps que durera l'op\u00e9ration de synchronisation, qui est un temps incertain puisque... r\u00e9seau. Assurer donc une coh\u00e9rence forte dans un syst\u00e8me distribu\u00e9 est en contradiction avec le besoin de disponibilit\u00e9 du syst\u00e8me et des donn\u00e9es. D'ailleurs, c'est ce que font les bases de donn\u00e9es relationnelles r\u00e9parties, qui conservent les propri\u00e9t\u00e9s ACID tout en distribuant les donn\u00e9es, mais qui souffrent d'un manque notoire de performance. Les syst\u00e8mes Big Data, subissant les contraintes des V pr\u00e9c\u00e9demment cit\u00e9s, doivent donc faire un choix. Or ce choix est loin d'\u00eatre facile : qui voudra acheter un syst\u00e8me qui pr\u00f4ne haut et fort qu'il est incoh\u00e9rent ? L'id\u00e9e serait donc de partir sur le principe de coh\u00e9rence \u00e9ventuelle ou parfois de coh\u00e9rence ajustable . Ainsi, un syst\u00e8me Big Data est un syst\u00e8me principalement disponible, fondamentalement r\u00e9parti, et qui assure une coh\u00e9rence \u00e9ventuelle au bout d'un temps g\u00e9n\u00e9ralement n\u00e9gligeable, avec la possibilit\u00e9 de configurer les niveau de coh\u00e9rence parfois m\u00eame dynamiquement. Les experts les appellent donc les syst\u00e8mes BASE (admirez le jeux de mot.. ACID, BASE \ud83d\ude0e): B asically A vailable S oft-state E ventual consistency La propri\u00e9t\u00e9 de Soft State ou d'\u00e9tat \"mou\" veut dire que l'\u00e9tat du syst\u00e8me peut changer dans le temps, m\u00eame sans qu'il y ait une nouvelle entr\u00e9e, \u00e0 cause du principe de coh\u00e9rence \u00e9ventuelle expliqu\u00e9 pr\u00e9c\u00e9demment. Maintenant que vous \u00eates plus familiaris\u00e9s avec les caract\u00e9ristiques d'un syst\u00e8me Big Data, listons quelques principes, appel\u00e9s ici MOTTOS , qui vont r\u00e9gir nos futures d\u00e9cisions dans ce domaine.","title":"Th\u00e9or\u00e8me CAP"},{"location":"p1-big-data/#principes-de-base-du-domaine-des-big-data","text":"Il est important, avant d'entamer n'importe quel travail sur les syst\u00e8mes Big Data, de consid\u00e9rer certains principes, qui sont parfois en enti\u00e8re contradiction avec les principes classiques de d\u00e9veloppement d'application. Ce n'est pas si \u00e9tonnant : le domaine des Big Data n'est pas cens\u00e9 prendre la place des domaines relationnel et d\u00e9cisionnel, mais plut\u00f4t les enrichir et les agr\u00e9menter. MOTTO 1 : Stocker d'abord, r\u00e9fl\u00e9chir ensuite \u00c0 cause de la v\u00e9locit\u00e9, il est important de consid\u00e9rer qu'il nous sera parfois difficile, voire impossible, de nettoyer les donn\u00e9es ou de faire un traitement quelconque dessus, avant de les stocker. Cela risque dans bien des cas de nous faire perdre des donn\u00e9es, le cauchemar de tout scientifique des donn\u00e9es! Nous devons donc envisager la possibilit\u00e9 de d\u00e9finir des syst\u00e8mes de stockage qui contiennent des donn\u00e9es non nettoy\u00e9es, en vrac (appel\u00e9es raw data ), pour ensuite lancer des traitements dessus.. l'horreur pour un gestionnaire de bases des donn\u00e9es! \ud83d\ude31 Bien entendu, ces \"bases\" ne sont pas con\u00e7ues pour \u00eatre directement exploit\u00e9es par des applications externes, mais plut\u00f4t pour conserver le plus longtemps possibles les donn\u00e9es brutes, sans perte, qui pourraient eventuellement \u00eatre r\u00e9utilis\u00e9es pour d'autres fins. MOTTO 2 : Absolument TOUTES les donn\u00e9es sont importantes! D'o\u00f9 l'int\u00e9r\u00eat du MOTTO 1 . Il nous est parfois difficile, au tout d\u00e9but de la conception des syst\u00e8mes Big Data, de cerner toutes les possibilit\u00e9s offertes par ces syst\u00e8mes et par les donn\u00e9es que nous avons \u00e0 notre disposition. Nous sommes donc en g\u00e9n\u00e9ral tent\u00e9s de supprimer les donn\u00e9es dont nous n'avons pas besoin une fois extraite l'information imm\u00e9diatement utile. Cela dit, gr\u00e2ce \u00e0 l'accessibilit\u00e9 des syst\u00e8mes de stockage magn\u00e9tiques et leur prix de plus en plus bas, nous consid\u00e9rons qu'il est largement plus b\u00e9n\u00e9fique de stocker des donn\u00e9es qu'on n'utilisera peut-\u00eatre jamais, plut\u00f4t que de gagner de la place et perdre un potentiel pouvoir concurrentiel. MOTTO 3 : Ce sont les donn\u00e9es qui pilotent le traitement Dans un syst\u00e8me op\u00e9rationnel classique, ainsi que dans la plupart des syst\u00e8mes d\u00e9cisionnels, ce sont les besoins m\u00e9tier qui pr\u00e9valoient : le responsable m\u00e9tier commence par d\u00e9finir les besoins (ou les KPIs : Key Performance Indicators dans le cas d'un syst\u00e8me d\u00e9cisionnel), puis le responsable technique con\u00e7oit les structures de donn\u00e9es pour r\u00e9pondre \u00e0 ces besoins. Par essence, un syst\u00e8me Big Data fonctionne diff\u00e9remment : les donn\u00e9es sont collect\u00e9es tout d'abord \u00e0 partir de toutes les sources possibles; des traitements de fouille et d'exploration de ces donn\u00e9es sont lanc\u00e9s ensuite, pour extraire de la valeur \u00e0 partir de ces donn\u00e9es. L'objectif est toujours le m\u00eame : chercher l'effet WOW! D'o\u00f9 l'int\u00e9r\u00eat de ce MOTTO : d\u00e9finir le traitement \u00e0 r\u00e9aliser d\u00e9pend des donn\u00e9es que nous avons r\u00e9ussi \u00e0 collecter, et pas le contraire. Cela implique donc l'utilisation d'autres types de syst\u00e8mes de traitement et d'algorithmes d'analyse. MOTTO 4 : Co-localisation des donn\u00e9es et du traitement Un syst\u00e8me classique \u00e0 plusieurs couches, tel que le syst\u00e8me trois tiers par exemple, se base sur le principe de s\u00e9paration des donn\u00e9es et du traitement. On trouve en g\u00e9n\u00e9ral des donn\u00e9es sur un serveur de bases de donn\u00e9es s\u00e9par\u00e9, et les traitement complexes sur un serveur d'application qui se charge de l'aggr\u00e9gation et de l'affichage de ces donn\u00e9es. Ceci est agr\u00e9ment\u00e9 d'un langage de requ\u00eatage d\u00e9claratif (typiquement SQL) pour r\u00e9aliser des op\u00e9rations de filtrage, parfois assez lourdes et complexes, au niveau de la base de donn\u00e9es. Cela dit, dans un contexte Big Data, le volume des donn\u00e9es peut s'av\u00e9rer assez cons\u00e9quent, trop m\u00eame pour envisager de le d\u00e9placer \u00e0 chaque fois vers un autre syst\u00e8me pour en extraire une vraie valeur. De plus, compter sur un langage comme SQL pour diminuer le volume ou faire de simples agr\u00e9gations au niveau de la base de donn\u00e9es pourra la rendre indisponible pendant un moment (car n'oublions pas que nous parlons d'un syst\u00e8me r\u00e9parti), ce qui va \u00e0 l'encontre du principe de v\u00e9locit\u00e9, qui exige une disponibilit\u00e9 \u00e0 toute \u00e9preuve du syst\u00e8me de stockage. C'est pour cette raison que, pour r\u00e9aliser les traitements voulus en un temps raisonnable et sans avoir \u00e0 trimballer les donn\u00e9es sur le r\u00e9seau, il est question dans les syst\u00e8mes Big Data de d\u00e9placer le traitement vers les donn\u00e9es massives, au lieu de d\u00e9placer les donn\u00e9es vers le traitement. MOTTO 5 : La redondance, c'est bien Dans les bases de donn\u00e9es relationnelles, le plus grand ennemi \u00e0 combattre dans la conception de la structure de donn\u00e9es est la redondance, et ce pour deux raisons. La premi\u00e8re, \u00e9vidente, est le gain d'espace : notre espace de stockage est pr\u00e9cieux, et nous devons \u00e9viter de le gaspiller sans raison pr\u00e9cise. La deuxi\u00e8me est un besoin de coh\u00e9rence : si nous dupliquons une m\u00eame information \u00e0 plusieurs endroits dans la base, nous devrons par la suite faire attention, parfois par des m\u00e9canismes compliqu\u00e9s et co\u00fbteux, \u00e0 ce que cette information soit mise \u00e0 jour instantan\u00e9ment sur la totalit\u00e9 de ses copies. Ce besoin d'\u00e9viter la redondance a cr\u00e9\u00e9 la n\u00e9cessit\u00e9 d'utiliser plusieurs techniques, telles que les jointures et clefs \u00e9trang\u00e8res, et entra\u00eene parfois la cr\u00e9ation d'un tr\u00e8s grand nombre de tables. Ceci rajoute une complexit\u00e9 pour le requ\u00eatage, et une lourdeur d'ex\u00e9cution des t\u00e2ches sur la base. Un syst\u00e8me Big Data qui, non seulement est caract\u00e9ris\u00e9 par un gros volume de donn\u00e9es, mais \u00e9galement une grande v\u00e9locit\u00e9, et qui doit donc \u00eatre imm\u00e9diatement disponible, ne peut pas se permettre de gaspiller ses ressources en requ\u00eates inutiles. On tol\u00e8re donc \u00e0 un certain point les risques dus \u00e0 la redondance, pour gagner en disponibilit\u00e9, primordiale dans ce type de syst\u00e8mes. D'autre part, un syst\u00e8me Big Data est un syst\u00e8me r\u00e9parti par excellence, et dans un syst\u00e8me r\u00e9parti, il est primordial d'assurer une bonne tol\u00e9rance aux fautes en cr\u00e9ant des r\u00e9pliques des donn\u00e9es, diss\u00e9min\u00e9es partout sur le cluster. Ces r\u00e9pliques assurent qu'aucune machine n'est compl\u00e8tement indispensable, et diminue le risque d'indisponibilit\u00e9 des donn\u00e9es. Un autre signe de redondance. MOTTO 6 : Vive le Polyglottisme! \u00catre polyglotte, c'est \u00eatre capable de parler plusieurs langues. Et les syst\u00e8mes Big Data encouragent le polyglottisme. En effet, ce sont des syst\u00e8mes complexes qui impliquent en g\u00e9n\u00e9ral plusieurs traitements et plusieurs types de donn\u00e9es diff\u00e9rentes (donn\u00e9es brutes, donn\u00e9es nettoy\u00e9es, donn\u00e9es trait\u00e9es), ce qui fait qu'il existe deux principes importants \u00e0 encourager : Polyglot Programming : Une application peut comporter plusieurs langages et paradigmes de programmation, chacun assurant un besoin particulier, de fa\u00e7on \u00e0 profiter des avantages de chacun \u00e0 sa juste valeur. Polyglot Persistence : Dans une m\u00eame application, il est possible d'utiliser plusieurs syst\u00e8mes de stockage diff\u00e9rents (relationnels, NOSQL, syst\u00e8mes de fichiers, etc.). Gr\u00e2ce \u00e0 ces deux principes, on pourra cr\u00e9er des applications complexes mais compl\u00e8tes, qui permettent d'assurer tous les besoins en terme de stockage et de traitement.","title":"Principes de base du Domaine des Big Data"},{"location":"p1-big-data/#technologies-et-paradigmes","text":"Les op\u00e9rations \u00e0 r\u00e9aliser sur les syst\u00e8mes Big Data consistent principalement en : Ingestion des donn\u00e9es : repr\u00e9sente les phases de collecte et d'importation des donn\u00e9es pour \u00eatre stock\u00e9es ou trait\u00e9es \u00e0 la vol\u00e9e. Cela peut se faire en \"temps r\u00e9el\", c'est \u00e0 dire que les donn\u00e9es sont import\u00e9es au moment o\u00f9 elles sont \u00e9mises par leur source, ou bien \"par lots\", ce qui veut dire que les donn\u00e9es sont import\u00e9es par portions \u00e0 intervalles r\u00e9gulier. Exemples de technologies Apache Kafka , Amazon Kinesis , Apache Flume , Sqoop , etc. Stockage des donn\u00e9es : Les syst\u00e8mes de stockage de donn\u00e9es respectant les propri\u00e9t\u00e9s le Big Data se distinguent principalement en syst\u00e8mes de fichiers distribu\u00e9s, tel que Hadoop HDFS ou Google GFS , ou bases de donn\u00e9es NOSQL, tel que MongoDB , Cassandra , Redis ou Neo4J . Traitement des donn\u00e9es : Plusieurs types de traitement de donn\u00e9es sont possibles, nous citons : Traitement par lot (Batch Processing) : c'est le traitement des donn\u00e9es au repos (data at rest) qui se fait sur l'ensemble des donn\u00e9es stock\u00e9es, sans avoir besoin d'une interaction avec l'utilisateur. Le traitement par lot est adapt\u00e9 principalement aux op\u00e9rations ayant lieu \u00e0 la fin d'un cycle, permettant d'avoir une vision globale sur la totalit\u00e9 des donn\u00e9es, par exemple pour avoir un rapport global ou une analyse mensuelle. Les op\u00e9rations de traitement par lots sont en g\u00e9n\u00e9ral lanc\u00e9es \u00e0 des p\u00e9riodes r\u00e9guli\u00e8res, car elles sont connues pour avoir une grande latence (temps total de traitement). Exemples de technologies Hadoop Map Reduce et Spark Batch . Traitement en Streaming (Stream Processing) : c'est le traitement des donn\u00e9es en transit (data in motion) , ou en d'autres termes, le traitement des donn\u00e9es pendant qu'elles sont produites ou re\u00e7ues. Les donn\u00e9es \u00e9tant en g\u00e9n\u00e9ral cr\u00e9\u00e9es en tant que flux continu (\u00e9v\u00e8nements de capteurs, activit\u00e9 des utilisateurs sur un site web, flux vid\u00e9o, etc.), elles sont captur\u00e9es comme une s\u00e9rie d'\u00e9v\u00e8nements continus dans le temps. Avant la cr\u00e9ation des traitements en streaming, ces donn\u00e9es \u00e9taient stock\u00e9es dans une base de donn\u00e9es, un syst\u00e8me de fichier ou tout autre forme de stockage en masse. Les applications appelleront ensuite les donn\u00e9es au besoin. Gr\u00e2ce \u00e0 ce nouveau paradigme, les donn\u00e9es peuvent maintenant \u00eatre trait\u00e9es \u00e0 la vol\u00e9e, ce qui permet \u00e0 la couche applicative d'\u00eatre toujours sur \u00e9coute et \u00e0 jour. Exemples de technologies Apache Flink et Apache Storm . Traitement par Micro-Lot (Micro-Batch Processing) : c'est la pratique de collecter les donn\u00e9es en petits groupes (appel\u00e9s des micro-lots ou des micro-batchs ) pour les traiter. Contrairement au traditionnel traitement par lot, cette variante fait en sorte que le traitement des donn\u00e9es soit plus fr\u00e9quent, et que les r\u00e9sultats soient produits avec une latence beaucoup plus petite. Les donn\u00e9es sont collect\u00e9es par intervalles selon un seuil pr\u00e9d\u00e9fini, limit\u00e9 par un temps (par exemple toutes les secondes), ou par un nombre (tous les 20 \u00e9l\u00e9ments). Ce traitement est en g\u00e9n\u00e9ral une alternative au traitement en streaming, o\u00f9 les donn\u00e9es sont trait\u00e9es \u00e0 la vol\u00e9e, mais risquent d'\u00eatre perdues si le temps de traitement est sup\u00e9rieur \u00e0 la fr\u00e9quence de g\u00e9n\u00e9ration des donn\u00e9es. Le micro-batching permet, par contraste, de sauvegarder les donn\u00e9es dans un buffer, ralentissant ainsi le flux g\u00e9n\u00e9r\u00e9. D'autre part, les donn\u00e9es \u00e9tant trait\u00e9es par micro-lots, il est possible d'avoir une visibilit\u00e9 sur ce petit lot de donn\u00e9es, contrairement au traitement en streaming qui n'a de visibilit\u00e9 que sur la derni\u00e8re donn\u00e9e g\u00e9n\u00e9r\u00e9e, \u00e0 moins de proc\u00e9der \u00e0 des m\u00e9canismes parfois co\u00fbteux. En contrepartie, le traitement en micro-batch donne des r\u00e9sultats moins r\u00e9cents que le \"vrai\" streaming, et s'ex\u00e9cute sous forme de bursts r\u00e9guliers, qui peuvent parfois \u00eatre g\u00eanants pour le syst\u00e8me sous-jacent. Exemples de technologies Spark Streaming et Logstash . Traitement Interactif (Interactive Processing) : Dans les syst\u00e8mes Big Data, la notion de transaction n'est plus exactement la m\u00eame que pour les syst\u00e8mes classiques: finies les sacro-saintes propri\u00e9t\u00e9s ACID dont le premier objectif est d'avoir des donn\u00e9es correctes et coh\u00e9rentes, et bonjour les propri\u00e9t\u00e9s BASE, qui favorisent un acc\u00e8s moins rigide aux donn\u00e9es. On parle donc rarement de traitement transactionnel en Big Data, mais de traitements plut\u00f4t interactifs : une requ\u00eate est envoy\u00e9e par le client, trait\u00e9e imm\u00e9diatement par le syst\u00e8me qui renverra un r\u00e9sultat dans un temps raisonnable. On parle alors d' interaction entre l'utilisateur et le syst\u00e8me. Les traitements en batch et en streaming ne sont pas cens\u00e9s communiquer avec un utilisateur de l'autre c\u00f4t\u00e9. En g\u00e9n\u00e9ral, les r\u00e9sultats de ces traitements sont enregistr\u00e9s dans un syst\u00e8me de stockage, qui sera, lui, par la suite interrog\u00e9 par l'utilisateur. Le traitement interactif est donc le r\u00e9sultat d'une requ\u00eate de l'utilisateur, faite en g\u00e9n\u00e9ral sur une base de donn\u00e9es (relationnelle ou NOSQL). Exemples de technologies Apache Drill , Cloudera Impala ou Apache Zeppelin .","title":"Technologies et Paradigmes"},{"location":"p2-spark/","text":"Partie 2 - Introduction \u00e0 Apache Spark Apache Spark - Pr\u00e9sentation Apache Spark est une plateforme de traitement sur cluster g\u00e9n\u00e9rique. C'est un moteur de traitement libre, assurant un traitement parall\u00e8le et distribu\u00e9 sur des donn\u00e9es massives. Il fournit une API de d\u00e9veloppement pour permettre un traitement en streaming, l'apprentissage automatique ou la gestion de requ\u00eates SQL et demandant des acc\u00e8s r\u00e9p\u00e9t\u00e9s sur un grand volume de donn\u00e9es. 1 Apache Spark permet de r\u00e9aliser des traitements par lot ( batch processing ) ou \u00e0 la vol\u00e9e ( stream processing ) et est con\u00e7u de fa\u00e7on \u00e0 pouvoir int\u00e9grer tous les outils et technologies Big Data. Par exemple, non seulement Spark peut-il acc\u00e9der aux sources de donn\u00e9es de Hadoop, il peut \u00e9galement tourner sur un cluster Hadoop. \u00c9tant donn\u00e9 que Spark n'offre pas de solution de stockage (pas encore en tout cas), il est logique qu'il puisse profiter de la puissance de HDFS (le syst\u00e8me de fichiers de Hadoop), tout en offrant lui des performances in\u00e9gal\u00e9es pour le traitement en batch, ainsi que d'autres facilit\u00e9s (non offertes par Hadoop Map Reduce) telles que le traitement it\u00e9ratif, interactif et \u00e0 la vol\u00e9e. Spark offre des APIs de haut niveau en Java, Scala, Python et R. Il utilise le traitement en m\u00e9moire ( in-memory processing ), en exploitant les ressources combin\u00e9es du cluster comme si c'\u00e9tait une machine unique. Apache Spark a \u00e9t\u00e9 cr\u00e9\u00e9 en 2009 au laboratoire UC Berkeley R&D Lab (appel\u00e9 maintenant AMPLab), et est devenu open-source en 2010 avec une licence BSD. En 2013, il a int\u00e9gr\u00e9 Apache Software Foundation, pour devenir, en 2014, un projet Apache de haut niveau. Apache Spark - Composants Apache Spark utilise une architecture en couches, comportant plusieurs composants, dont l'objectif est de permettre de r\u00e9aliser des traitements performants tout en promettant un d\u00e9veloppement et une int\u00e9gration facilit\u00e9es. Il est n\u00e9 \u00e0 la base pour pallier les probl\u00e8mes pos\u00e9s par Hadoop Map Reduce, mais est devenu une entit\u00e9 \u00e0 lui seul, offrant bien plus que le traitement par lot classique. 1 Voici les composants de Spark: Spark Core Spark Core est le point central de Spark, qui fournit une plateforme d'ex\u00e9cution pour toutes les applications Spark. De plus, il supporte un large \u00e9ventail d'applications. Spark SQL Spark SQL se situe au dessus de Spark, pour permettre aux utilisateurs d'utiliser des requ\u00eates SQL/HQL. Les donn\u00e9es structur\u00e9es et semi-structur\u00e9es peuvent ainsi \u00eatre trait\u00e9es gr\u00e2ce \u00e0 Spark SQL, avec une performance am\u00e9lior\u00e9e. Spark Streaming Spark Streaming permet de cr\u00e9er des applications d'analyse de donn\u00e9es interactives. Les flux de donn\u00e9es sont transform\u00e9s en micro-lots et trait\u00e9s par dessus Spark Core. Spark MLlib La biblioth\u00e8que de machine learning MLlib fournit des algorithmes de haute qualit\u00e9 pour l'apprentissage automatique. Ce sont des libraries riches, tr\u00e8s utiles pour les data scientists, autorisant de plus des traitements en m\u00e9moire am\u00e9liorant de fa\u00e7on drastique la performance de ces algorithmes sur des donn\u00e9es massives. Spark GraphX Spark Graphx est le moteur d'ex\u00e9cution permettant un traitement scalable utilisant les graphes, se basant sur Spark Core. Architecture de Spark Les applications Spark s'ex\u00e9cutent comme un ensemble ind\u00e9pendant de processus sur un cluster, coordonn\u00e9s par un objet SparkContext dans le programme principal, appel\u00e9 driver program . 2 Pour s'ex\u00e9cuter sur un cluster, SparkContext peut se connecter \u00e0 plusieurs types de gestionnaires de clusters ( Cluster Managers ): Sur le gestionnaire autonome de Spark , qui est inclus dans Spark, et qui pr\u00e9sente le moyen le plus rapide et simple de mettre en place un cluster. Sur Apache Mesos , un gestionnaire de cluster g\u00e9n\u00e9ral qui peut aussi tourner sur Hadoop Map Reduce. Sur Hadoop YARN , le gestionnaire de ressources de Hadoop 2. Sur Kubernetes , un syst\u00e8me open-source pour l'automatisation du d\u00e9ploiement et la gestion des applications conteneuris\u00e9es. Ces gestionnaires permettent d'allouer les ressources n\u00e9cessaires pour l'ex\u00e9cution de plusieurs applications Spark. Une fois connect\u00e9, Spark lance des ex\u00e9cuteurs sur les noeuds du cluster, qui sont des processus qui lancent des traitements et stockent des donn\u00e9es pour les applications. Il envoie ensuite le code de l'application (dans un fichier JAR ou Python) aux ex\u00e9cuteurs . Spark Context envoie finalement les t\u00e2ches \u00e0 ex\u00e9cuter aux ex\u00e9cuteurs . Il est \u00e0 noter que: Chaque application a son lot d'ex\u00e9cuteurs, qui restent actifs tout au long de l'ex\u00e9cution de l'application, et qui lancent des t\u00e2ches sur plusieurs threads. Ainsi, les applications sont isol\u00e9es les unes des autres, du point de vue de l'orchestration (chaque driver ex\u00e9cute ses propres t\u00e2ches), et des ex\u00e9cuteurs (les t\u00e2ches des diff\u00e9rentes applications tournent sur des JVM diff\u00e9rentes). Ceci implique \u00e9galement que les applications (ou Jobs) Sparks ne peuvent pas \u00e9changer des donn\u00e9es, sans les enregistrer sur un support de stockage externe. Spark est ind\u00e9pendant du gestionnaire de cluster sous-jacent. Il suffit de configurer Spark pour utiliser ce gestionnaire, il peut g\u00e9rer ses ressources en m\u00eame temps que d'autres applications, m\u00eame non-Spark. L'application principale ( driver ) doit \u00eatre \u00e0 l'\u00e9coute des connexions entrantes venant de ses ex\u00e9cuteurs. Caract\u00e9ristiques de Spark Spark est connu pour avoir plusieurs caract\u00e9ristiques qui en font l'une des plateformes les plus utilis\u00e9es dans le domaine des Big Data. Nous citons: 1 Performance de traitement : Il est possible de r\u00e9aliser une vitesse de traitement tr\u00e8s \u00e9lev\u00e9e avec Spark sur des fichiers volumineux qui peut \u00eatre jusqu'\u00e0 100x meilleur que Hadoop Map Reduce, par exemple, et ceci gr\u00e2ce \u00e0 des m\u00e9canismes tel que la r\u00e9duction du nombre de lectures \u00e9critures sur le disque, la valorisation du traitement en m\u00e9moire et l'utilisation des m\u00e9moires cache et RAM pour les donn\u00e9es interm\u00e9diaires. Dynamicit\u00e9 : Il est facile de d\u00e9velopper des applications parall\u00e8les, gr\u00e2ce aux op\u00e9rateurs haut niveau fournis par Spark (allant jusqu'\u00e0 80 op\u00e9rateurs). Tol\u00e9rance aux Fautes : Apache Spark fournit un m\u00e9canisme de tol\u00e9rance aux fautes gr\u00e2ce aux RDD. Ces structures en m\u00e9moire sont con\u00e7ues pour r\u00e9cup\u00e9rer les donn\u00e9es en cas de panne. Traitements \u00e0 la vol\u00e9e : L'un des avantages de Spark par rapport \u00e0 Hadoop Map Reduce, c'est qu'il permet de traiter les donn\u00e9es \u00e0 la vol\u00e9e, pas uniquement en batch. \u00c9valuations Paresseuses ( Lazy Evaluations ) : Toutes les transformations faites sur Spark RDD sont paresseuses de nature, ce qui veut dire qu'elles ne donnent pas de r\u00e9sultat direct apr\u00e8s leur ex\u00e9cution, mais g\u00e9n\u00e8rent un nouvel RDD \u00e0 partir de l'ancien. On n'ex\u00e9cute effectivement les transformations qu'au moment de lancer une action sur les donn\u00e9es. Nous allons d\u00e9tailler cet aspect plus tard dans le cours. Support de plusieurs langages : Plusieurs langages de programmation sont support\u00e9s par Spark, tel que Java, R, Scala et Python. Une communaut\u00e9 active et en expansion : Des d\u00e9veloppeurs de plus de 50 entreprises sont impliqu\u00e9s dans le d\u00e9veloppement et l'am\u00e9lioration de Spark. Ce projet a \u00e9t\u00e9 initi\u00e9 en 2009 et est encore en expansion. Support d'analyses sophistiqu\u00e9es : Spark est fourni avec un ensemble d'outils d\u00e9di\u00e9s pour le streaming, les requ\u00eates interactives, le machine learning, etc. Int\u00e9gration avec Hadoop : Spark peut s'ex\u00e9cuter ind\u00e9pendamment ou sur Hadoop YARN, et profiter ainsi de la puissance du syst\u00e8me de fichiers distribu\u00e9 Hadoop HDFS. Limitations de Spark Spark a plusieurs limitations, tel que : 1 Pas de support pour le traitement en temps r\u00e9el : Spark permet le traitement en temps-presque-r\u00e9el, car il utilise le traitement en micro-lot plut\u00f4t que le traitement en streaming. Probl\u00e8mes avec les fichiers de petite taille : Spark partitionne le traitement sur plusieurs ex\u00e9cuteurs, et est optimis\u00e9 principalement pour les grands volumes de donn\u00e9es. L'utiliser pour des fichiers de petite taille va rajouter un co\u00fbt suppl\u00e9mentaire, il est donc plus judicieux dans ce cas d'utiliser un traitement s\u00e9quentiel classique sur une seule machine. Pas de syst\u00e8me de gestion des fichiers : Spark est principalement un syst\u00e8me de traitement, et ne fournit pas de solution pour le stockage des donn\u00e9es. Il doit donc se baser sur d'autres syst\u00e8mes de stockage tel que Hadoop HDFS ou Amazon S3. Co\u00fbteux : En tant que syst\u00e8me de traitement en m\u00e9moire, le co\u00fbt d'ex\u00e9cuter Spark sur un cluster peut \u00eatre tr\u00e8s \u00e9lev\u00e9 en terme de consommation m\u00e9moire. Nombre d'algorithmes limit\u00e9 : Malgr\u00e9 la disponibilit\u00e9 de la biblioth\u00e8que MLlib, elle reste limit\u00e9e en termes de nombre d'algorithmes impl\u00e9ment\u00e9s. Latence : La latence de Spark pour l'ex\u00e9cution de Jobs \u00e0 la vol\u00e9e est plus \u00e9lev\u00e9e que d'autres solutions de traitement en streaming tel que Flink . R\u00e9f\u00e9rences Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 02/2020 \u21a9 \u21a9 \u21a9 \u21a9 Spark Documentation, Cluster Mode Overview , https://spark.apache.org/docs/latest/cluster-overview.html , consult\u00e9 le 02/2020 \u21a9","title":"P2 - Introduction \u00e0 Apache Spark"},{"location":"p2-spark/#partie-2-introduction-a-apache-spark","text":"","title":"Partie 2 - Introduction \u00e0 Apache Spark"},{"location":"p2-spark/#apache-spark-presentation","text":"Apache Spark est une plateforme de traitement sur cluster g\u00e9n\u00e9rique. C'est un moteur de traitement libre, assurant un traitement parall\u00e8le et distribu\u00e9 sur des donn\u00e9es massives. Il fournit une API de d\u00e9veloppement pour permettre un traitement en streaming, l'apprentissage automatique ou la gestion de requ\u00eates SQL et demandant des acc\u00e8s r\u00e9p\u00e9t\u00e9s sur un grand volume de donn\u00e9es. 1 Apache Spark permet de r\u00e9aliser des traitements par lot ( batch processing ) ou \u00e0 la vol\u00e9e ( stream processing ) et est con\u00e7u de fa\u00e7on \u00e0 pouvoir int\u00e9grer tous les outils et technologies Big Data. Par exemple, non seulement Spark peut-il acc\u00e9der aux sources de donn\u00e9es de Hadoop, il peut \u00e9galement tourner sur un cluster Hadoop. \u00c9tant donn\u00e9 que Spark n'offre pas de solution de stockage (pas encore en tout cas), il est logique qu'il puisse profiter de la puissance de HDFS (le syst\u00e8me de fichiers de Hadoop), tout en offrant lui des performances in\u00e9gal\u00e9es pour le traitement en batch, ainsi que d'autres facilit\u00e9s (non offertes par Hadoop Map Reduce) telles que le traitement it\u00e9ratif, interactif et \u00e0 la vol\u00e9e. Spark offre des APIs de haut niveau en Java, Scala, Python et R. Il utilise le traitement en m\u00e9moire ( in-memory processing ), en exploitant les ressources combin\u00e9es du cluster comme si c'\u00e9tait une machine unique. Apache Spark a \u00e9t\u00e9 cr\u00e9\u00e9 en 2009 au laboratoire UC Berkeley R&D Lab (appel\u00e9 maintenant AMPLab), et est devenu open-source en 2010 avec une licence BSD. En 2013, il a int\u00e9gr\u00e9 Apache Software Foundation, pour devenir, en 2014, un projet Apache de haut niveau.","title":"Apache Spark - Pr\u00e9sentation"},{"location":"p2-spark/#apache-spark-composants","text":"Apache Spark utilise une architecture en couches, comportant plusieurs composants, dont l'objectif est de permettre de r\u00e9aliser des traitements performants tout en promettant un d\u00e9veloppement et une int\u00e9gration facilit\u00e9es. Il est n\u00e9 \u00e0 la base pour pallier les probl\u00e8mes pos\u00e9s par Hadoop Map Reduce, mais est devenu une entit\u00e9 \u00e0 lui seul, offrant bien plus que le traitement par lot classique. 1 Voici les composants de Spark: Spark Core Spark Core est le point central de Spark, qui fournit une plateforme d'ex\u00e9cution pour toutes les applications Spark. De plus, il supporte un large \u00e9ventail d'applications. Spark SQL Spark SQL se situe au dessus de Spark, pour permettre aux utilisateurs d'utiliser des requ\u00eates SQL/HQL. Les donn\u00e9es structur\u00e9es et semi-structur\u00e9es peuvent ainsi \u00eatre trait\u00e9es gr\u00e2ce \u00e0 Spark SQL, avec une performance am\u00e9lior\u00e9e. Spark Streaming Spark Streaming permet de cr\u00e9er des applications d'analyse de donn\u00e9es interactives. Les flux de donn\u00e9es sont transform\u00e9s en micro-lots et trait\u00e9s par dessus Spark Core. Spark MLlib La biblioth\u00e8que de machine learning MLlib fournit des algorithmes de haute qualit\u00e9 pour l'apprentissage automatique. Ce sont des libraries riches, tr\u00e8s utiles pour les data scientists, autorisant de plus des traitements en m\u00e9moire am\u00e9liorant de fa\u00e7on drastique la performance de ces algorithmes sur des donn\u00e9es massives. Spark GraphX Spark Graphx est le moteur d'ex\u00e9cution permettant un traitement scalable utilisant les graphes, se basant sur Spark Core.","title":"Apache Spark - Composants"},{"location":"p2-spark/#architecture-de-spark","text":"Les applications Spark s'ex\u00e9cutent comme un ensemble ind\u00e9pendant de processus sur un cluster, coordonn\u00e9s par un objet SparkContext dans le programme principal, appel\u00e9 driver program . 2 Pour s'ex\u00e9cuter sur un cluster, SparkContext peut se connecter \u00e0 plusieurs types de gestionnaires de clusters ( Cluster Managers ): Sur le gestionnaire autonome de Spark , qui est inclus dans Spark, et qui pr\u00e9sente le moyen le plus rapide et simple de mettre en place un cluster. Sur Apache Mesos , un gestionnaire de cluster g\u00e9n\u00e9ral qui peut aussi tourner sur Hadoop Map Reduce. Sur Hadoop YARN , le gestionnaire de ressources de Hadoop 2. Sur Kubernetes , un syst\u00e8me open-source pour l'automatisation du d\u00e9ploiement et la gestion des applications conteneuris\u00e9es. Ces gestionnaires permettent d'allouer les ressources n\u00e9cessaires pour l'ex\u00e9cution de plusieurs applications Spark. Une fois connect\u00e9, Spark lance des ex\u00e9cuteurs sur les noeuds du cluster, qui sont des processus qui lancent des traitements et stockent des donn\u00e9es pour les applications. Il envoie ensuite le code de l'application (dans un fichier JAR ou Python) aux ex\u00e9cuteurs . Spark Context envoie finalement les t\u00e2ches \u00e0 ex\u00e9cuter aux ex\u00e9cuteurs . Il est \u00e0 noter que: Chaque application a son lot d'ex\u00e9cuteurs, qui restent actifs tout au long de l'ex\u00e9cution de l'application, et qui lancent des t\u00e2ches sur plusieurs threads. Ainsi, les applications sont isol\u00e9es les unes des autres, du point de vue de l'orchestration (chaque driver ex\u00e9cute ses propres t\u00e2ches), et des ex\u00e9cuteurs (les t\u00e2ches des diff\u00e9rentes applications tournent sur des JVM diff\u00e9rentes). Ceci implique \u00e9galement que les applications (ou Jobs) Sparks ne peuvent pas \u00e9changer des donn\u00e9es, sans les enregistrer sur un support de stockage externe. Spark est ind\u00e9pendant du gestionnaire de cluster sous-jacent. Il suffit de configurer Spark pour utiliser ce gestionnaire, il peut g\u00e9rer ses ressources en m\u00eame temps que d'autres applications, m\u00eame non-Spark. L'application principale ( driver ) doit \u00eatre \u00e0 l'\u00e9coute des connexions entrantes venant de ses ex\u00e9cuteurs.","title":"Architecture de Spark"},{"location":"p2-spark/#caracteristiques-de-spark","text":"Spark est connu pour avoir plusieurs caract\u00e9ristiques qui en font l'une des plateformes les plus utilis\u00e9es dans le domaine des Big Data. Nous citons: 1 Performance de traitement : Il est possible de r\u00e9aliser une vitesse de traitement tr\u00e8s \u00e9lev\u00e9e avec Spark sur des fichiers volumineux qui peut \u00eatre jusqu'\u00e0 100x meilleur que Hadoop Map Reduce, par exemple, et ceci gr\u00e2ce \u00e0 des m\u00e9canismes tel que la r\u00e9duction du nombre de lectures \u00e9critures sur le disque, la valorisation du traitement en m\u00e9moire et l'utilisation des m\u00e9moires cache et RAM pour les donn\u00e9es interm\u00e9diaires. Dynamicit\u00e9 : Il est facile de d\u00e9velopper des applications parall\u00e8les, gr\u00e2ce aux op\u00e9rateurs haut niveau fournis par Spark (allant jusqu'\u00e0 80 op\u00e9rateurs). Tol\u00e9rance aux Fautes : Apache Spark fournit un m\u00e9canisme de tol\u00e9rance aux fautes gr\u00e2ce aux RDD. Ces structures en m\u00e9moire sont con\u00e7ues pour r\u00e9cup\u00e9rer les donn\u00e9es en cas de panne. Traitements \u00e0 la vol\u00e9e : L'un des avantages de Spark par rapport \u00e0 Hadoop Map Reduce, c'est qu'il permet de traiter les donn\u00e9es \u00e0 la vol\u00e9e, pas uniquement en batch. \u00c9valuations Paresseuses ( Lazy Evaluations ) : Toutes les transformations faites sur Spark RDD sont paresseuses de nature, ce qui veut dire qu'elles ne donnent pas de r\u00e9sultat direct apr\u00e8s leur ex\u00e9cution, mais g\u00e9n\u00e8rent un nouvel RDD \u00e0 partir de l'ancien. On n'ex\u00e9cute effectivement les transformations qu'au moment de lancer une action sur les donn\u00e9es. Nous allons d\u00e9tailler cet aspect plus tard dans le cours. Support de plusieurs langages : Plusieurs langages de programmation sont support\u00e9s par Spark, tel que Java, R, Scala et Python. Une communaut\u00e9 active et en expansion : Des d\u00e9veloppeurs de plus de 50 entreprises sont impliqu\u00e9s dans le d\u00e9veloppement et l'am\u00e9lioration de Spark. Ce projet a \u00e9t\u00e9 initi\u00e9 en 2009 et est encore en expansion. Support d'analyses sophistiqu\u00e9es : Spark est fourni avec un ensemble d'outils d\u00e9di\u00e9s pour le streaming, les requ\u00eates interactives, le machine learning, etc. Int\u00e9gration avec Hadoop : Spark peut s'ex\u00e9cuter ind\u00e9pendamment ou sur Hadoop YARN, et profiter ainsi de la puissance du syst\u00e8me de fichiers distribu\u00e9 Hadoop HDFS.","title":"Caract\u00e9ristiques de Spark"},{"location":"p2-spark/#limitations-de-spark","text":"Spark a plusieurs limitations, tel que : 1 Pas de support pour le traitement en temps r\u00e9el : Spark permet le traitement en temps-presque-r\u00e9el, car il utilise le traitement en micro-lot plut\u00f4t que le traitement en streaming. Probl\u00e8mes avec les fichiers de petite taille : Spark partitionne le traitement sur plusieurs ex\u00e9cuteurs, et est optimis\u00e9 principalement pour les grands volumes de donn\u00e9es. L'utiliser pour des fichiers de petite taille va rajouter un co\u00fbt suppl\u00e9mentaire, il est donc plus judicieux dans ce cas d'utiliser un traitement s\u00e9quentiel classique sur une seule machine. Pas de syst\u00e8me de gestion des fichiers : Spark est principalement un syst\u00e8me de traitement, et ne fournit pas de solution pour le stockage des donn\u00e9es. Il doit donc se baser sur d'autres syst\u00e8mes de stockage tel que Hadoop HDFS ou Amazon S3. Co\u00fbteux : En tant que syst\u00e8me de traitement en m\u00e9moire, le co\u00fbt d'ex\u00e9cuter Spark sur un cluster peut \u00eatre tr\u00e8s \u00e9lev\u00e9 en terme de consommation m\u00e9moire. Nombre d'algorithmes limit\u00e9 : Malgr\u00e9 la disponibilit\u00e9 de la biblioth\u00e8que MLlib, elle reste limit\u00e9e en termes de nombre d'algorithmes impl\u00e9ment\u00e9s. Latence : La latence de Spark pour l'ex\u00e9cution de Jobs \u00e0 la vol\u00e9e est plus \u00e9lev\u00e9e que d'autres solutions de traitement en streaming tel que Flink .","title":"Limitations de Spark"},{"location":"p2-spark/#references","text":"Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 02/2020 \u21a9 \u21a9 \u21a9 \u21a9 Spark Documentation, Cluster Mode Overview , https://spark.apache.org/docs/latest/cluster-overview.html , consult\u00e9 le 02/2020 \u21a9","title":"R\u00e9f\u00e9rences"},{"location":"p3-install/","text":"Partie 3 - Installation de Spark Installation de Spark sur un seul Noeud Pour installer Spark, nous allons utiliser des contenaires Docker . Docker nous permettra de mettre en place un environnement complet, enti\u00e8rement portable, sans rien installer sur la machine h\u00f4te, pour utiliser Spark de fa\u00e7on uniforme gr\u00e2ce aux lignes de commande. Nous allons suivre les \u00e9tapes suivantes pour installer l'environnement Spark sur une machine ubuntu. \u00c9tape 1 - T\u00e9l\u00e9charger l'image de base Avant de suivre les \u00e9tapes suivantes, il faut commencer par installer Docker. Suivre les \u00e9tapes se trouvant dans le lien suivant, suivant votre syst\u00e8me d'exploitation: https://docs.docker.com/install/ Nous avons choisi Ubuntu comme environnement cible pour notre contenaire Docker. Nous commen\u00e7ons donc par t\u00e9l\u00e9charger l'image Ubuntu \u00e0 partir de Docker Hub, avec la commande suivante: 1 docker pull ubuntu Nous allons ensuite cr\u00e9er un contenaire \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. 1 docker run - itd - p 8080 : 8080 -- name spark -- hostname spark ubuntu Nous avons lanc\u00e9 un nouveau contenaire intitul\u00e9 spark \u00e0 partir de la machine ubuntu, en exposant sur le localhost son port 8080, pour pouvoir acc\u00e9der \u00e0 sa WebURL. On pourra v\u00e9rifier que la machine est bien d\u00e9marr\u00e9e en utilisant: 1 docker ps On devrait obtenir un r\u00e9sultat semblable au suivant: Pour se connecter \u00e0 la machine et la manipuler avec les lignes de commandes, utiliser: 1 docker exec - it spark bash Le r\u00e9sultat sera comme suit: Attention Ces \u00e9tapes sont faites une seule fois, \u00e0 la premi\u00e8re cr\u00e9ation de la machine. Si vous voulez relancer une machine d\u00e9j\u00e0 cr\u00e9\u00e9e, suivre les \u00e9tapes suivantes: V\u00e9rifier que la machine n'est pas d\u00e9j\u00e0 d\u00e9marr\u00e9e. Pour cela, taper la commande suivante: 1 docker ps Si vous retrouvez le contenaire dans la liste affich\u00e9e, vous pouvez ex\u00e9cuter la commande docker exec ... pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Sinon, v\u00e9rifier que le contenaire existe bien, mais qu'il est juste stopp\u00e9, gr\u00e2ce \u00e0 la commande: 1 docker ps - a Une fois le contenaire retrouv\u00e9, le d\u00e9marrer, simplement en tapant la commande suivante: 1 docker start spark Le contenaire sera lanc\u00e9. \u00c9tape 2 - Installer Java Afin d'installer Java sur la machine, commencer par mettre \u00e0 jour les packages syst\u00e8mes de Ubuntu: 1 2 apt update apt - y upgrade Installer ensuite la version par d\u00e9faut de Java: 1 apt install default - jdk V\u00e9rifier la version de Java que vous venez d'installer: 1 java - version \u00c9tape 3 - Installer Scala Installer Scala : 1 apt install scala \u00c9tape 4 - T\u00e9l\u00e9charger Spark Pour installer Spark sur la machine docker, utiliser la commande suivante: 1 2 apt install curl curl - O https : //archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz La version stable actuelle est 2.4.5, mais vous pouvez t\u00e9l\u00e9charger la version de votre choix. Vous retrouverez les liens de t\u00e9l\u00e9chargement de toutes les versions ICI . Extraire ensuite le fichier tgz: 1 tar xvf spark - 2.4.5 - bin - hadoop2 .7 . tgz D\u00e9placer le dossier obtenu vers le r\u00e9pertoire /opt comme suit: 1 2 mv spark - 2.4.5 - bin - hadoop2 .7 / opt / spark rm spark - 2.4.5 - bin - hadoop2 .7 . tgz \u00c9tape 5 - Mise en place de l'environnement Spark Nous devons mettre en place certains param\u00e8tres d'environnement pour assurer une bonne ex\u00e9cution de Spark: Ouvrir le fichier de configuration bashrc (installer vim si n\u00e9cessaire avec apt install vim ) 1 vim ~/.bashrc 2. Ajouter les lignes suivantes \u00e0 la fin du fichier (taper G pour aller \u00e0 la fin du fichier, puis o pour ins\u00e9rer une nouvelle ligne et passer en mode \u00e9dition) 1 2 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin Quitter l'\u00e9diteur en tapant : wq Activer les changements r\u00e9alis\u00e9s en tapant ` source ~/ . bashrc \u00c9tape 6 - D\u00e9marrer un serveur master en standalone Il est d\u00e9sormais possible de d\u00e9marrer un serveur en standalone, en utilisant la commande suivante: 1 start-master.sh Vous pourrez ensuite v\u00e9rifier que votre serveur est bien d\u00e9marr\u00e9 en tapant: jps Il suffit de plus, d'aller sur le navigateur de votre machine h\u00f4te, et d'ouvrir le lien: http: //localhost:8080 (apr\u00e8s avoir v\u00e9rifi\u00e9 que rien d'autre ne tourne sur le m\u00eame port). L'interface Web de Spark s'affichera, comme suit: On remarque que la fen\u00eatre indique que le spark master se trouve sur spark: //spark:7077 \u00c9tape 7 - D\u00e9marrer un processus Worker Pour lancer un processus Worker, utiliser la commande suivante: 1 start-slave.sh spark://spark:7077 Un nouveau processus sera lanc\u00e9, qu'on pourra voir avec jps Vous pouvez maintenant lancer Spark Shell pour executer des Jobs Spark. 1 spark-shell Installation de Spark sur un cluster Nous allons maintenant proc\u00e9der \u00e0 l'installation de Spark sur un cluster, c'est \u00e0 dire un ensemble de machines interconnect\u00e9es, repr\u00e9sent\u00e9es dans notre cas par des contenaires Docker. L'objectif sera donc de cr\u00e9er un r\u00e9seau de contenaires, installer Spark dessus, et lancer les processus sur les diff\u00e9rents contenaires, de fa\u00e7on \u00e0 obtenir le cluster suivant: Pour r\u00e9aliser cela, nous allons nous baser sur le contenaire cr\u00e9\u00e9 pr\u00e9c\u00e9demment, dans lequel nous avons install\u00e9 Java et Spark. \u00c9tape 1 - Installer SSH Installer OpenSSH sur la machine : 1 apt install openssh-server openssh-client G\u00e9n\u00e9rer une paire de clefs (quand on vous le demande, valider le chemin par d\u00e9faut propos\u00e9 pour enregistrer la paire de clefs): 1 ssh-keygen -t rsa -P \"\" D\u00e9finir la clef g\u00e9n\u00e9r\u00e9e comme clef autoris\u00e9e: 1 cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys Programmer ssh pour qu'il soit lanc\u00e9 au d\u00e9marrage du contenaire. Pour cela, ajouter les lignes suivantes \u00e0 la fin du fichier ~/ . bashrc : 1 service ssh start \u00c9tape 2 - Configurer Spark Il faudrait \u00e9diter le fichier de configuration spark - env . sh (se trouvant dans le r\u00e9pertoire $ SPARK_HOME / conf ) pour ajouter les param\u00e8tres suivants: Cr\u00e9er une copie du template du fichier spark - env . sh et le renommer: 1 cp $SPARK_HOME /conf/spark-env.sh.template $SPARK_HOME /conf/spark-env.sh Ajouter les deux lignes suivantes \u00e0 la fin du fichier ~/ . bashrc (n'oubliez pas de le recharger apr\u00e8s modification avec source ~/ . bashrc ) 1 export SPARK_WORKER_CORES = 8 Cr\u00e9er le fichier de configuration slaves dans le r\u00e9pertoire $ SPARK_HOME / conf : 1 vim $SPARK_HOME /conf/slaves Ajouter dans le fichier slaves les noms des contenaires workers (que nous allons cr\u00e9er tout \u00e0 l'heure): 1 2 spark-slave1 spark-slave2 Vous avez configur\u00e9 Spark pour supporter deux esclaves ( workers ou slaves ) en plus du master. \u00c9tape 3 - Cr\u00e9er une image \u00e0 partir du contenaire Une fois le contenaire cr\u00e9\u00e9 et configur\u00e9 tel que pr\u00e9sent\u00e9 pr\u00e9c\u00e9demment, nous allons le dupliquer pour en cr\u00e9er un cluster. Mais d'abord, il faut cr\u00e9er une image du contenaire, de fa\u00e7on \u00e0 l'utiliser pour cr\u00e9er les deux autre contenaires. Commencer par quitter le noeud spark et retourner vers la machine h\u00f4te, en tapant exit . Taper la commande suivante pour cr\u00e9er une image \u00e0 partir du contenaire spark: 1 docker commit spark spark-image commit permet de cr\u00e9er une nouvelle image spark - image \u00e0 partir du contenaire spark . V\u00e9rifier que spark - image existe bien en tapant: docker images . \u00c9tape 4 - Cr\u00e9er le Cluster Pour cr\u00e9er le cluster \u00e0 partir de l'image d\u00e9j\u00e0 g\u00e9n\u00e9r\u00e9e, suivre les \u00e9tapes suivantes: Supprimer le contenaire spark pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 2 docker stop spark docker rm spark Cr\u00e9er un r\u00e9seau qui permettra de connecter les trois noeuds du cluster: 1 docker network create --driver = bridge spark-network Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): 1 2 3 4 5 6 7 8 9 10 11 docker run -itd --net = spark-network -p 8080:8080 --expose 22 \\ --name spark-master --hostname spark-master \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave1 --hostname spark-slave1 \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave2 --hostname spark-slave2 \\ spark-image V\u00e9rifier que les trois contenaires sont bien cr\u00e9\u00e9s: 1 docker ps Vous devriez retrouver la liste des trois contenaires: \u00c9tape 5 - D\u00e9marrer les services Spark Pour d\u00e9marrer les services spark sur tous les noeuds, utiliser la commande suivante: 1 start-all.sh Vous obtiendrez le r\u00e9sultat suivant: Pour v\u00e9rifier que les services sont bien d\u00e9marr\u00e9s, aller sur le noeud Master et taper la commande jps , vous trouverez le r\u00e9sultat suivant: Si on fait la m\u00eame chose sur un des slaves, on obtiendra le r\u00e9sultat suivant:","title":"P3 - Installation de Spark"},{"location":"p3-install/#partie-3-installation-de-spark","text":"","title":"Partie 3 - Installation de Spark"},{"location":"p3-install/#installation-de-spark-sur-un-seul-noeud","text":"Pour installer Spark, nous allons utiliser des contenaires Docker . Docker nous permettra de mettre en place un environnement complet, enti\u00e8rement portable, sans rien installer sur la machine h\u00f4te, pour utiliser Spark de fa\u00e7on uniforme gr\u00e2ce aux lignes de commande. Nous allons suivre les \u00e9tapes suivantes pour installer l'environnement Spark sur une machine ubuntu.","title":"Installation de Spark sur un seul Noeud"},{"location":"p3-install/#etape-1-telecharger-limage-de-base","text":"Avant de suivre les \u00e9tapes suivantes, il faut commencer par installer Docker. Suivre les \u00e9tapes se trouvant dans le lien suivant, suivant votre syst\u00e8me d'exploitation: https://docs.docker.com/install/ Nous avons choisi Ubuntu comme environnement cible pour notre contenaire Docker. Nous commen\u00e7ons donc par t\u00e9l\u00e9charger l'image Ubuntu \u00e0 partir de Docker Hub, avec la commande suivante: 1 docker pull ubuntu Nous allons ensuite cr\u00e9er un contenaire \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. 1 docker run - itd - p 8080 : 8080 -- name spark -- hostname spark ubuntu Nous avons lanc\u00e9 un nouveau contenaire intitul\u00e9 spark \u00e0 partir de la machine ubuntu, en exposant sur le localhost son port 8080, pour pouvoir acc\u00e9der \u00e0 sa WebURL. On pourra v\u00e9rifier que la machine est bien d\u00e9marr\u00e9e en utilisant: 1 docker ps On devrait obtenir un r\u00e9sultat semblable au suivant: Pour se connecter \u00e0 la machine et la manipuler avec les lignes de commandes, utiliser: 1 docker exec - it spark bash Le r\u00e9sultat sera comme suit: Attention Ces \u00e9tapes sont faites une seule fois, \u00e0 la premi\u00e8re cr\u00e9ation de la machine. Si vous voulez relancer une machine d\u00e9j\u00e0 cr\u00e9\u00e9e, suivre les \u00e9tapes suivantes: V\u00e9rifier que la machine n'est pas d\u00e9j\u00e0 d\u00e9marr\u00e9e. Pour cela, taper la commande suivante: 1 docker ps Si vous retrouvez le contenaire dans la liste affich\u00e9e, vous pouvez ex\u00e9cuter la commande docker exec ... pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Sinon, v\u00e9rifier que le contenaire existe bien, mais qu'il est juste stopp\u00e9, gr\u00e2ce \u00e0 la commande: 1 docker ps - a Une fois le contenaire retrouv\u00e9, le d\u00e9marrer, simplement en tapant la commande suivante: 1 docker start spark Le contenaire sera lanc\u00e9.","title":"\u00c9tape 1 - T\u00e9l\u00e9charger l'image de base"},{"location":"p3-install/#etape-2-installer-java","text":"Afin d'installer Java sur la machine, commencer par mettre \u00e0 jour les packages syst\u00e8mes de Ubuntu: 1 2 apt update apt - y upgrade Installer ensuite la version par d\u00e9faut de Java: 1 apt install default - jdk V\u00e9rifier la version de Java que vous venez d'installer: 1 java - version","title":"\u00c9tape 2 - Installer Java"},{"location":"p3-install/#etape-3-installer-scala","text":"Installer Scala : 1 apt install scala","title":"\u00c9tape 3 - Installer Scala"},{"location":"p3-install/#etape-4-telecharger-spark","text":"Pour installer Spark sur la machine docker, utiliser la commande suivante: 1 2 apt install curl curl - O https : //archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz La version stable actuelle est 2.4.5, mais vous pouvez t\u00e9l\u00e9charger la version de votre choix. Vous retrouverez les liens de t\u00e9l\u00e9chargement de toutes les versions ICI . Extraire ensuite le fichier tgz: 1 tar xvf spark - 2.4.5 - bin - hadoop2 .7 . tgz D\u00e9placer le dossier obtenu vers le r\u00e9pertoire /opt comme suit: 1 2 mv spark - 2.4.5 - bin - hadoop2 .7 / opt / spark rm spark - 2.4.5 - bin - hadoop2 .7 . tgz","title":"\u00c9tape 4 - T\u00e9l\u00e9charger Spark"},{"location":"p3-install/#etape-5-mise-en-place-de-lenvironnement-spark","text":"Nous devons mettre en place certains param\u00e8tres d'environnement pour assurer une bonne ex\u00e9cution de Spark: Ouvrir le fichier de configuration bashrc (installer vim si n\u00e9cessaire avec apt install vim ) 1 vim ~/.bashrc 2. Ajouter les lignes suivantes \u00e0 la fin du fichier (taper G pour aller \u00e0 la fin du fichier, puis o pour ins\u00e9rer une nouvelle ligne et passer en mode \u00e9dition) 1 2 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin Quitter l'\u00e9diteur en tapant : wq Activer les changements r\u00e9alis\u00e9s en tapant ` source ~/ . bashrc","title":"\u00c9tape 5 - Mise en place de l'environnement Spark"},{"location":"p3-install/#etape-6-demarrer-un-serveur-master-en-standalone","text":"Il est d\u00e9sormais possible de d\u00e9marrer un serveur en standalone, en utilisant la commande suivante: 1 start-master.sh Vous pourrez ensuite v\u00e9rifier que votre serveur est bien d\u00e9marr\u00e9 en tapant: jps Il suffit de plus, d'aller sur le navigateur de votre machine h\u00f4te, et d'ouvrir le lien: http: //localhost:8080 (apr\u00e8s avoir v\u00e9rifi\u00e9 que rien d'autre ne tourne sur le m\u00eame port). L'interface Web de Spark s'affichera, comme suit: On remarque que la fen\u00eatre indique que le spark master se trouve sur spark: //spark:7077","title":"\u00c9tape 6 - D\u00e9marrer un serveur master en standalone"},{"location":"p3-install/#etape-7-demarrer-un-processus-worker","text":"Pour lancer un processus Worker, utiliser la commande suivante: 1 start-slave.sh spark://spark:7077 Un nouveau processus sera lanc\u00e9, qu'on pourra voir avec jps Vous pouvez maintenant lancer Spark Shell pour executer des Jobs Spark. 1 spark-shell","title":"\u00c9tape 7 - D\u00e9marrer un processus Worker"},{"location":"p3-install/#installation-de-spark-sur-un-cluster","text":"Nous allons maintenant proc\u00e9der \u00e0 l'installation de Spark sur un cluster, c'est \u00e0 dire un ensemble de machines interconnect\u00e9es, repr\u00e9sent\u00e9es dans notre cas par des contenaires Docker. L'objectif sera donc de cr\u00e9er un r\u00e9seau de contenaires, installer Spark dessus, et lancer les processus sur les diff\u00e9rents contenaires, de fa\u00e7on \u00e0 obtenir le cluster suivant: Pour r\u00e9aliser cela, nous allons nous baser sur le contenaire cr\u00e9\u00e9 pr\u00e9c\u00e9demment, dans lequel nous avons install\u00e9 Java et Spark.","title":"Installation de Spark sur un cluster"},{"location":"p3-install/#etape-1-installer-ssh","text":"Installer OpenSSH sur la machine : 1 apt install openssh-server openssh-client G\u00e9n\u00e9rer une paire de clefs (quand on vous le demande, valider le chemin par d\u00e9faut propos\u00e9 pour enregistrer la paire de clefs): 1 ssh-keygen -t rsa -P \"\" D\u00e9finir la clef g\u00e9n\u00e9r\u00e9e comme clef autoris\u00e9e: 1 cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys Programmer ssh pour qu'il soit lanc\u00e9 au d\u00e9marrage du contenaire. Pour cela, ajouter les lignes suivantes \u00e0 la fin du fichier ~/ . bashrc : 1 service ssh start","title":"\u00c9tape 1 - Installer SSH"},{"location":"p3-install/#etape-2-configurer-spark","text":"Il faudrait \u00e9diter le fichier de configuration spark - env . sh (se trouvant dans le r\u00e9pertoire $ SPARK_HOME / conf ) pour ajouter les param\u00e8tres suivants: Cr\u00e9er une copie du template du fichier spark - env . sh et le renommer: 1 cp $SPARK_HOME /conf/spark-env.sh.template $SPARK_HOME /conf/spark-env.sh Ajouter les deux lignes suivantes \u00e0 la fin du fichier ~/ . bashrc (n'oubliez pas de le recharger apr\u00e8s modification avec source ~/ . bashrc ) 1 export SPARK_WORKER_CORES = 8 Cr\u00e9er le fichier de configuration slaves dans le r\u00e9pertoire $ SPARK_HOME / conf : 1 vim $SPARK_HOME /conf/slaves Ajouter dans le fichier slaves les noms des contenaires workers (que nous allons cr\u00e9er tout \u00e0 l'heure): 1 2 spark-slave1 spark-slave2 Vous avez configur\u00e9 Spark pour supporter deux esclaves ( workers ou slaves ) en plus du master.","title":"\u00c9tape 2 - Configurer Spark"},{"location":"p3-install/#etape-3-creer-une-image-a-partir-du-contenaire","text":"Une fois le contenaire cr\u00e9\u00e9 et configur\u00e9 tel que pr\u00e9sent\u00e9 pr\u00e9c\u00e9demment, nous allons le dupliquer pour en cr\u00e9er un cluster. Mais d'abord, il faut cr\u00e9er une image du contenaire, de fa\u00e7on \u00e0 l'utiliser pour cr\u00e9er les deux autre contenaires. Commencer par quitter le noeud spark et retourner vers la machine h\u00f4te, en tapant exit . Taper la commande suivante pour cr\u00e9er une image \u00e0 partir du contenaire spark: 1 docker commit spark spark-image commit permet de cr\u00e9er une nouvelle image spark - image \u00e0 partir du contenaire spark . V\u00e9rifier que spark - image existe bien en tapant: docker images .","title":"\u00c9tape 3 - Cr\u00e9er une image \u00e0 partir du contenaire"},{"location":"p3-install/#etape-4-creer-le-cluster","text":"Pour cr\u00e9er le cluster \u00e0 partir de l'image d\u00e9j\u00e0 g\u00e9n\u00e9r\u00e9e, suivre les \u00e9tapes suivantes: Supprimer le contenaire spark pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 2 docker stop spark docker rm spark Cr\u00e9er un r\u00e9seau qui permettra de connecter les trois noeuds du cluster: 1 docker network create --driver = bridge spark-network Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): 1 2 3 4 5 6 7 8 9 10 11 docker run -itd --net = spark-network -p 8080:8080 --expose 22 \\ --name spark-master --hostname spark-master \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave1 --hostname spark-slave1 \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave2 --hostname spark-slave2 \\ spark-image V\u00e9rifier que les trois contenaires sont bien cr\u00e9\u00e9s: 1 docker ps Vous devriez retrouver la liste des trois contenaires:","title":"\u00c9tape 4 - Cr\u00e9er le Cluster"},{"location":"p3-install/#etape-5-demarrer-les-services-spark","text":"Pour d\u00e9marrer les services spark sur tous les noeuds, utiliser la commande suivante: 1 start-all.sh Vous obtiendrez le r\u00e9sultat suivant: Pour v\u00e9rifier que les services sont bien d\u00e9marr\u00e9s, aller sur le noeud Master et taper la commande jps , vous trouverez le r\u00e9sultat suivant: Si on fait la m\u00eame chose sur un des slaves, on obtiendra le r\u00e9sultat suivant:","title":"\u00c9tape 5 - D\u00e9marrer les services Spark"},{"location":"p4-batch/","text":"Partie 4 - Spark RDD et Traitement par Lots 1 RDD: Resilient Distributed Dataset Spark gravite autour du concept de \"Resilient Distributed Dataset\" ou RDD, qui est une collection d'\u00e9l\u00e9ments tol\u00e9rante aux fautes qui peut \u00eatre g\u00e9r\u00e9e en parall\u00e8le. Les RDDs utilisent la m\u00e9moire et l'espace disque selon les besoins. R pour R\u00e9silient : capable de r\u00e9cup\u00e9rer rapidement en cas de probl\u00e8mes ou de conditions difficiles, D pour Distribu\u00e9 : partage les donn\u00e9es sur les diff\u00e9rents noeuds participants pour une ex\u00e9cution parall\u00e8le, D pour Dataset : une collection de donn\u00e9es compos\u00e9e d'\u00e9l\u00e9ments s\u00e9par\u00e9s mais qui sont manipul\u00e9s comme une unit\u00e9 compacte. Il existe deux moyens de cr\u00e9er les RDDs 2 : Parall\u00e9liser une collection existante en m\u00e9moire dans le programme Driver. Le g\u00e9n\u00e9rer \u00e0 partir d'un fichier enregistr\u00e9 sur un support de stockage externe. Parall\u00e9lisation de Collections Les collections parall\u00e9lis\u00e9es sont cr\u00e9\u00e9es en appelant la m\u00e9thode parallelize du JavaSparkContext sur une collection existante dans votre programme Driver. Les \u00e9l\u00e9ments de la collection sont copi\u00e9s pour former une structure distribu\u00e9e qui peut \u00eatre trait\u00e9e en parall\u00e8le. Par exemple, nous pouvons cr\u00e9er une collection parall\u00e9lis\u00e9e \u00e0 partir d'une liste contenant les chiffres de 1 \u00e0 5: 1 2 List < Integer > data = Arrays.asList ( 1 , 2 , 3 , 4 , 5 ); JavaRDD < Integer > distData = sc.parallelize ( data ); Une fois cr\u00e9\u00e9e, la structure distribu\u00e9e distData peut \u00eatre trait\u00e9e en parall\u00e8le. Par exemple, il est possible d'appeler distData . reduce (( a , b ) -> a + b ) pour faire la somme de tous les \u00e9l\u00e9ments de la liste. Un param\u00e8tre important \u00e0 d\u00e9finir dans une collection parall\u00e9lis\u00e9e, c'est le nombre de partitions \u00e0 utiliser pour diviser la collection. Spark ex\u00e9cute une t\u00e2che pour chaque partition du cluster. En temps normal, Spark essaiera de d\u00e9finir le nombre de partitions automatiquement selon votre cluster, cependant, il est possible de le d\u00e9finir manuellement en le passant comme second param\u00e8tre de la fonction parallelize : 1 sc . parallelize ( data , 10 ) G\u00e9n\u00e9ration \u00e0 partir d'un fichier externe Spark peut cr\u00e9er une collection distribu\u00e9e \u00e0 partir de n'importe quelle source de stockage support\u00e9e par Hadoop, incluant votre propre syst\u00e8me de stockage, HDFS, Cassandra, HBase, Amazon S3, etc. Il est possible de cr\u00e9er des RDDs \u00e0 partir de fichiers texte en utilisant la m\u00e9thode textfile du SparkContext . Cette m\u00e9thode prend l'URI du fichier (chemin du fichier local, ou bien en utilisant hdfs: // ou s3: // ), et le lit comme une collection de lignes. Par exemple: 1 JavaRDD < String > distFile = sc.textFile ( \"data.txt\" ); Op\u00e9rations sur les RDDs Les RDDs supportent deux types d'op\u00e9rations: les transformations , qui permettent de cr\u00e9er une nouvelle collection \u00e0 partir d'un RDD existant les actions , qui retournent une valeur au programme driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le RDD. Par exemple, un map est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fonction, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un reduce est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme. 3 Caract\u00e9ristiques des RDDs Immutabilit\u00e9 et Lign\u00e9e Les RDDs dans Spark sont des collection immuables , c'est \u00e0 dire qu'elle ne sont jamais modifi\u00e9es: toute transformation va cr\u00e9er un nouvel RDD au lieu de modifier le RDD initial. Quand un nouvel RDD a \u00e9t\u00e9 cr\u00e9\u00e9 \u00e0 partir d'un RDD existant, ce nouvel RDD contient un pointeur vers le RDD parent. De m\u00eame, toutes les d\u00e9pendances entre les RDDs sont logg\u00e9es dans un graphe, plut\u00f4t que directement sur les donn\u00e9es. Ce graphe s'appelle Graphe de Lign\u00e9e ou Lineage Graph . Par exemple, si on consid\u00e8re les op\u00e9rations suivantes: Cr\u00e9er un nouvel RDD \u00e0 partir d'un fichier texte -> RDD1 Appliquer des op\u00e9rations de Map sur RDD1 -> RDD2 Appliquer une op\u00e9ration de filtrage sur RDD2 -> RDD3 Appliquer une op\u00e9ration de comptage sur RDD3 -> RDD4 Le graphe de lign\u00e9e associ\u00e9 \u00e0 ces op\u00e9rations ressemble \u00e0 ce qui suit: Ce graphe peut \u00eatre utile au cas o\u00f9 certaines partitions sont perdues. Spark peut rejouer la transformation sur cette partition en utilisant le graphe de lign\u00e9e existant pour r\u00e9aliser le m\u00eame calcul, plut\u00f4t que de r\u00e9pliquer les donn\u00e9es \u00e0 partir de des diff\u00e9rents noeuds du cluster. Il est \u00e9galement utile en cas de r\u00e9utilisation d'un graphe existant. Si par exemple on d\u00e9sire appliquer une op\u00e9ration tri sur RDD2, il est inutile de recharger le fichier une deuxi\u00e8me fois \u00e0 partir du disque. Il suffit de modifier le graphe pour qu'il devienne comme suit: Lazy Evaluation Toutes les transformations dans Spark sont lazy (fain\u00e9antes), car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement. Exemple L'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9: 1 2 //Etape 1 - Cr\u00e9er un RDD \u00e0 partir d'un fichier texte val docs = spark . textFile ( \"/docs\" ) 1 2 //Etape 2 - Convertir les lignes en minuscule val lower = docs . map ( line => line . toLowerCase ) 1 2 //Etape 3 - S\u00e9parer les lignes en mots val words = lower . flatMap ( line => line . split ( \" \\\\ s+\" )) 1 2 //Etape 4 - produire les tuples (mot, 1) val counts = words . map ( word => ( word , 1 )) 1 2 //Etape 5 - Compter tous les mots val freq = counts . reduceByKey ( _ + _ ) 1 2 //Etape 6 - Inverser les tuples (transformation avec swap) freq . map ( _ . swap ) 1 2 //Etape 7 - Inverser les tuples (action de s\u00e9lection des n premiers) val top = freq . map ( _swap ). top ( N ) Test de Spark avec Spark-Shell Nous allons tester le comportement de Spark et des RDD en utilisant l'exemple type pour l'analyse des donn\u00e9es: le Wordcount, qui permet de compter le nombre de mots dans un fichier donn\u00e9 en entr\u00e9e. Commen\u00e7ons par lancer le cluster Spark install\u00e9 dans la partie P3 . 1 docker start spark-master spark-slave1 spark-slave2 Entrer dans le noeud Master comme suit: 1 docker exec -it spark-master bash Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier input/file1.txt dans le r\u00e9pertoire / root : 1 2 mkdir /root/input vim /root/input/file1.txt Remplir le fichier avec le texte suivant, ou tout texte de votre choix (vous devez taper i pour passer en mode \u00e9dition): 1 2 Hello Spark Wordcount! Hello everybody else! Lancer Spark Shell en utilisant la commande suivante: 1 spark - shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne): 1 2 3 4 val lines = sc . textFile ( \"/root/input/file1.txt\" ) val words = lines . flatMap ( _ . split ( \" \\\\ s+\" )) val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) wc . saveAsTextFile ( \"/root/file1.count\" ) Ce code vient de (1) charger le fichier file1.txt du syst\u00e8me de fichier courant, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . Afficher ensuite le contenu du fichier part-00000 du r\u00e9pertoire file1.count cr\u00e9\u00e9, comme suit: 1 cat / root / file1 . count / part - 00000 Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit: Spark Batch en Java Pr\u00e9paration de l'environnement et Code Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Sur votre propre machine, cr\u00e9er un projet Maven avec IntelliJ IDEA (ou tout IDE de votre choix), en utilisant la config suivante: 1 2 3 <groupId> spark.batch </groupId> <artifactId> wordcount </artifactId> <version> 1 </version> Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: 1 2 3 4 5 6 7 8 9 10 11 12 <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.12 </artifactId> <version> 2.4.5 </version> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez tn.spark.batch , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package tn . spark . batch ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import static jersey . repackaged . com . google . common . base . Preconditions . checkArgument ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ], args [ 1 ]); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <>( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell. Test du code en local Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Cr\u00e9er un fichier texte de votre choix (par exemple le fameux loremipsum.txt, que vous pourrez g\u00e9n\u00e9rer ici ) dans le r\u00e9pertoire src/main/resources. Cr\u00e9er une nouvelle configuration de type \"Application\" ( Run->Edit Configurations ): La nommer WordCountTask , et d\u00e9finir les arguments suivants (fichier de d\u00e9part et r\u00e9pertoire d'arriv\u00e9e) comme Program arguments : 1 src / main / resources / loremipsum . txt src / main / resources / out Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient deux fichiers: part-00000, part-00001. Lancement du code sur le cluster Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ], args [ 1 ]); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <>( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec les commandes package install . Un fichier intitul\u00e9 worcount-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target . Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante: 1 docker cp target / wordcount - 1. jar spark - master :/ root / wordcount - 1. jar Copier \u00e9galement le fichier loremipsum.txt que vous avez cr\u00e9\u00e9 dans votre projet: 1 docker cp src / main / resources / loremipsum . txt spark - master :/ root / input / loremipsum . txt Aller \u00e0 votre contenaire spark-master (en utilisant la commande docker exec ... ), et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark - submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. 1 2 cd / root spark - submit -- class tn . spark . batch . WordCountTask -- master local wordcount - 1. jar input / loremipsum . txt output Nous avons lanc\u00e9 le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier loremipsum.txt, et le r\u00e9sultat sera stock\u00e9 dans un r\u00e9pertoire output . Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire output , un fichier part-00000, qui ressemble \u00e0 ce qui suit: R\u00e9f\u00e9rences Spark for beginners, RDD in Spark , http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html , consult\u00e9 le 03/2020 \u21a9 Spark Documentation, Resilient Distributed Datasets(RDDs) , https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds , consult\u00e9 le 03/2020 \u21a9 Devopedia, Apache Spark , https://devopedia.org/apache-spark , consult\u00e9 le 03/2020 \u21a9","title":"P4 - RDD et Batch Processing avec Spark"},{"location":"p4-batch/#partie-4-spark-rdd-et-traitement-par-lots","text":"1","title":"Partie 4 - Spark RDD et Traitement par Lots"},{"location":"p4-batch/#rdd-resilient-distributed-dataset","text":"Spark gravite autour du concept de \"Resilient Distributed Dataset\" ou RDD, qui est une collection d'\u00e9l\u00e9ments tol\u00e9rante aux fautes qui peut \u00eatre g\u00e9r\u00e9e en parall\u00e8le. Les RDDs utilisent la m\u00e9moire et l'espace disque selon les besoins. R pour R\u00e9silient : capable de r\u00e9cup\u00e9rer rapidement en cas de probl\u00e8mes ou de conditions difficiles, D pour Distribu\u00e9 : partage les donn\u00e9es sur les diff\u00e9rents noeuds participants pour une ex\u00e9cution parall\u00e8le, D pour Dataset : une collection de donn\u00e9es compos\u00e9e d'\u00e9l\u00e9ments s\u00e9par\u00e9s mais qui sont manipul\u00e9s comme une unit\u00e9 compacte. Il existe deux moyens de cr\u00e9er les RDDs 2 : Parall\u00e9liser une collection existante en m\u00e9moire dans le programme Driver. Le g\u00e9n\u00e9rer \u00e0 partir d'un fichier enregistr\u00e9 sur un support de stockage externe.","title":"RDD: Resilient Distributed Dataset"},{"location":"p4-batch/#parallelisation-de-collections","text":"Les collections parall\u00e9lis\u00e9es sont cr\u00e9\u00e9es en appelant la m\u00e9thode parallelize du JavaSparkContext sur une collection existante dans votre programme Driver. Les \u00e9l\u00e9ments de la collection sont copi\u00e9s pour former une structure distribu\u00e9e qui peut \u00eatre trait\u00e9e en parall\u00e8le. Par exemple, nous pouvons cr\u00e9er une collection parall\u00e9lis\u00e9e \u00e0 partir d'une liste contenant les chiffres de 1 \u00e0 5: 1 2 List < Integer > data = Arrays.asList ( 1 , 2 , 3 , 4 , 5 ); JavaRDD < Integer > distData = sc.parallelize ( data ); Une fois cr\u00e9\u00e9e, la structure distribu\u00e9e distData peut \u00eatre trait\u00e9e en parall\u00e8le. Par exemple, il est possible d'appeler distData . reduce (( a , b ) -> a + b ) pour faire la somme de tous les \u00e9l\u00e9ments de la liste. Un param\u00e8tre important \u00e0 d\u00e9finir dans une collection parall\u00e9lis\u00e9e, c'est le nombre de partitions \u00e0 utiliser pour diviser la collection. Spark ex\u00e9cute une t\u00e2che pour chaque partition du cluster. En temps normal, Spark essaiera de d\u00e9finir le nombre de partitions automatiquement selon votre cluster, cependant, il est possible de le d\u00e9finir manuellement en le passant comme second param\u00e8tre de la fonction parallelize : 1 sc . parallelize ( data , 10 )","title":"Parall\u00e9lisation de Collections"},{"location":"p4-batch/#generation-a-partir-dun-fichier-externe","text":"Spark peut cr\u00e9er une collection distribu\u00e9e \u00e0 partir de n'importe quelle source de stockage support\u00e9e par Hadoop, incluant votre propre syst\u00e8me de stockage, HDFS, Cassandra, HBase, Amazon S3, etc. Il est possible de cr\u00e9er des RDDs \u00e0 partir de fichiers texte en utilisant la m\u00e9thode textfile du SparkContext . Cette m\u00e9thode prend l'URI du fichier (chemin du fichier local, ou bien en utilisant hdfs: // ou s3: // ), et le lit comme une collection de lignes. Par exemple: 1 JavaRDD < String > distFile = sc.textFile ( \"data.txt\" );","title":"G\u00e9n\u00e9ration \u00e0 partir d'un fichier externe"},{"location":"p4-batch/#operations-sur-les-rdds","text":"Les RDDs supportent deux types d'op\u00e9rations: les transformations , qui permettent de cr\u00e9er une nouvelle collection \u00e0 partir d'un RDD existant les actions , qui retournent une valeur au programme driver apr\u00e8s avoir ex\u00e9cut\u00e9 un calcul sur le RDD. Par exemple, un map est une transformation qui passe chaque \u00e9l\u00e9ment du dataset via une fonction, et retourne un nouvel RDD repr\u00e9sentant les r\u00e9sultats. Un reduce est une action qui agr\u00e8ge tous les \u00e9l\u00e9ments du RDD en utilisant une certaine fonction et retourne le r\u00e9sultat final au programme. 3","title":"Op\u00e9rations sur les RDDs"},{"location":"p4-batch/#caracteristiques-des-rdds","text":"","title":"Caract\u00e9ristiques des RDDs"},{"location":"p4-batch/#immutabilite-et-lignee","text":"Les RDDs dans Spark sont des collection immuables , c'est \u00e0 dire qu'elle ne sont jamais modifi\u00e9es: toute transformation va cr\u00e9er un nouvel RDD au lieu de modifier le RDD initial. Quand un nouvel RDD a \u00e9t\u00e9 cr\u00e9\u00e9 \u00e0 partir d'un RDD existant, ce nouvel RDD contient un pointeur vers le RDD parent. De m\u00eame, toutes les d\u00e9pendances entre les RDDs sont logg\u00e9es dans un graphe, plut\u00f4t que directement sur les donn\u00e9es. Ce graphe s'appelle Graphe de Lign\u00e9e ou Lineage Graph . Par exemple, si on consid\u00e8re les op\u00e9rations suivantes: Cr\u00e9er un nouvel RDD \u00e0 partir d'un fichier texte -> RDD1 Appliquer des op\u00e9rations de Map sur RDD1 -> RDD2 Appliquer une op\u00e9ration de filtrage sur RDD2 -> RDD3 Appliquer une op\u00e9ration de comptage sur RDD3 -> RDD4 Le graphe de lign\u00e9e associ\u00e9 \u00e0 ces op\u00e9rations ressemble \u00e0 ce qui suit: Ce graphe peut \u00eatre utile au cas o\u00f9 certaines partitions sont perdues. Spark peut rejouer la transformation sur cette partition en utilisant le graphe de lign\u00e9e existant pour r\u00e9aliser le m\u00eame calcul, plut\u00f4t que de r\u00e9pliquer les donn\u00e9es \u00e0 partir de des diff\u00e9rents noeuds du cluster. Il est \u00e9galement utile en cas de r\u00e9utilisation d'un graphe existant. Si par exemple on d\u00e9sire appliquer une op\u00e9ration tri sur RDD2, il est inutile de recharger le fichier une deuxi\u00e8me fois \u00e0 partir du disque. Il suffit de modifier le graphe pour qu'il devienne comme suit:","title":"Immutabilit\u00e9 et Lign\u00e9e"},{"location":"p4-batch/#lazy-evaluation","text":"Toutes les transformations dans Spark sont lazy (fain\u00e9antes), car elles ne calculent pas le r\u00e9sultat imm\u00e9diatement. Elles se souviennent des transformations appliqu\u00e9es \u00e0 un dataset de base (par ex. un fichier). Les transformations ne sont calcul\u00e9es que quand une action n\u00e9cessite qu'un r\u00e9sultat soit retourn\u00e9 au programme principal. Cela permet \u00e0 Spark de s'ex\u00e9cuter plus efficacement.","title":"Lazy Evaluation"},{"location":"p4-batch/#exemple","text":"L'exemple que nous allons pr\u00e9senter ici par \u00e9tapes permet de relever les mots les plus fr\u00e9quents dans un fichier. Pour cela, le code suivant est utilis\u00e9: 1 2 //Etape 1 - Cr\u00e9er un RDD \u00e0 partir d'un fichier texte val docs = spark . textFile ( \"/docs\" ) 1 2 //Etape 2 - Convertir les lignes en minuscule val lower = docs . map ( line => line . toLowerCase ) 1 2 //Etape 3 - S\u00e9parer les lignes en mots val words = lower . flatMap ( line => line . split ( \" \\\\ s+\" )) 1 2 //Etape 4 - produire les tuples (mot, 1) val counts = words . map ( word => ( word , 1 )) 1 2 //Etape 5 - Compter tous les mots val freq = counts . reduceByKey ( _ + _ ) 1 2 //Etape 6 - Inverser les tuples (transformation avec swap) freq . map ( _ . swap ) 1 2 //Etape 7 - Inverser les tuples (action de s\u00e9lection des n premiers) val top = freq . map ( _swap ). top ( N )","title":"Exemple"},{"location":"p4-batch/#test-de-spark-avec-spark-shell","text":"Nous allons tester le comportement de Spark et des RDD en utilisant l'exemple type pour l'analyse des donn\u00e9es: le Wordcount, qui permet de compter le nombre de mots dans un fichier donn\u00e9 en entr\u00e9e. Commen\u00e7ons par lancer le cluster Spark install\u00e9 dans la partie P3 . 1 docker start spark-master spark-slave1 spark-slave2 Entrer dans le noeud Master comme suit: 1 docker exec -it spark-master bash Dans le but de tester l'ex\u00e9cution de spark, commencer par cr\u00e9er un fichier input/file1.txt dans le r\u00e9pertoire / root : 1 2 mkdir /root/input vim /root/input/file1.txt Remplir le fichier avec le texte suivant, ou tout texte de votre choix (vous devez taper i pour passer en mode \u00e9dition): 1 2 Hello Spark Wordcount! Hello everybody else! Lancer Spark Shell en utilisant la commande suivante: 1 spark - shell Vous devriez avoir un r\u00e9sultat semblable au suivant: Vous pourrez tester spark avec un code scala simple comme suit (\u00e0 ex\u00e9cuter ligne par ligne): 1 2 3 4 val lines = sc . textFile ( \"/root/input/file1.txt\" ) val words = lines . flatMap ( _ . split ( \" \\\\ s+\" )) val wc = words . map ( w => ( w , 1 )). reduceByKey ( _ + _ ) wc . saveAsTextFile ( \"/root/file1.count\" ) Ce code vient de (1) charger le fichier file1.txt du syst\u00e8me de fichier courant, (2) s\u00e9parer les mots selon les caract\u00e8res d'espacement, (3) appliquer un map sur les mots obtenus qui produit le couple ( <mot> , 1), puis un reduce qui permet de faire la somme des 1 des mots identiques. Pour afficher le r\u00e9sultat, sortir de spark-shell en cliquant sur Ctrl-C . Afficher ensuite le contenu du fichier part-00000 du r\u00e9pertoire file1.count cr\u00e9\u00e9, comme suit: 1 cat / root / file1 . count / part - 00000 Le contenu des deux fichiers part-00000 et part-00001 ressemble \u00e0 ce qui suit:","title":"Test de Spark avec Spark-Shell"},{"location":"p4-batch/#spark-batch-en-java","text":"","title":"Spark Batch en Java"},{"location":"p4-batch/#preparation-de-lenvironnement-et-code","text":"Nous allons dans cette partie cr\u00e9er un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job. Sur votre propre machine, cr\u00e9er un projet Maven avec IntelliJ IDEA (ou tout IDE de votre choix), en utilisant la config suivante: 1 2 3 <groupId> spark.batch </groupId> <artifactId> wordcount </artifactId> <version> 1 </version> Rajouter dans le fichier pom les d\u00e9pendances n\u00e9cessaires, et indiquer la version du compilateur Java: 1 2 3 4 5 6 7 8 9 10 11 12 <properties> <maven.compiler.source> 1.8 </maven.compiler.source> <maven.compiler.target> 1.8 </maven.compiler.target> </properties> <dependencies> <!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --> <dependency> <groupId> org.apache.spark </groupId> <artifactId> spark-core_2.12 </artifactId> <version> 2.4.5 </version> </dependency> </dependencies> Sous le r\u00e9pertoire java, cr\u00e9er un package que vous appellerez tn.spark.batch , et dedans, une classe appel\u00e9e WordCountTask . \u00c9crire le code suivant dans WordCountTask : 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 package tn . spark . batch ; import org.apache.spark.SparkConf ; import org.apache.spark.api.java.JavaPairRDD ; import org.apache.spark.api.java.JavaRDD ; import org.apache.spark.api.java.JavaSparkContext ; import org.slf4j.Logger ; import org.slf4j.LoggerFactory ; import scala.Tuple2 ; import java.util.Arrays ; import static jersey . repackaged . com . google . common . base . Preconditions . checkArgument ; public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ], args [ 1 ]); } public void run ( String inputFilePath , String outputDir ) { String master = \"local[*]\" ; SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()) . setMaster ( master ); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <>( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } La premi\u00e8re chose \u00e0 faire dans un programme Spark est de cr\u00e9er un objet JavaSparkContext , qui indique \u00e0 Spark comment acc\u00e9der \u00e0 un cluster. Pour cr\u00e9er ce contexte, vous aurez besoin de construire un objet SparkConf qui contient toutes les informations sur l'application. appName est le nom de l'application master est une URL d'un cluster Spark, Mesos ou YARN, ou bien une cha\u00eene sp\u00e9ciale local pour lancer le job en mode local. Warning Nous avons indiqu\u00e9 ici que notre master est local pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet d\u00e9conseill\u00e9 de la hard-coder dans le programme, il faudrait plut\u00f4t l'indiquer comme option de commande \u00e0 chaque fois que nous lan\u00e7ons le job. Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.","title":"Pr\u00e9paration de l'environnement et Code"},{"location":"p4-batch/#test-du-code-en-local","text":"Pour tester le code sur votre machine, proc\u00e9der aux \u00e9tapes suivantes: Cr\u00e9er un fichier texte de votre choix (par exemple le fameux loremipsum.txt, que vous pourrez g\u00e9n\u00e9rer ici ) dans le r\u00e9pertoire src/main/resources. Cr\u00e9er une nouvelle configuration de type \"Application\" ( Run->Edit Configurations ): La nommer WordCountTask , et d\u00e9finir les arguments suivants (fichier de d\u00e9part et r\u00e9pertoire d'arriv\u00e9e) comme Program arguments : 1 src / main / resources / loremipsum . txt src / main / resources / out Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un r\u00e9pertoire out sera cr\u00e9\u00e9 sous resources , qui contient deux fichiers: part-00000, part-00001.","title":"Test du code en local"},{"location":"p4-batch/#lancement-du-code-sur-le-cluster","text":"Pour ex\u00e9cuter le code sur le cluster, modifier comme indiqu\u00e9 les lignes en jaune dans ce qui suit: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 public class WordCountTask { private static final Logger LOGGER = LoggerFactory . getLogger ( WordCountTask . class ); public static void main ( String [] args ) { checkArgument ( args . length > 1 , \"Please provide the path of input file and output dir as parameters.\" ); new WordCountTask (). run ( args [ 0 ], args [ 1 ]); } public void run ( String inputFilePath , String outputDir ) { SparkConf conf = new SparkConf () . setAppName ( WordCountTask . class . getName ()); JavaSparkContext sc = new JavaSparkContext ( conf ); JavaRDD < String > textFile = sc . textFile ( inputFilePath ); JavaPairRDD < String , Integer > counts = textFile . flatMap ( s -> Arrays . asList ( s . split ( \" \" )). iterator ()) . mapToPair ( word -> new Tuple2 <>( word , 1 )) . reduceByKey (( a , b ) -> a + b ); counts . saveAsTextFile ( outputDir ); } } Lancer ensuite une configuration de type Maven, avec les commandes package install . Un fichier intitul\u00e9 worcount-1.jar sera cr\u00e9\u00e9 sous le r\u00e9pertoire target . Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le r\u00e9pertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante: 1 docker cp target / wordcount - 1. jar spark - master :/ root / wordcount - 1. jar Copier \u00e9galement le fichier loremipsum.txt que vous avez cr\u00e9\u00e9 dans votre projet: 1 docker cp src / main / resources / loremipsum . txt spark - master :/ root / input / loremipsum . txt Aller \u00e0 votre contenaire spark-master (en utilisant la commande docker exec ... ), et lancer un job Spark en utilisant ce fichier jar g\u00e9n\u00e9r\u00e9, avec la commande spark - submit , un script utilis\u00e9 pour lancer des applications spark sur un cluster. 1 2 cd / root spark - submit -- class tn . spark . batch . WordCountTask -- master local wordcount - 1. jar input / loremipsum . txt output Nous avons lanc\u00e9 le job en mode local, pour commencer. Le fichier en entr\u00e9e est le fichier loremipsum.txt, et le r\u00e9sultat sera stock\u00e9 dans un r\u00e9pertoire output . Si tout se passe bien, vous devriez trouver, dans le r\u00e9pertoire output , un fichier part-00000, qui ressemble \u00e0 ce qui suit:","title":"Lancement du code sur le cluster"},{"location":"p4-batch/#references","text":"Spark for beginners, RDD in Spark , http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html , consult\u00e9 le 03/2020 \u21a9 Spark Documentation, Resilient Distributed Datasets(RDDs) , https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds , consult\u00e9 le 03/2020 \u21a9 Devopedia, Apache Spark , https://devopedia.org/apache-spark , consult\u00e9 le 03/2020 \u21a9","title":"R\u00e9f\u00e9rences"},{"location":"p5-sql/","text":"Partie 5 - Spark SQL Pr\u00e9sentation de Spark SQL Spark SQL 1 est un module de Spark pour le traitement des donn\u00e9es structur\u00e9es. Contrairement aux RDD, les interfaces fournies par Spark SQL informent Spark de la structure des donn\u00e9es et traitements r\u00e9alis\u00e9s. En interne, Spark SQl utilise ces informations pour r\u00e9aliser des optimisations. Il est possible d'interagir avec Spark SQL de deux fa\u00e7ons: en utilisant SQL et l'API Dataset. Pour ces deux interfaces, le m\u00eame moteur d'ex\u00e9cution est utilis\u00e9 par Spark SQL, ce qui permet aux d\u00e9veloppeurs de passer facilement d'une API \u00e0 une autre. SQL SQL est utilis\u00e9 comme langage de requ\u00eatage dans Spark SQL. Ce dernier peut \u00e9galement lire des donn\u00e9es \u00e0 partir d'une installation Hive existante. En ex\u00e9cutant des requ\u00eates SQL \u00e0 partir d'autres langages de programmation, le r\u00e9sultat est retourn\u00e9 sous forme de Dataset ou DataFrame. Datasets et DataFrames Un DataFrame est une structure organis\u00e9e en colonnes nomm\u00e9es et non typ\u00e9es. Il est conceptuellement \u00e9quivalent \u00e0 une table dans une base de donn\u00e9es relationnelle ou un data frame en Python ou R, avec en plus des optimisations plus riches. Ils ont \u00e9t\u00e9 con\u00e7us comme couche au dessus des RDDs, pour ajouter des m\u00e9tadonn\u00e9es suppl\u00e9mentaires gr\u00e2ce \u00e0 leur format tabulaire. Les DataFrames peuvent \u00eatre construites \u00e0 partir de fichiers structur\u00e9s, tables dans Hive, bases de donn\u00e9es externes ou de RDDs existants. Un Dataset est une collection distribu\u00e9e de donn\u00e9es typ\u00e9es. C'est une nouvelle structure ajout\u00e9e dans Spark 1.6 qui fournit les avantages des RDDs en plus de ceux du moteur d'ex\u00e9cution optimis\u00e9 de Spark SQL. C'est principalement une am\u00e9lioration des DataFrames, qui y rajoute le typage des donn\u00e9es. Un dataset peut \u00eatre constuit \u00e0 partir d'objets de la JVM, puis manipul\u00e9 en utilisant les transformations telles que map , flatMap , filter , etc. A partir de la version 2 de Spark, les APIs des Datasets et DataFrame sont unifi\u00e9es. D\u00e9sormais, un DataFrame est r\u00e9f\u00e9renc\u00e9 comme \u00e9tant un Dataset [ Row ] . Le tableau suivant permet de comparer les trois structures (RDD, Dataset et DataFrame) selon plusieurs crit\u00e8res 2 : Caract\u00e9ristiques de Spark SQL 3 Int\u00e9gr\u00e9 : Permet de mixer les programmes Spark avec les requ\u00eates SQL, ce qui autorise un requ\u00eatage de donn\u00e9es structur\u00e9es gr\u00e2ce \u00e0 SQL ou \u00e0 l'API Dataframe en Java, Scala, Python et R. Acc\u00e8s unifi\u00e9 aux donn\u00e9es : Les Dataframes et SQL dans Spark communiquent de faon unifi\u00e9e avec plusieurs sources de donn\u00e9es telles que Hive, Avro, Parquet, JSON et JDBC. Compatible avec Hive : Ex\u00e9cute des requ\u00eates Hive sans modification sur les donn\u00e9es courantes. Spark SQL r\u00e9\u00e9crit le frontend de Hive, permettant une compatibilit\u00e9 compl\u00e8te avec les donn\u00e9es, requ\u00eates et UDFs de Hive. Connectivit\u00e9 standart : La connexion peut se faire via JDBC ou ODBC. Performance et scalabilit\u00e9 : Spark SQL incorpore un optimiseur, un g\u00e9n\u00e9rateur de code et un stockage orient\u00e9 colonnes. De plus, il profite de la puissance du moteur Spark, qui fournit une excellente tol\u00e9rance au fautes. Optimisation de Spark SQL avec Catalyst Le framework d'optimisation de Spark SQL permet aux d\u00e9veloppeurs d'exprimer des requ\u00eates de transformation en tr\u00e8s peu de lignes de code. 3 Spark SQL incorpore un optimiseur appel\u00e9 Catalyst, bas\u00e9 sur des constructions fonctionneles en Scala. Il supporte une optimisation \u00e0 base de r\u00e8gles ( rule-based ) et \u00e0 base de co\u00fbt ( cost_based ). L'optimisation \u00e0 base de r\u00e8gles utilise un ensemble de r\u00e8gles pour d\u00e9terminer comment ex\u00e9cuter une requ\u00eate donn\u00e9e, alors que l'optimisation \u00e0 base de co\u00fbt trouve le meilleur moyen pour ex\u00e9cuter une requ\u00eate SQL. Cette derni\u00e8re cat\u00e9gorie g\u00e9n\u00e8re plusieurs r\u00e8gles, calcule les co\u00fbts induits par chacune, et choisit la plus optimis\u00e9e. En interne, Catalyst contient une biblioth\u00e8que pour repr\u00e9senter des arbres et appliquer des r\u00e8gles pour les manipuler. Par dessus, d'autres biblioth\u00e8ques ont \u00e9t\u00e9 construites pour assurer le traitement de requ\u00eates relationnelles, ainsi que plusieurs r\u00e8gles qui g\u00e8rent diff\u00e9rentes phases de l'ex\u00e9cution des requ\u00eates: analyse, optimisation logique, planification physique et g\u00e9n\u00e9ration de code, pour compiler des parties de la requ\u00eate en Bytecode Java. Pour cette derni\u00e8re op\u00e9ration, une autre caract\u00e9ristique de Scala, les quasiquotes , est utilis\u00e9e, pour faciliter la g\u00e9n\u00e9ration de code \u00e0 l'ex\u00e9cution \u00e0 partir d'expressions composables. 4 Manipulation de Spark SQL avec le Shell RDD vs DataFrame vs Dataset Nous allons dans cette partie vous montrer les diff\u00e9rences entre ces trois structures de donn\u00e9es, en utilisant du code, inspir\u00e9 du tutoriel de Zenika 2 . Supposons qu'on ait le fichier purchases.txt , qui contient la totalit\u00e9 des achats r\u00e9alis\u00e9s dans une grande distribution, dont la structure est la suivante: date , heure , ville , categorie_pdt , prix , moyen_paiement Nous voulons r\u00e9aliser une op\u00e9ration de Map Reduce simple, o\u00f9 nous allons calculer pour chaque ville, la somme totale des ventes r\u00e9alis\u00e9es. En premier lieu, nous allons commencer par charger le fichier purchases.txt dans le r\u00e9pertoire input de notre cluster spark. Commencer par d\u00e9marrer votre cluster (si ce n'est pas d\u00e9j\u00e0 fait): 1 docker start spark - master spark - slave1 spark - slave2 Entrer dans le master 1 docker exec - it spark - master bash Naviguer vers le r\u00e9pertoire input pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 cd ~/ input T\u00e9l\u00e9charger le fichier purchases sur votre ordinateur \u00e0 partir de ce LIEN (ceci peut prendre quelques minutes). Charger le fichier t\u00e9l\u00e9charg\u00e9 dans votre master gr\u00e2ce \u00e0 la commande docker cp (il faudra ouvrir un autre terminal ) 1 docker cp < chemin_de_purchases . txt >/ purchases . txt spark - master : //root/input Si tout se passe bien, vous devriez retrouver le fichier purchases dans le r\u00e9pertoire input. Revenir dans votre contenaire spark, et taper: 1 ls / root / input le fichier devrait appara\u00eetre: Utilisation des RDD Nous allons ex\u00e9cuter l'op\u00e9ration de calcul sur les donn\u00e9es gr\u00e2ce aux structures RDD. Pour cela, suivre les \u00e9tapes suivantes: Dans votre contenaire spark-master, ouvrir spark-shell: 1 spark-shell Charger le fichier purchases dans un RDD 1 val allData = sc . textFile ( \"/root/input/purchases.txt\" ) Transformation 1: s\u00e9parer les champs 1 val splitted = allData . map ( line => line . split ( \"\\t\" )) Transformation 2: extraire les couples clef-valeur 1 val pairs = splitted . map ( splitted => ( splitted ( 2 ), splitted ( 4 ). toFloat )) Transformation 3: r\u00e9aliser la somme des prix de vente pour chaque clef 1 val finalResult = pairs . reduceByKey ( _ + _ ) Action : sauvegarder le r\u00e9sultat dans un fichier texte 1 finalResult . saveAsTextFile ( \"/root/output/purchase-rdd.count\" ) Ouvrir le fichier texte et visualiser le r\u00e9sultat (sortir de spark-shell pour cela avec Ctrl - C ) 1 head /root/output/purchase-rdd.count/part-00000 Le r\u00e9sultat obtenu devrait ressembler \u00e0 ce qui suit: Toutes les transformations repr\u00e9sent\u00e9es ci-dessus produisent des RDD distribu\u00e9s sur les workers. La transformation 3 ( reduceByKey ) r\u00e9alise ce qu'on appelle une transformation large (ou wide transformation ) o\u00f9 une partition du RDD est produite \u00e0 partir de plusieurs partitions du RDD parent, contrairement aux transformations \u00e9troites (ou narrow transformations ) o\u00f9 chaque partition du RDD fils est cr\u00e9\u00e9e \u00e0 partir d'une seule partition du RDD parent (tel que indiqu\u00e9 dans la figure suivante). Utilisation des DataFrames Contrairement aux RDD, les DataFrames sont non typ\u00e9es. En effet, m\u00eame si ce n'est pas visible dans le code pr\u00e9c\u00e9dent, car on utilise le langage Scala qui inf\u00e8re les types des variables, les RDD cr\u00e9\u00e9s sont de type RDD[String]. En ce qui concerne les DataFrames, par contre, les types ne sont pas d\u00e9finis, par contre, la structure des donn\u00e9es l'est. C'est \u00e0 dire que nous pouvons faire r\u00e9f\u00e9rence \u00e0 chacun des champs par son nom, d\u00e9fini pr\u00e9alablement dans un sch\u00e9ma, tel une base de donn\u00e9es. Commencer par importer les classes n\u00e9cessaires dans Spark-shell 1 import org.apache.spark.sql.types. { StructType , StructField , StringType , FloatType }; D\u00e9finir le sch\u00e9ma des donn\u00e9es qui se trouvent dans le fichier purchases.txt que nous allons charger 1 val customSchema = StructType ( Seq ( StructField ( \"date\" , StringType , true ), StructField ( \"time\" , StringType , true ), StructField ( \"town\" , StringType , true ), StructField ( \"product\" , StringType , true ), StructField ( \"price\" , FloatType , true ), StructField ( \"payment\" , StringType , true ))) Lire le fichier comme \u00e9taht un document CSV ( Comma-separated Values ), et mapper le r\u00e9sultat avec le sch\u00e9ma d\u00e9fini 1 val resultAsACsvFormat = spark . read . schema ( customSchema ). option ( \"delimiter\" , \"\\t\" ). csv ( \"/root/input/purchases.txt\" ) Grouper les donn\u00e9es par ville et faire la somme des prix 1 val finalResult = resultAsACsvFormat . groupBy ( \"town\" ). sum ( \"price\" ) Sauvegarder les r\u00e9sultats dans un fichier 1 finalResult . rdd . saveAsTextFile ( \"/root/output/purchase-df.count\" ) On remarque qu'il n'est pas possible de sauvegarder les DataFrame dans un fichier directement. Il faudra les transformer en RDD d'abord. De plus, les r\u00e9sultats seront sauvegard\u00e9s sur un grand nombre de fichiers, certains vides. Pour visualliser le r\u00e9sultat complet, il est possible d'utiliser l'action finalResult.collect() , qui permet de retourner le RDD complet au programme Driver. Cela suppose bien s\u00fbr que le RDD peut \u00eatre charg\u00e9 en entier dans la m\u00e9moire de la machine master. Le r\u00e9sultat qu'on obtient alors sera comme suit: Attention Il est possible que la fonction collect ne fonctionne pas si la version de Java n'est pas compatible avec Spark. Un message du type \" Unsupported class file major version 55 \" s'affichera alors. Pour \u00e9viter cela, installer la version 1.8 de JDK avec apt install openjdk - 8 - jdk , puis ajouter les lignes suivantes \u00e0 ~/ . bashrc : 1 2 export JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64/ export PATH = $JAVA_HOME /bin: $PATH Charger le fichier .bashrc en utilisant source ~/ . bashrc , puis v\u00e9rifier que la version de Java a bien chang\u00e9 en utilisant: java - version . L'affichage suivant devra appara\u00eetre: Utilisation des Datasets Les Datasets sont une am\u00e9lioration des DataFrames, qui y rajoutent le typage. Les donn\u00e9es ont donc une structure bien d\u00e9finie, mais en plus, telles que les bases de donn\u00e9es relationnelles, un type pour chaque \u00e9l\u00e9ment. Pour utiliser les Datasets comme structure de donn\u00e9es dans notre exemple, suivre les \u00e9tapes suivantes: Importer les classes n\u00e9cessaires dans Spark-shell, et d\u00e9finir le sch\u00e9ma des donn\u00e9es 1 2 3 4 5 6 import org.apache.spark.sql.types. { StructType , StructField , StringType , FloatType }; val customSchema = StructType ( Seq ( StructField ( \"date\" , StringType , true ), StructField ( \"time\" , StringType , true ), StructField ( \"town\" , StringType , true ), StructField ( \"product\" , StringType , true ), StructField ( \"price\" , FloatType , true ), StructField ( \"payment\" , StringType , true ))) Cr\u00e9er une classe associ\u00e9e \u00e0 ce sch\u00e9ma 1 case class Product ( date : String , time : String , town : String , product : String , price : Float , payment : String ) Lire le fichier comme \u00e9taht un document CSV ( Comma-separated Values ), en y associant la classe cr\u00e9\u00e9e 1 val result = spark . read . schema ( customSchema ). option ( \"delimiter\" , \"\\t\" ). csv ( \"/root/input/purchases.txt\" ). as [ Product ] Grouper, faire la somme et afficher le r\u00e9sultat 1 2 val finalResult = result . groupBy ( \"town\" ). sum ( \"price\" ) finalResult . collect () Le r\u00e9sultat ressemblera \u00e0 ce qui suit: Exemple Spark SQL avec Scala Nous allons montrer dans ce qui suit un exemple SparkSQL simple 5 , qui lit \u00e0 partir de donn\u00e9es se trouvant sur un fichier JSON, et interroge les donnees en utilisant les DataFrames et les op\u00e9rations de transformations et actions optimis\u00e9es de Spark. Commencer par cr\u00e9er un fichier JSON intitul\u00e9 employe . json dans le r\u00e9pertoire /root/input de votre spark-master, avec le contenu suivant: 1 2 3 4 5 { \"id\" : \"1201\" , \"name\" : \"ahmed\" , \"age\" : \"25\" } { \"id\" : \"1202\" , \"name\" : \"salma\" , \"age\" : \"58\" } { \"id\" : \"1203\" , \"name\" : \"amina\" , \"age\" : \"39\" } { \"id\" : \"1204\" , \"name\" : \"ali\" , \"age\" : \"23\" } { \"id\" : \"1205\" , \"name\" : \"mourad\" , \"age\" : \"23\" } Le fichier doit contenir une liste de documents JSON successifs. D\u00e9marrer ensuite le Spark-Shell 1 spark-shell D\u00e9finir le SQL context 1 2 val sqlcontext = new org . apache . spark . sql . SQLContext ( sc ) // R\u00e9sultat: sqlcontext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@75b41ff3 Lire le contenu du fichier et le charger dans un DataFrame 1 2 val dfs = sqlcontext . read . json ( \"/root/input/employe.json\" ) // R\u00e9sultat: dfs: org.apache.spark.sql.DataFrame = [age: string, id: string ... 1 more field] Afficher le contenu du fichier. 1 2 3 4 5 6 7 8 9 10 11 12 dfs . show () /* R\u00e9sultat: +---+----+------+ |age| id| name| +---+----+------+ | 25|1201| ahmed| | 58|1202| salma| | 39|1203| amina| | 23|1204| ali| | 23|1205|mourad| +---+----+------+ */ Afficher le sch\u00e9ma inf\u00e9r\u00e9 des donn\u00e9es 1 2 3 4 5 6 7 dfs . printSchema () /* R\u00e9sultat: root |-- age: string (nullable = true) |-- id: string (nullable = true) |-- name: string (nullable = true) */ S\u00e9lectionner les noms des employ\u00e9s 1 2 3 4 5 6 7 8 9 10 11 12 dfs . select ( \"name\" ). show () /* R\u00e9sultat: +------+ | name| +------+ | ahmed| | salma| | amina| | ali| |mourad| +------+ */ Filter les donn\u00e9es par age 1 2 3 4 5 6 7 8 9 10 dfs . filter ( dfs ( \"age\" ) > 23 ). show () /* R\u00e9sultat: +---+----+-----+ |age| id| name| +---+----+-----+ | 25|1201|ahmed| | 58|1202|salma| | 39|1203|amina| +---+----+-----+ */ Grouper les donn\u00e9es par age et compter le nombre de personnes pour chaque age 1 2 3 4 5 6 7 8 9 10 11 dfs . groupBy ( \"age\" ). count (). show () /* R\u00e9sultat: +---+-----+ |age|count| +---+-----+ | 23| 2| | 25| 1| | 58| 1| | 39| 1| +---+-----+ */ R\u00e9f\u00e9rences Spark Documentation, Spark SQL, DataFrames and Datasets Guide , https://spark.apache.org/docs/latest/sql-programming-guide.html , consult\u00e9 le 03/2020 \u21a9 Nastasia Saby from Zenika, A comparison between RDD, DataFrame and Dataset in Spark from a developer's point of view , https://medium.zenika.com/a-comparison-between-rdd-dataframe-and-dataset-in-spark-from-a-developers-point-of-view-a539b5acf734 , consult\u00e9 le 03/2020 \u21a9 \u21a9 Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 03/2020 \u21a9 \u21a9 DataBricks, Deep Dive into Spark SQL's Catalyst Optimizer , https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html , consult\u00e9 le 03/2020 \u21a9 TutorialsPoint, SparkSQL Tutorial , https://www.tutorialspoint.com/spark_sql/spark_sql_dataframes.htm , consult\u00e9 le 03/2020 \u21a9","title":"P5 - Spark SQL"},{"location":"p5-sql/#partie-5-spark-sql","text":"","title":"Partie 5 - Spark SQL"},{"location":"p5-sql/#presentation-de-spark-sql","text":"Spark SQL 1 est un module de Spark pour le traitement des donn\u00e9es structur\u00e9es. Contrairement aux RDD, les interfaces fournies par Spark SQL informent Spark de la structure des donn\u00e9es et traitements r\u00e9alis\u00e9s. En interne, Spark SQl utilise ces informations pour r\u00e9aliser des optimisations. Il est possible d'interagir avec Spark SQL de deux fa\u00e7ons: en utilisant SQL et l'API Dataset. Pour ces deux interfaces, le m\u00eame moteur d'ex\u00e9cution est utilis\u00e9 par Spark SQL, ce qui permet aux d\u00e9veloppeurs de passer facilement d'une API \u00e0 une autre.","title":"Pr\u00e9sentation de Spark SQL"},{"location":"p5-sql/#sql","text":"SQL est utilis\u00e9 comme langage de requ\u00eatage dans Spark SQL. Ce dernier peut \u00e9galement lire des donn\u00e9es \u00e0 partir d'une installation Hive existante. En ex\u00e9cutant des requ\u00eates SQL \u00e0 partir d'autres langages de programmation, le r\u00e9sultat est retourn\u00e9 sous forme de Dataset ou DataFrame.","title":"SQL"},{"location":"p5-sql/#datasets-et-dataframes","text":"Un DataFrame est une structure organis\u00e9e en colonnes nomm\u00e9es et non typ\u00e9es. Il est conceptuellement \u00e9quivalent \u00e0 une table dans une base de donn\u00e9es relationnelle ou un data frame en Python ou R, avec en plus des optimisations plus riches. Ils ont \u00e9t\u00e9 con\u00e7us comme couche au dessus des RDDs, pour ajouter des m\u00e9tadonn\u00e9es suppl\u00e9mentaires gr\u00e2ce \u00e0 leur format tabulaire. Les DataFrames peuvent \u00eatre construites \u00e0 partir de fichiers structur\u00e9s, tables dans Hive, bases de donn\u00e9es externes ou de RDDs existants. Un Dataset est une collection distribu\u00e9e de donn\u00e9es typ\u00e9es. C'est une nouvelle structure ajout\u00e9e dans Spark 1.6 qui fournit les avantages des RDDs en plus de ceux du moteur d'ex\u00e9cution optimis\u00e9 de Spark SQL. C'est principalement une am\u00e9lioration des DataFrames, qui y rajoute le typage des donn\u00e9es. Un dataset peut \u00eatre constuit \u00e0 partir d'objets de la JVM, puis manipul\u00e9 en utilisant les transformations telles que map , flatMap , filter , etc. A partir de la version 2 de Spark, les APIs des Datasets et DataFrame sont unifi\u00e9es. D\u00e9sormais, un DataFrame est r\u00e9f\u00e9renc\u00e9 comme \u00e9tant un Dataset [ Row ] . Le tableau suivant permet de comparer les trois structures (RDD, Dataset et DataFrame) selon plusieurs crit\u00e8res 2 :","title":"Datasets et DataFrames"},{"location":"p5-sql/#caracteristiques-de-spark-sql","text":"3 Int\u00e9gr\u00e9 : Permet de mixer les programmes Spark avec les requ\u00eates SQL, ce qui autorise un requ\u00eatage de donn\u00e9es structur\u00e9es gr\u00e2ce \u00e0 SQL ou \u00e0 l'API Dataframe en Java, Scala, Python et R. Acc\u00e8s unifi\u00e9 aux donn\u00e9es : Les Dataframes et SQL dans Spark communiquent de faon unifi\u00e9e avec plusieurs sources de donn\u00e9es telles que Hive, Avro, Parquet, JSON et JDBC. Compatible avec Hive : Ex\u00e9cute des requ\u00eates Hive sans modification sur les donn\u00e9es courantes. Spark SQL r\u00e9\u00e9crit le frontend de Hive, permettant une compatibilit\u00e9 compl\u00e8te avec les donn\u00e9es, requ\u00eates et UDFs de Hive. Connectivit\u00e9 standart : La connexion peut se faire via JDBC ou ODBC. Performance et scalabilit\u00e9 : Spark SQL incorpore un optimiseur, un g\u00e9n\u00e9rateur de code et un stockage orient\u00e9 colonnes. De plus, il profite de la puissance du moteur Spark, qui fournit une excellente tol\u00e9rance au fautes.","title":"Caract\u00e9ristiques de Spark SQL"},{"location":"p5-sql/#optimisation-de-spark-sql-avec-catalyst","text":"Le framework d'optimisation de Spark SQL permet aux d\u00e9veloppeurs d'exprimer des requ\u00eates de transformation en tr\u00e8s peu de lignes de code. 3 Spark SQL incorpore un optimiseur appel\u00e9 Catalyst, bas\u00e9 sur des constructions fonctionneles en Scala. Il supporte une optimisation \u00e0 base de r\u00e8gles ( rule-based ) et \u00e0 base de co\u00fbt ( cost_based ). L'optimisation \u00e0 base de r\u00e8gles utilise un ensemble de r\u00e8gles pour d\u00e9terminer comment ex\u00e9cuter une requ\u00eate donn\u00e9e, alors que l'optimisation \u00e0 base de co\u00fbt trouve le meilleur moyen pour ex\u00e9cuter une requ\u00eate SQL. Cette derni\u00e8re cat\u00e9gorie g\u00e9n\u00e8re plusieurs r\u00e8gles, calcule les co\u00fbts induits par chacune, et choisit la plus optimis\u00e9e. En interne, Catalyst contient une biblioth\u00e8que pour repr\u00e9senter des arbres et appliquer des r\u00e8gles pour les manipuler. Par dessus, d'autres biblioth\u00e8ques ont \u00e9t\u00e9 construites pour assurer le traitement de requ\u00eates relationnelles, ainsi que plusieurs r\u00e8gles qui g\u00e8rent diff\u00e9rentes phases de l'ex\u00e9cution des requ\u00eates: analyse, optimisation logique, planification physique et g\u00e9n\u00e9ration de code, pour compiler des parties de la requ\u00eate en Bytecode Java. Pour cette derni\u00e8re op\u00e9ration, une autre caract\u00e9ristique de Scala, les quasiquotes , est utilis\u00e9e, pour faciliter la g\u00e9n\u00e9ration de code \u00e0 l'ex\u00e9cution \u00e0 partir d'expressions composables. 4","title":"Optimisation de Spark SQL avec Catalyst"},{"location":"p5-sql/#manipulation-de-spark-sql-avec-le-shell","text":"","title":"Manipulation de Spark SQL avec le Shell"},{"location":"p5-sql/#rdd-vs-dataframe-vs-dataset","text":"Nous allons dans cette partie vous montrer les diff\u00e9rences entre ces trois structures de donn\u00e9es, en utilisant du code, inspir\u00e9 du tutoriel de Zenika 2 . Supposons qu'on ait le fichier purchases.txt , qui contient la totalit\u00e9 des achats r\u00e9alis\u00e9s dans une grande distribution, dont la structure est la suivante: date , heure , ville , categorie_pdt , prix , moyen_paiement Nous voulons r\u00e9aliser une op\u00e9ration de Map Reduce simple, o\u00f9 nous allons calculer pour chaque ville, la somme totale des ventes r\u00e9alis\u00e9es. En premier lieu, nous allons commencer par charger le fichier purchases.txt dans le r\u00e9pertoire input de notre cluster spark. Commencer par d\u00e9marrer votre cluster (si ce n'est pas d\u00e9j\u00e0 fait): 1 docker start spark - master spark - slave1 spark - slave2 Entrer dans le master 1 docker exec - it spark - master bash Naviguer vers le r\u00e9pertoire input pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 cd ~/ input T\u00e9l\u00e9charger le fichier purchases sur votre ordinateur \u00e0 partir de ce LIEN (ceci peut prendre quelques minutes). Charger le fichier t\u00e9l\u00e9charg\u00e9 dans votre master gr\u00e2ce \u00e0 la commande docker cp (il faudra ouvrir un autre terminal ) 1 docker cp < chemin_de_purchases . txt >/ purchases . txt spark - master : //root/input Si tout se passe bien, vous devriez retrouver le fichier purchases dans le r\u00e9pertoire input. Revenir dans votre contenaire spark, et taper: 1 ls / root / input le fichier devrait appara\u00eetre:","title":"RDD vs DataFrame vs Dataset"},{"location":"p5-sql/#utilisation-des-rdd","text":"Nous allons ex\u00e9cuter l'op\u00e9ration de calcul sur les donn\u00e9es gr\u00e2ce aux structures RDD. Pour cela, suivre les \u00e9tapes suivantes: Dans votre contenaire spark-master, ouvrir spark-shell: 1 spark-shell Charger le fichier purchases dans un RDD 1 val allData = sc . textFile ( \"/root/input/purchases.txt\" ) Transformation 1: s\u00e9parer les champs 1 val splitted = allData . map ( line => line . split ( \"\\t\" )) Transformation 2: extraire les couples clef-valeur 1 val pairs = splitted . map ( splitted => ( splitted ( 2 ), splitted ( 4 ). toFloat )) Transformation 3: r\u00e9aliser la somme des prix de vente pour chaque clef 1 val finalResult = pairs . reduceByKey ( _ + _ ) Action : sauvegarder le r\u00e9sultat dans un fichier texte 1 finalResult . saveAsTextFile ( \"/root/output/purchase-rdd.count\" ) Ouvrir le fichier texte et visualiser le r\u00e9sultat (sortir de spark-shell pour cela avec Ctrl - C ) 1 head /root/output/purchase-rdd.count/part-00000 Le r\u00e9sultat obtenu devrait ressembler \u00e0 ce qui suit: Toutes les transformations repr\u00e9sent\u00e9es ci-dessus produisent des RDD distribu\u00e9s sur les workers. La transformation 3 ( reduceByKey ) r\u00e9alise ce qu'on appelle une transformation large (ou wide transformation ) o\u00f9 une partition du RDD est produite \u00e0 partir de plusieurs partitions du RDD parent, contrairement aux transformations \u00e9troites (ou narrow transformations ) o\u00f9 chaque partition du RDD fils est cr\u00e9\u00e9e \u00e0 partir d'une seule partition du RDD parent (tel que indiqu\u00e9 dans la figure suivante).","title":"Utilisation des RDD"},{"location":"p5-sql/#utilisation-des-dataframes","text":"Contrairement aux RDD, les DataFrames sont non typ\u00e9es. En effet, m\u00eame si ce n'est pas visible dans le code pr\u00e9c\u00e9dent, car on utilise le langage Scala qui inf\u00e8re les types des variables, les RDD cr\u00e9\u00e9s sont de type RDD[String]. En ce qui concerne les DataFrames, par contre, les types ne sont pas d\u00e9finis, par contre, la structure des donn\u00e9es l'est. C'est \u00e0 dire que nous pouvons faire r\u00e9f\u00e9rence \u00e0 chacun des champs par son nom, d\u00e9fini pr\u00e9alablement dans un sch\u00e9ma, tel une base de donn\u00e9es. Commencer par importer les classes n\u00e9cessaires dans Spark-shell 1 import org.apache.spark.sql.types. { StructType , StructField , StringType , FloatType }; D\u00e9finir le sch\u00e9ma des donn\u00e9es qui se trouvent dans le fichier purchases.txt que nous allons charger 1 val customSchema = StructType ( Seq ( StructField ( \"date\" , StringType , true ), StructField ( \"time\" , StringType , true ), StructField ( \"town\" , StringType , true ), StructField ( \"product\" , StringType , true ), StructField ( \"price\" , FloatType , true ), StructField ( \"payment\" , StringType , true ))) Lire le fichier comme \u00e9taht un document CSV ( Comma-separated Values ), et mapper le r\u00e9sultat avec le sch\u00e9ma d\u00e9fini 1 val resultAsACsvFormat = spark . read . schema ( customSchema ). option ( \"delimiter\" , \"\\t\" ). csv ( \"/root/input/purchases.txt\" ) Grouper les donn\u00e9es par ville et faire la somme des prix 1 val finalResult = resultAsACsvFormat . groupBy ( \"town\" ). sum ( \"price\" ) Sauvegarder les r\u00e9sultats dans un fichier 1 finalResult . rdd . saveAsTextFile ( \"/root/output/purchase-df.count\" ) On remarque qu'il n'est pas possible de sauvegarder les DataFrame dans un fichier directement. Il faudra les transformer en RDD d'abord. De plus, les r\u00e9sultats seront sauvegard\u00e9s sur un grand nombre de fichiers, certains vides. Pour visualliser le r\u00e9sultat complet, il est possible d'utiliser l'action finalResult.collect() , qui permet de retourner le RDD complet au programme Driver. Cela suppose bien s\u00fbr que le RDD peut \u00eatre charg\u00e9 en entier dans la m\u00e9moire de la machine master. Le r\u00e9sultat qu'on obtient alors sera comme suit: Attention Il est possible que la fonction collect ne fonctionne pas si la version de Java n'est pas compatible avec Spark. Un message du type \" Unsupported class file major version 55 \" s'affichera alors. Pour \u00e9viter cela, installer la version 1.8 de JDK avec apt install openjdk - 8 - jdk , puis ajouter les lignes suivantes \u00e0 ~/ . bashrc : 1 2 export JAVA_HOME = /usr/lib/jvm/java-8-openjdk-amd64/ export PATH = $JAVA_HOME /bin: $PATH Charger le fichier .bashrc en utilisant source ~/ . bashrc , puis v\u00e9rifier que la version de Java a bien chang\u00e9 en utilisant: java - version . L'affichage suivant devra appara\u00eetre:","title":"Utilisation des DataFrames"},{"location":"p5-sql/#utilisation-des-datasets","text":"Les Datasets sont une am\u00e9lioration des DataFrames, qui y rajoutent le typage. Les donn\u00e9es ont donc une structure bien d\u00e9finie, mais en plus, telles que les bases de donn\u00e9es relationnelles, un type pour chaque \u00e9l\u00e9ment. Pour utiliser les Datasets comme structure de donn\u00e9es dans notre exemple, suivre les \u00e9tapes suivantes: Importer les classes n\u00e9cessaires dans Spark-shell, et d\u00e9finir le sch\u00e9ma des donn\u00e9es 1 2 3 4 5 6 import org.apache.spark.sql.types. { StructType , StructField , StringType , FloatType }; val customSchema = StructType ( Seq ( StructField ( \"date\" , StringType , true ), StructField ( \"time\" , StringType , true ), StructField ( \"town\" , StringType , true ), StructField ( \"product\" , StringType , true ), StructField ( \"price\" , FloatType , true ), StructField ( \"payment\" , StringType , true ))) Cr\u00e9er une classe associ\u00e9e \u00e0 ce sch\u00e9ma 1 case class Product ( date : String , time : String , town : String , product : String , price : Float , payment : String ) Lire le fichier comme \u00e9taht un document CSV ( Comma-separated Values ), en y associant la classe cr\u00e9\u00e9e 1 val result = spark . read . schema ( customSchema ). option ( \"delimiter\" , \"\\t\" ). csv ( \"/root/input/purchases.txt\" ). as [ Product ] Grouper, faire la somme et afficher le r\u00e9sultat 1 2 val finalResult = result . groupBy ( \"town\" ). sum ( \"price\" ) finalResult . collect () Le r\u00e9sultat ressemblera \u00e0 ce qui suit:","title":"Utilisation des Datasets"},{"location":"p5-sql/#exemple-spark-sql-avec-scala","text":"Nous allons montrer dans ce qui suit un exemple SparkSQL simple 5 , qui lit \u00e0 partir de donn\u00e9es se trouvant sur un fichier JSON, et interroge les donnees en utilisant les DataFrames et les op\u00e9rations de transformations et actions optimis\u00e9es de Spark. Commencer par cr\u00e9er un fichier JSON intitul\u00e9 employe . json dans le r\u00e9pertoire /root/input de votre spark-master, avec le contenu suivant: 1 2 3 4 5 { \"id\" : \"1201\" , \"name\" : \"ahmed\" , \"age\" : \"25\" } { \"id\" : \"1202\" , \"name\" : \"salma\" , \"age\" : \"58\" } { \"id\" : \"1203\" , \"name\" : \"amina\" , \"age\" : \"39\" } { \"id\" : \"1204\" , \"name\" : \"ali\" , \"age\" : \"23\" } { \"id\" : \"1205\" , \"name\" : \"mourad\" , \"age\" : \"23\" } Le fichier doit contenir une liste de documents JSON successifs. D\u00e9marrer ensuite le Spark-Shell 1 spark-shell D\u00e9finir le SQL context 1 2 val sqlcontext = new org . apache . spark . sql . SQLContext ( sc ) // R\u00e9sultat: sqlcontext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@75b41ff3 Lire le contenu du fichier et le charger dans un DataFrame 1 2 val dfs = sqlcontext . read . json ( \"/root/input/employe.json\" ) // R\u00e9sultat: dfs: org.apache.spark.sql.DataFrame = [age: string, id: string ... 1 more field] Afficher le contenu du fichier. 1 2 3 4 5 6 7 8 9 10 11 12 dfs . show () /* R\u00e9sultat: +---+----+------+ |age| id| name| +---+----+------+ | 25|1201| ahmed| | 58|1202| salma| | 39|1203| amina| | 23|1204| ali| | 23|1205|mourad| +---+----+------+ */ Afficher le sch\u00e9ma inf\u00e9r\u00e9 des donn\u00e9es 1 2 3 4 5 6 7 dfs . printSchema () /* R\u00e9sultat: root |-- age: string (nullable = true) |-- id: string (nullable = true) |-- name: string (nullable = true) */ S\u00e9lectionner les noms des employ\u00e9s 1 2 3 4 5 6 7 8 9 10 11 12 dfs . select ( \"name\" ). show () /* R\u00e9sultat: +------+ | name| +------+ | ahmed| | salma| | amina| | ali| |mourad| +------+ */ Filter les donn\u00e9es par age 1 2 3 4 5 6 7 8 9 10 dfs . filter ( dfs ( \"age\" ) > 23 ). show () /* R\u00e9sultat: +---+----+-----+ |age| id| name| +---+----+-----+ | 25|1201|ahmed| | 58|1202|salma| | 39|1203|amina| +---+----+-----+ */ Grouper les donn\u00e9es par age et compter le nombre de personnes pour chaque age 1 2 3 4 5 6 7 8 9 10 11 dfs . groupBy ( \"age\" ). count (). show () /* R\u00e9sultat: +---+-----+ |age|count| +---+-----+ | 23| 2| | 25| 1| | 58| 1| | 39| 1| +---+-----+ */","title":"Exemple Spark SQL avec Scala"},{"location":"p5-sql/#references","text":"Spark Documentation, Spark SQL, DataFrames and Datasets Guide , https://spark.apache.org/docs/latest/sql-programming-guide.html , consult\u00e9 le 03/2020 \u21a9 Nastasia Saby from Zenika, A comparison between RDD, DataFrame and Dataset in Spark from a developer's point of view , https://medium.zenika.com/a-comparison-between-rdd-dataframe-and-dataset-in-spark-from-a-developers-point-of-view-a539b5acf734 , consult\u00e9 le 03/2020 \u21a9 \u21a9 Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 03/2020 \u21a9 \u21a9 DataBricks, Deep Dive into Spark SQL's Catalyst Optimizer , https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html , consult\u00e9 le 03/2020 \u21a9 TutorialsPoint, SparkSQL Tutorial , https://www.tutorialspoint.com/spark_sql/spark_sql_dataframes.htm , consult\u00e9 le 03/2020 \u21a9","title":"R\u00e9f\u00e9rences"},{"location":"tips/","text":"See mkdocs Cheat Sheet italique input gras 50070 image ou lien Apache Hadoop code 1 git clone https : //github.com/liliasfaxi/hadoop-cluster-docker inline code hadoop fs - mkdir - p / user / root Attention blablabla Erreur blablabla Activit\u00e9 Modifier tables |Instruction|Fonctionnalit\u00e9| |---------|-------------------------------------------------------------| | hadoop fs \u2013 ls | Afficher le contenu du re\u0301pertoire racine | | hadoop fs \u2013 put file . txt | Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux) | | hadoop fs \u2013 get file . txt | Download un fichier a\u0300 partir de hadoop sur votre disque local | | hadoop fs \u2013 tail file . txt | Lire les dernie\u0300res lignes du fichier | | hadoop fs \u2013 cat file . txt | Affiche tout le contenu du fichier | | hadoop fs \u2013 mv file . txt newfile . txt | Renommer le fichier | | hadoop fs \u2013 rm newfile . txt | Supprimer le fichier | | hadoop fs \u2013 mkdir myinput | Cre\u0301er un re\u0301pertoire | | hadoop fs \u2013 cat file . txt \\ | less | Lire le fichier page par page| Footnotes 1 2 3 4 5 6 7 8 Footnotes [ ^ 1 ] are added in - text like so ... #end of the document [ ^ 1 ] : Footnotes are the mind killer . Footnotes are the little - death that brings total obliteration . I will face my footnotes . get the IP address of a docker container docker inspect | grep IPAddress","title":"Tips"}]}