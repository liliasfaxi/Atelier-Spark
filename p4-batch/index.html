<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Cours et Travaux Pratiques pour se familiariser avec Apache Spark"><meta name=author content="Lilia Sfaxi"><link href=http://liliasfaxi.github.io/Atelier-Spark/p4-batch/ rel=canonical><link rel=icon href=../img/favicon.ico><meta name=generator content="mkdocs-1.2.3, mkdocs-material-8.1.10"><title>P4 - RDD et Batch Processing avec Spark - Atelier Apache Spark</title><link rel=stylesheet href=../assets/stylesheets/main.d6be258b.min.css><link rel=stylesheet href=../assets/stylesheets/palette.e6a45f82.min.css><meta name=theme-color content=#546d78><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../css/timeago.css><link rel=stylesheet href=../stylesheets/extra.css><link rel=stylesheet href=../stylesheets/links.css><script>__md_scope=new URL("..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme data-md-color-primary=blue-grey data-md-color-accent=amber> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#partie-4-spark-rdd-et-traitement-par-lots class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="Atelier Apache Spark" class="md-header__button md-logo" aria-label="Atelier Apache Spark" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Atelier Apache Spark </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> P4 - RDD et Batch Processing avec Spark </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/liliasfaxi/Atelier-Spark/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> liliasfaxi/Atelier-Spark </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="Atelier Apache Spark" class="md-nav__button md-logo" aria-label="Atelier Apache Spark" data-md-component=logo> <img src=../img/logo.png alt=logo> </a> Atelier Apache Spark </label> <div class=md-nav__source> <a href=https://github.com/liliasfaxi/Atelier-Spark/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> liliasfaxi/Atelier-Spark </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Atelier Apache Spark </a> </li> <li class=md-nav__item> <a href=../p1-big-data/ class=md-nav__link> P1 - Introduction au Big Data </a> </li> <li class=md-nav__item> <a href=../p2-spark/ class=md-nav__link> P2 - Introduction à Apache Spark </a> </li> <li class=md-nav__item> <a href=../p3-install/ class=md-nav__link> P3 - Installation de Spark </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> P4 - RDD et Batch Processing avec Spark <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> P4 - RDD et Batch Processing avec Spark </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#partie-4-spark-rdd-et-traitement-par-lots class=md-nav__link> Partie 4 - Spark RDD et Traitement par Lots </a> <nav class=md-nav aria-label="Partie 4 - Spark RDD et Traitement par Lots"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rdd-resilient-distributed-dataset class=md-nav__link> RDD: Resilient Distributed Dataset </a> <nav class=md-nav aria-label="RDD: Resilient Distributed Dataset"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#parallelisation-de-collections class=md-nav__link> Parallélisation de Collections </a> </li> <li class=md-nav__item> <a href=#generation-a-partir-dun-fichier-externe class=md-nav__link> Génération à partir d'un fichier externe </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#operations-sur-les-rdds class=md-nav__link> Opérations sur les RDDs </a> </li> <li class=md-nav__item> <a href=#caracteristiques-des-rdds class=md-nav__link> Caractéristiques des RDDs </a> <nav class=md-nav aria-label="Caractéristiques des RDDs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#immutabilite-et-lignee class=md-nav__link> Immutabilité et Lignée </a> </li> <li class=md-nav__item> <a href=#lazy-evaluation class=md-nav__link> Lazy Evaluation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#exemple class=md-nav__link> Exemple </a> </li> <li class=md-nav__item> <a href=#test-de-spark-avec-spark-shell class=md-nav__link> Test de Spark avec Spark-Shell </a> </li> <li class=md-nav__item> <a href=#spark-batch-en-java class=md-nav__link> Spark Batch en Java </a> <nav class=md-nav aria-label="Spark Batch en Java"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#preparation-de-lenvironnement-et-code class=md-nav__link> Préparation de l'environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local class=md-nav__link> Test du code en local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> Références </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../p5-sql/ class=md-nav__link> P5 - Spark SQL </a> </li> <li class=md-nav__item> <a href=../p6-stream/ class=md-nav__link> P6 - Spark Streaming </a> </li> <li class=md-nav__item> <a href=../p7-ml/ class=md-nav__link> P7 - Spark MLLib </a> </li> <li class=md-nav__item> <a href=../p8-graphx/ class=md-nav__link> P8 - Spark GraphX </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#partie-4-spark-rdd-et-traitement-par-lots class=md-nav__link> Partie 4 - Spark RDD et Traitement par Lots </a> <nav class=md-nav aria-label="Partie 4 - Spark RDD et Traitement par Lots"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#rdd-resilient-distributed-dataset class=md-nav__link> RDD: Resilient Distributed Dataset </a> <nav class=md-nav aria-label="RDD: Resilient Distributed Dataset"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#parallelisation-de-collections class=md-nav__link> Parallélisation de Collections </a> </li> <li class=md-nav__item> <a href=#generation-a-partir-dun-fichier-externe class=md-nav__link> Génération à partir d'un fichier externe </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#operations-sur-les-rdds class=md-nav__link> Opérations sur les RDDs </a> </li> <li class=md-nav__item> <a href=#caracteristiques-des-rdds class=md-nav__link> Caractéristiques des RDDs </a> <nav class=md-nav aria-label="Caractéristiques des RDDs"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#immutabilite-et-lignee class=md-nav__link> Immutabilité et Lignée </a> </li> <li class=md-nav__item> <a href=#lazy-evaluation class=md-nav__link> Lazy Evaluation </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#exemple class=md-nav__link> Exemple </a> </li> <li class=md-nav__item> <a href=#test-de-spark-avec-spark-shell class=md-nav__link> Test de Spark avec Spark-Shell </a> </li> <li class=md-nav__item> <a href=#spark-batch-en-java class=md-nav__link> Spark Batch en Java </a> <nav class=md-nav aria-label="Spark Batch en Java"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#preparation-de-lenvironnement-et-code class=md-nav__link> Préparation de l'environnement et Code </a> </li> <li class=md-nav__item> <a href=#test-du-code-en-local class=md-nav__link> Test du code en local </a> </li> <li class=md-nav__item> <a href=#lancement-du-code-sur-le-cluster class=md-nav__link> Lancement du code sur le cluster </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#references class=md-nav__link> Références </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <a href=https://github.com/liliasfaxi/Atelier-Spark/edit/master/docs/p4-batch.md title="Edit this page" class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg> </a> <h1>P4 - RDD et Batch Processing avec Spark</h1> <h2 id=partie-4-spark-rdd-et-traitement-par-lots>Partie 4 - Spark RDD et Traitement par Lots<a class=headerlink href=#partie-4-spark-rdd-et-traitement-par-lots title="Permanent link">&para;</a></h2> <p><center><img src=../img/p4/rdd.png width=500pt>[^sparkforbeginners]</center></p> <h3 id=rdd-resilient-distributed-dataset>RDD: Resilient Distributed Dataset<a class=headerlink href=#rdd-resilient-distributed-dataset title="Permanent link">&para;</a></h3> <p>Spark gravite autour du concept de "Resilient Distributed Dataset" ou RDD, qui est une collection d'éléments tolérante aux fautes qui peut être gérée en parallèle. Les RDDs utilisent la mémoire et l'espace disque selon les besoins.</p> <ul> <li><em>R pour Résilient</em>: capable de récupérer rapidement en cas de problèmes ou de conditions difficiles,</li> <li><em>D pour Distribué</em>: partage les données sur les différents noeuds participants pour une exécution parallèle,</li> <li><em>D pour Dataset</em>: une collection de données composée d'éléments séparés mais qui sont manipulés comme une unité compacte.</li> </ul> <p>Il existe deux moyens de créer les RDDs [^spark-official]:</p> <ul> <li>Paralléliser une collection existante en mémoire dans le programme Driver.</li> <li>Le générer à partir d'un fichier enregistré sur un support de stockage externe.</li> </ul> <h4 id=parallelisation-de-collections>Parallélisation de Collections<a class=headerlink href=#parallelisation-de-collections title="Permanent link">&para;</a></h4> <p>Les collections parallélisées sont créées en appelant la méthode <code>parallelize</code> du <code>JavaSparkContext</code> sur une collection existante dans votre programme Driver. Les éléments de la collection sont copiés pour former une structure distribuée qui peut être traitée en parallèle.</p> <p>Par exemple, nous pouvons créer une collection parallélisée à partir d'une liste contenant les chiffres de 1 à 5:</p> <div class=highlight><pre><span></span><code>  <span class=n>List</span><span class=o>&lt;</span><span class=n>Integer</span><span class=o>&gt;</span> <span class=n>data</span> <span class=o>=</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=mi>2</span><span class=p>,</span> <span class=mi>3</span><span class=p>,</span> <span class=mi>4</span><span class=p>,</span> <span class=mi>5</span><span class=p>);</span>
  <span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>Integer</span><span class=o>&gt;</span> <span class=n>distData</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>parallelize</span><span class=p>(</span><span class=n>data</span><span class=p>);</span>
</code></pre></div> <p>Une fois créée, la structure distribuée <code>distData</code> peut être traitée en parallèle. Par exemple, il est possible d'appeler <code>distData.reduce((a,b)-&gt; a + b)</code> pour faire la somme de tous les éléments de la liste.</p> <p>Un paramètre important à définir dans une collection parallélisée, c'est le nombre de partitions à utiliser pour diviser la collection. Spark exécute une tâche pour chaque partition du cluster. En temps normal, Spark essaiera de définir le nombre de partitions automatiquement selon votre cluster, cependant, il est possible de le définir manuellement en le passant comme second paramètre de la fonction <code>parallelize</code>:</p> <div class=highlight><pre><span></span><code>  <span class=n>sc</span><span class=p>.</span><span class=na>parallelize</span><span class=p>(</span><span class=n>data</span><span class=p>,</span> <span class=mi>10</span><span class=p>)</span>
</code></pre></div> <h4 id=generation-a-partir-dun-fichier-externe>Génération à partir d'un fichier externe<a class=headerlink href=#generation-a-partir-dun-fichier-externe title="Permanent link">&para;</a></h4> <p>Spark peut créer une collection distribuée à partir de n'importe quelle source de stockage supportée par Hadoop, incluant votre propre système de stockage, HDFS, Cassandra, HBase, Amazon S3, etc.</p> <p>Il est possible de créer des RDDs à partir de fichiers texte en utilisant la méthode <code>textfile</code> du <code>SparkContext</code>. Cette méthode prend l'URI du fichier (chemin du fichier local, ou bien en utilisant <code>hdfs://</code> ou <code>s3://</code>), et le lit comme une collection de lignes. Par exemple:</p> <div class=highlight><pre><span></span><code><span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>distFile</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>textFile</span><span class=p>(</span><span class=s>&quot;data.txt&quot;</span><span class=p>);</span>
</code></pre></div> <h3 id=operations-sur-les-rdds>Opérations sur les RDDs<a class=headerlink href=#operations-sur-les-rdds title="Permanent link">&para;</a></h3> <p>Les RDDs supportent deux types d'opérations:</p> <ul> <li>les <em>transformations</em>, qui permettent de créer une nouvelle collection à partir d'un RDD existant</li> <li>les <em>actions</em>, qui retournent une valeur au programme <em>driver</em> après avoir exécuté un calcul sur le RDD.</li> </ul> <p>Par exemple, un <em>map</em> est une transformation qui passe chaque élément du dataset via une fonction, et retourne un nouvel RDD représentant les résultats. Un <em>reduce</em> est une action qui agrège tous les éléments du RDD en utilisant une certaine fonction et retourne le résultat final au programme.</p> <p><center><img src=../img/p4/operations.png width=500pt>[^devopedia]</center></p> <h3 id=caracteristiques-des-rdds>Caractéristiques des RDDs<a class=headerlink href=#caracteristiques-des-rdds title="Permanent link">&para;</a></h3> <h4 id=immutabilite-et-lignee>Immutabilité et Lignée<a class=headerlink href=#immutabilite-et-lignee title="Permanent link">&para;</a></h4> <p>Les RDDs dans Spark sont des collection <em>immuables</em>, c'est à dire qu'elle ne sont jamais modifiées: toute transformation va créer un nouvel RDD au lieu de modifier le RDD initial. Quand un nouvel RDD a été créé à partir d'un RDD existant, ce nouvel RDD contient un pointeur vers le RDD parent. De même, toutes les dépendances entre les RDDs sont loggées dans un graphe, plutôt que directement sur les données. Ce graphe s'appelle <em>Graphe de Lignée</em> ou <em>Lineage Graph</em>.</p> <p>Par exemple, si on considère les opérations suivantes:</p> <ol> <li>Créer un nouvel RDD à partir d'un fichier texte -&gt; RDD1</li> <li>Appliquer des opérations de Map sur RDD1 -&gt; RDD2</li> <li>Appliquer une opération de filtrage sur RDD2 -&gt; RDD3</li> <li>Appliquer une opération de comptage sur RDD3 -&gt; RDD4</li> </ol> <p>Le graphe de lignée associé à ces opérations ressemble à ce qui suit:</p> <p><center><img src=../img/p4/dag.png width=300pt></center></p> <p>Ce graphe peut être utile au cas où certaines partitions sont perdues. Spark peut rejouer la transformation sur cette partition en utilisant le graphe de lignée existant pour réaliser le même calcul, plutôt que de répliquer les données à partir de des différents noeuds du cluster.</p> <p>Il est également utile en cas de réutilisation d'un graphe existant. Si par exemple on désire appliquer une opération tri sur RDD2, il est inutile de recharger le fichier une deuxième fois à partir du disque. Il suffit de modifier le graphe pour qu'il devienne comme suit:</p> <p><center><img src=../img/p4/dag2.png width=300pt></center></p> <h4 id=lazy-evaluation>Lazy Evaluation<a class=headerlink href=#lazy-evaluation title="Permanent link">&para;</a></h4> <p>Toutes les transformations dans Spark sont <em>lazy</em> (fainéantes), car elles ne calculent pas le résultat immédiatement. Elles se souviennent des transformations appliquées à un dataset de base (par ex. un fichier). Les transformations ne sont calculées que quand une action nécessite qu'un résultat soit retourné au programme principal. Cela permet à Spark de s'exécuter plus efficacement.</p> <h3 id=exemple>Exemple<a class=headerlink href=#exemple title="Permanent link">&para;</a></h3> <p>L'exemple que nous allons présenter ici par étapes permet de relever les mots les plus fréquents dans un fichier. Pour cela, le code suivant est utilisé:</p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 1 - Créer un RDD à partir d&#39;un fichier texte</span>
  <span class=kd>val</span> <span class=n>docs</span> <span class=o>=</span> <span class=n>spark</span><span class=p>.</span><span class=n>textFile</span><span class=p>(</span><span class=s>&quot;/docs&quot;</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/p4/ex1.png width=500></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 2 - Convertir les lignes en minuscule</span>
  <span class=kd>val</span> <span class=n>lower</span> <span class=o>=</span> <span class=n>docs</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>line</span> <span class=o>=&gt;</span> <span class=n>line</span><span class=p>.</span><span class=n>toLowerCase</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/p4/ex2.png width=500></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 3 - Séparer les lignes en mots</span>
  <span class=kd>val</span> <span class=n>words</span> <span class=o>=</span> <span class=n>lower</span><span class=p>.</span><span class=n>flatMap</span><span class=p>(</span><span class=n>line</span> <span class=o>=&gt;</span> <span class=n>line</span><span class=p>.</span><span class=n>split</span><span class=p>(</span><span class=s>&quot;\\s+&quot;</span><span class=p>))</span>
</code></pre></div> <center><img src=../img/p4/ex3.png width=500></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 4 - produire les tuples (mot, 1)</span>
  <span class=kd>val</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>words</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>word</span> <span class=o>=&gt;</span> <span class=p>(</span><span class=n>word</span><span class=p>,</span><span class=mi>1</span><span class=p>))</span>
</code></pre></div> <center><img src=../img/p4/ex4.png width=500></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 5 - Compter tous les mots</span>
  <span class=kd>val</span> <span class=n>freq</span> <span class=o>=</span> <span class=n>counts</span><span class=p>.</span><span class=n>reduceByKey</span><span class=p>(</span><span class=n>_</span> <span class=o>+</span> <span class=n>_</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/p4/ex5.png width=500></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 6 - Inverser les tuples (transformation avec swap)</span>
  <span class=n>freq</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>_</span><span class=p>.</span><span class=n>swap</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/p4/ex6.png width=400></center></p> <p><div class=highlight><pre><span></span><code>  <span class=c1>//Etape 7 - Inverser les tuples (action de sélection des n premiers)</span>
  <span class=kd>val</span> <span class=n>top</span> <span class=o>=</span> <span class=n>freq</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>_swap</span><span class=p>).</span><span class=n>top</span><span class=p>(</span><span class=nc>N</span><span class=p>)</span>
</code></pre></div> <center><img src=../img/p4/ex7.png width=500></center></p> <h3 id=test-de-spark-avec-spark-shell>Test de Spark avec Spark-Shell<a class=headerlink href=#test-de-spark-avec-spark-shell title="Permanent link">&para;</a></h3> <p>Nous allons tester le comportement de Spark et des RDD en utilisant l'exemple type pour l'analyse des données: le Wordcount, qui permet de compter le nombre de mots dans un fichier donné en entrée.</p> <p>Commençons par lancer le cluster Spark installé dans la partie <a href=../p3-install/index.html>P3</a>.</p> <div class=highlight><pre><span></span><code>  docker start spark-master spark-slave1 spark-slave2
</code></pre></div> <p>Entrer dans le noeud Master comme suit:</p> <div class=highlight><pre><span></span><code>  docker <span class=nb>exec</span> -it spark-master bash
</code></pre></div> <p>Dans le but de tester l'exécution de spark, commencer par créer un fichier <em>input/file1.txt</em> dans le répertoire <code>/root</code>:</p> <div class=highlight><pre><span></span><code>  mkdir /root/input
  vim /root/input/file1.txt
</code></pre></div> <p>Remplir le fichier avec le texte suivant, ou tout texte de votre choix (vous devez taper <code>i</code> pour passer en mode édition):</p> <div class=highlight><pre><span></span><code>  Hello Spark Wordcount!
  Hello everybody else!
</code></pre></div> <p>Lancer Spark Shell en utilisant la commande suivante: <div class=highlight><pre><span></span><code>  spark-shell
</code></pre></div></p> <p>Vous devriez avoir un résultat semblable au suivant: <img alt="Spark Shell" src=../img/p3/spark-shell.png></p> <p>Vous pourrez tester spark avec un code scala simple comme suit (à exécuter ligne par ligne):</p> <div class=highlight><pre><span></span><code>  <span class=kd>val</span> <span class=n>lines</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=n>textFile</span><span class=p>(</span><span class=s>&quot;/root/input/file1.txt&quot;</span><span class=p>)</span>
  <span class=kd>val</span> <span class=n>words</span> <span class=o>=</span> <span class=n>lines</span><span class=p>.</span><span class=n>flatMap</span><span class=p>(</span><span class=n>_</span><span class=p>.</span><span class=n>split</span><span class=p>(</span><span class=s>&quot;\\s+&quot;</span><span class=p>))</span>
  <span class=kd>val</span> <span class=n>wc</span> <span class=o>=</span> <span class=n>words</span><span class=p>.</span><span class=n>map</span><span class=p>(</span><span class=n>w</span> <span class=o>=&gt;</span> <span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=mi>1</span><span class=p>)).</span><span class=n>reduceByKey</span><span class=p>(</span><span class=n>_</span> <span class=o>+</span> <span class=n>_</span><span class=p>)</span>
  <span class=n>wc</span><span class=p>.</span><span class=n>saveAsTextFile</span><span class=p>(</span><span class=s>&quot;/root/file1.count&quot;</span><span class=p>)</span>
</code></pre></div> <p>Ce code vient de (1) charger le fichier <em>file1.txt</em> du système de fichier courant, (2) séparer les mots selon les caractères d'espacement, (3) appliquer un <em>map</em> sur les mots obtenus qui produit le couple (<em>&lt;mot></em>, 1), puis un <em>reduce</em> qui permet de faire la somme des 1 des mots identiques.</p> <p>Pour afficher le résultat, sortir de spark-shell en cliquant sur <em>Ctrl-C</em>. Afficher ensuite le contenu du fichier <em>part-00000</em> du répertoire <em>file1.count</em> créé, comme suit: <div class=highlight><pre><span></span><code>  cat /root/file1.count/part-00000
</code></pre></div> Le contenu des deux fichiers <em>part-00000</em> et <em>part-00001</em> ressemble à ce qui suit:</p> <p><center><img src=../img/p4/result.png width=400px></center></p> <h3 id=spark-batch-en-java>Spark Batch en Java<a class=headerlink href=#spark-batch-en-java title="Permanent link">&para;</a></h3> <h4 id=preparation-de-lenvironnement-et-code>Préparation de l'environnement et Code<a class=headerlink href=#preparation-de-lenvironnement-et-code title="Permanent link">&para;</a></h4> <p>Nous allons dans cette partie créer un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.</p> <ol> <li>Sur votre propre machine, créer un projet Maven avec IntelliJ IDEA (ou tout IDE de votre choix), en utilisant la config suivante: <div class=highlight><pre><span></span><code>  <span class=nt>&lt;groupId&gt;</span>spark.batch<span class=nt>&lt;/groupId&gt;</span>
  <span class=nt>&lt;artifactId&gt;</span>wordcount<span class=nt>&lt;/artifactId&gt;</span>
  <span class=nt>&lt;version&gt;</span>1<span class=nt>&lt;/version&gt;</span>
</code></pre></div> <center><img src=../img/p4/proj1.png width=400px></center></li> <li>Rajouter dans le fichier pom les dépendances nécessaires, et indiquer la version du compilateur Java: <div class=highlight><pre><span></span><code><span class=nt>&lt;properties&gt;</span>
    <span class=nt>&lt;maven.compiler.source&gt;</span>1.8<span class=nt>&lt;/maven.compiler.source&gt;</span>
    <span class=nt>&lt;maven.compiler.target&gt;</span>1.8<span class=nt>&lt;/maven.compiler.target&gt;</span>
<span class=nt>&lt;/properties&gt;</span>
<span class=nt>&lt;dependencies&gt;</span>
    <span class=cm>&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span>
    <span class=nt>&lt;dependency&gt;</span>
        <span class=nt>&lt;groupId&gt;</span>org.apache.spark<span class=nt>&lt;/groupId&gt;</span>
        <span class=nt>&lt;artifactId&gt;</span>spark-core_2.12<span class=nt>&lt;/artifactId&gt;</span>
        <span class=nt>&lt;version&gt;</span>2.4.5<span class=nt>&lt;/version&gt;</span>
    <span class=nt>&lt;/dependency&gt;</span>
<span class=nt>&lt;/dependencies&gt;</span>
</code></pre></div></li> <li>Sous le répertoire java, créer un package que vous appellerez <em>tn.spark.batch</em>, et dedans, une classe appelée <em>WordCountTask</em>.</li> <li>Écrire le code suivant dans <em>WordCountTask</em>:</li> </ol> <div class=highlight><pre><span></span><code><span class=kn>package</span> <span class=nn>tn.spark.batch</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>org.apache.spark.SparkConf</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaPairRDD</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaRDD</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.apache.spark.api.java.JavaSparkContext</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.slf4j.Logger</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>org.slf4j.LoggerFactory</span><span class=p>;</span>
<span class=kn>import</span> <span class=nn>scala.Tuple2</span><span class=p>;</span>

<span class=kn>import</span> <span class=nn>java.util.Arrays</span><span class=p>;</span>

<span class=kn>import static</span> <span class=nn>jersey.repackaged.com.google.common.base.Preconditions.checkArgument</span><span class=p>;</span>
<span class=kd>public</span> <span class=kd>class</span> <span class=nc>WordCountTask</span> <span class=p>{</span>
    <span class=kd>private</span> <span class=kd>static</span> <span class=kd>final</span> <span class=n>Logger</span> <span class=n>LOGGER</span> <span class=o>=</span> <span class=n>LoggerFactory</span><span class=p>.</span><span class=na>getLogger</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>

    <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>checkArgument</span><span class=p>(</span><span class=n>args</span><span class=p>.</span><span class=na>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>,</span> <span class=s>&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class=p>);</span>
        <span class=k>new</span> <span class=n>WordCountTask</span><span class=p>().</span><span class=na>run</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>]</span><span class=p>,</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>]</span><span class=p>);</span>
    <span class=p>}</span>

    <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=p>(</span><span class=n>String</span> <span class=n>inputFilePath</span><span class=p>,</span> <span class=n>String</span> <span class=n>outputDir</span><span class=p>)</span> <span class=p>{</span>
        <span class=n>String</span> <span class=n>master</span> <span class=o>=</span> <span class=s>&quot;local[*]&quot;</span><span class=p>;</span>
        <span class=n>SparkConf</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>SparkConf</span><span class=p>()</span>
                <span class=p>.</span><span class=na>setAppName</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>.</span><span class=na>getName</span><span class=p>())</span>
                <span class=p>.</span><span class=na>setMaster</span><span class=p>(</span><span class=n>master</span><span class=p>);</span>
        <span class=n>JavaSparkContext</span> <span class=n>sc</span> <span class=o>=</span> <span class=k>new</span> <span class=n>JavaSparkContext</span><span class=p>(</span><span class=n>conf</span><span class=p>);</span>

        <span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>textFile</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>textFile</span><span class=p>(</span><span class=n>inputFilePath</span><span class=p>);</span>
        <span class=n>JavaPairRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=p>,</span> <span class=n>Integer</span><span class=o>&gt;</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>textFile</span>
                <span class=p>.</span><span class=na>flatMap</span><span class=p>(</span><span class=n>s</span> <span class=o>-&gt;</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=n>s</span><span class=p>.</span><span class=na>split</span><span class=p>(</span><span class=s>&quot; &quot;</span><span class=p>)).</span><span class=na>iterator</span><span class=p>())</span>
                <span class=p>.</span><span class=na>mapToPair</span><span class=p>(</span><span class=n>word</span> <span class=o>-&gt;</span> <span class=k>new</span> <span class=n>Tuple2</span><span class=o>&lt;&gt;</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
                <span class=p>.</span><span class=na>reduceByKey</span><span class=p>((</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>);</span>
        <span class=n>counts</span><span class=p>.</span><span class=na>saveAsTextFile</span><span class=p>(</span><span class=n>outputDir</span><span class=p>);</span>
    <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <p>La première chose à faire dans un programme Spark est de créer un objet <em>JavaSparkContext</em>, qui indique à Spark comment accéder à un cluster. Pour créer ce contexte, vous aurez besoin de construire un objet <em>SparkConf</em> qui contient toutes les informations sur l'application.</p> <ul> <li><em>appName</em> est le nom de l'application</li> <li><em>master</em> est une URL d'un cluster Spark, Mesos ou YARN, ou bien une chaîne spéciale <em>local</em> pour lancer le job en mode local.</li> </ul> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>Nous avons indiqué ici que notre master est <em>local</em> pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet déconseillé de la hard-coder dans le programme, il faudrait plutôt l'indiquer comme option de commande à chaque fois que nous lançons le job.</p> </div> <p>Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.</p> <h4 id=test-du-code-en-local>Test du code en local<a class=headerlink href=#test-du-code-en-local title="Permanent link">&para;</a></h4> <p>Pour tester le code sur votre machine, procéder aux étapes suivantes:</p> <ol> <li>Créer un fichier texte de votre choix (par exemple le fameux loremipsum.txt, que vous pourrez générer <a href=https://generator.lorem-ipsum.info/_latin>ici</a>) dans le répertoire src/main/resources.</li> <li>Créer une nouvelle configuration de type "Application" (<em>Run-&gt;Edit Configurations</em>): <center><img src=../img/p4/proj2.png width=600px></center> <center><img src=../img/p4/proj3.png width=600px></center></li> <li>La nommer <em>WordCountTask</em>, et définir les arguments suivants (fichier de départ et répertoire d'arrivée) comme <em>Program arguments</em>: <div class=highlight><pre><span></span><code>  src/main/resources/loremipsum.txt src/main/resources/out
</code></pre></div> <center><img src=../img/p4/proj4.png width=600px></center></li> <li>Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un répertoire <em>out</em> sera créé sous <em>resources</em>, qui contient deux fichiers: part-00000, part-00001.</li> </ol> <p><center><img src=../img/p4/proj5.png width=600px></center></p> <h4 id=lancement-du-code-sur-le-cluster>Lancement du code sur le cluster<a class=headerlink href=#lancement-du-code-sur-le-cluster title="Permanent link">&para;</a></h4> <p>Pour exécuter le code sur le cluster, modifier comme indiqué les lignes en jaune dans ce qui suit:</p> <div class=highlight><pre><span></span><code><span class=kd>public</span> <span class=kd>class</span> <span class=nc>WordCountTask</span> <span class=p>{</span>
  <span class=kd>private</span> <span class=kd>static</span> <span class=kd>final</span> <span class=n>Logger</span> <span class=n>LOGGER</span> <span class=o>=</span> <span class=n>LoggerFactory</span><span class=p>.</span><span class=na>getLogger</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>);</span>

  <span class=kd>public</span> <span class=kd>static</span> <span class=kt>void</span> <span class=nf>main</span><span class=p>(</span><span class=n>String</span><span class=o>[]</span> <span class=n>args</span><span class=p>)</span> <span class=p>{</span>
      <span class=n>checkArgument</span><span class=p>(</span><span class=n>args</span><span class=p>.</span><span class=na>length</span> <span class=o>&gt;</span> <span class=mi>1</span><span class=p>,</span> <span class=s>&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class=p>);</span>
      <span class=k>new</span> <span class=n>WordCountTask</span><span class=p>().</span><span class=na>run</span><span class=p>(</span><span class=n>args</span><span class=o>[</span><span class=mi>0</span><span class=o>]</span><span class=p>,</span> <span class=n>args</span><span class=o>[</span><span class=mi>1</span><span class=o>]</span><span class=p>);</span>
  <span class=p>}</span>

  <span class=kd>public</span> <span class=kt>void</span> <span class=nf>run</span><span class=p>(</span><span class=n>String</span> <span class=n>inputFilePath</span><span class=p>,</span> <span class=n>String</span> <span class=n>outputDir</span><span class=p>)</span> <span class=p>{</span>

<span class=hll>      <span class=n>SparkConf</span> <span class=n>conf</span> <span class=o>=</span> <span class=k>new</span> <span class=n>SparkConf</span><span class=p>()</span>
</span><span class=hll>              <span class=p>.</span><span class=na>setAppName</span><span class=p>(</span><span class=n>WordCountTask</span><span class=p>.</span><span class=na>class</span><span class=p>.</span><span class=na>getName</span><span class=p>());</span>
</span>
      <span class=n>JavaSparkContext</span> <span class=n>sc</span> <span class=o>=</span> <span class=k>new</span> <span class=n>JavaSparkContext</span><span class=p>(</span><span class=n>conf</span><span class=p>);</span>

      <span class=n>JavaRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=o>&gt;</span> <span class=n>textFile</span> <span class=o>=</span> <span class=n>sc</span><span class=p>.</span><span class=na>textFile</span><span class=p>(</span><span class=n>inputFilePath</span><span class=p>);</span>
      <span class=n>JavaPairRDD</span><span class=o>&lt;</span><span class=n>String</span><span class=p>,</span> <span class=n>Integer</span><span class=o>&gt;</span> <span class=n>counts</span> <span class=o>=</span> <span class=n>textFile</span>
              <span class=p>.</span><span class=na>flatMap</span><span class=p>(</span><span class=n>s</span> <span class=o>-&gt;</span> <span class=n>Arrays</span><span class=p>.</span><span class=na>asList</span><span class=p>(</span><span class=n>s</span><span class=p>.</span><span class=na>split</span><span class=p>(</span><span class=s>&quot; &quot;</span><span class=p>)).</span><span class=na>iterator</span><span class=p>())</span>
              <span class=p>.</span><span class=na>mapToPair</span><span class=p>(</span><span class=n>word</span> <span class=o>-&gt;</span> <span class=k>new</span> <span class=n>Tuple2</span><span class=o>&lt;&gt;</span><span class=p>(</span><span class=n>word</span><span class=p>,</span> <span class=mi>1</span><span class=p>))</span>
              <span class=p>.</span><span class=na>reduceByKey</span><span class=p>((</span><span class=n>a</span><span class=p>,</span> <span class=n>b</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>a</span> <span class=o>+</span> <span class=n>b</span><span class=p>);</span>
      <span class=n>counts</span><span class=p>.</span><span class=na>saveAsTextFile</span><span class=p>(</span><span class=n>outputDir</span><span class=p>);</span>
  <span class=p>}</span>
<span class=p>}</span>
</code></pre></div> <p>Lancer ensuite une configuration de type Maven, avec les commandes <em>package install</em>. <center><img src=../img/p4/proj6.png width=600px></center></p> <p>Un fichier intitulé <em>worcount-1.jar</em> sera créé sous le répertoire <em>target</em>.</p> <p>Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le répertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:</p> <div class=highlight><pre><span></span><code>  docker cp target/wordcount-1.jar spark-master:/root/wordcount-1.jar
</code></pre></div> <p>Copier également le fichier <em>loremipsum.txt</em> que vous avez créé dans votre projet:</p> <div class=highlight><pre><span></span><code>  docker cp src/main/resources/loremipsum.txt spark-master:/root/input/loremipsum.txt
</code></pre></div> <p>Aller à votre contenaire spark-master (en utilisant la commande <code>docker exec ...</code>), et lancer un job Spark en utilisant ce fichier jar généré, avec la commande <code>spark-submit</code>, un script utilisé pour lancer des applications spark sur un cluster.</p> <div class=highlight><pre><span></span><code>  <span class=nb>cd</span> /root
  spark-submit  --class tn.spark.batch.WordCountTask --master <span class=nb>local</span> wordcount-1.jar input/loremipsum.txt output
</code></pre></div> <ul> <li>Nous avons lancé le job en mode local, pour commencer.</li> <li>Le fichier en entrée est le fichier loremipsum.txt, et le résultat sera stocké dans un répertoire <em>output</em>.</li> </ul> <p>Si tout se passe bien, vous devriez trouver, dans le répertoire <em>output</em>, un fichier part-00000, qui ressemble à ce qui suit:</p> <p><center><img src=../img/p4/proj7.png width=600></center></p> <h3 id=references>Références<a class=headerlink href=#references title="Permanent link">&para;</a></h3> <p>[^sparkforbeginners]: Spark for beginners, <em>RDD in Spark</em>, <a href=http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html>http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html</a>, consulté le 03/2020</p> <p>[^spark-official]: Spark Documentation, <em>Resilient Distributed Datasets(RDDs)</em>, <a href=https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds>https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a>, consulté le 03/2020</p> <p>[^devopedia]: Devopedia, <em>Apache Spark</em>, <a href=https://devopedia.org/apache-spark>https://devopedia.org/apache-spark</a>, consulté le 03/2020</p> <hr> <div class=md-source-file> <small> Last update: <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-timeago"><span class=timeago datetime=2020-03-08T12:10:48+01:00 locale=en></span></span><span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-iso_date">2020-03-08</span> </small> </div> </article> </div> </div> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../p3-install/ class="md-footer__link md-footer__link--prev" aria-label="Previous: P3 - Installation de Spark" rel=prev> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg> </div> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Previous </span> P3 - Installation de Spark </div> </div> </a> <a href=../p5-sql/ class="md-footer__link md-footer__link--next" aria-label="Next: P5 - Spark SQL" rel=next> <div class=md-footer__title> <div class=md-ellipsis> <span class=md-footer__direction> Next </span> P5 - Spark SQL </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2020 Lilia Sfaxi </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../assets/javascripts/workers/search.092fa1f6.min.js"}</script> <script src=../assets/javascripts/bundle.e3b2bf44.min.js></script> <script src=../js/timeago.min.js></script> <script src=../js/timeago_mkdocs_material.js></script> </body> </html>