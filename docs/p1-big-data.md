# Partie 1 - Introduction au Big Data
![Big Data](img/p1/big-data.jpeg)

## Les "Big Data", Pourquoi?
L'être humain, à travers l'humanité, a toujours cherché trois choses: Savoir (qu'est-ce qui s'est passé?), Comprendre (pourquoi cela s'est-il passé?) et Prédire (qu'est-ce que qui se passera?). Plusieurs cultures ont clamé l'omniscience en ayant recours à des subterfuges, tels que les oracles, l'astrologie, le tarot, ou les boules de cristal.

Cela dit, ces moyens ne sont guères satisfaisants à l'esprit méticuleux du scientifique, qui cherche toujours une explication logique et rationnelle à tout évènement, et une justification convainquante à tout comportement. Le scientifique se base sur des faits. Il veut arriver à faire de la magie grâce à la technologie.

Pour arriver à ces fins, le scientifique a besoin de données. L'intérêt de collecter des données et de les exploiter a longtemps été négligé, et a été limité au peu de données, jugées "utiles", qui semblaient suffisantes pour atteindre un objectif immédiat. Cependant, adopter le chemin évident et peu risqué n'aurait jamais permis de réaliser les miracles auxquelles on s'attendait. Il fallait trouver un autre moyen..

Le terme Big Data est apparu peu de temps après l'apparition du terme Web 2.0, qui montre la transition de l'internet d'une ère où l'ajout des données était exclusivement réservé à une élite experte, où le volume des données disponible était petit mais où les données étaient précieuses et pertinentes, vers une ère où tout un chacun était capable d'introduire des connaissances, véridiques ou pas, qui seraient sauvegardées dans une mémoire collective jusqu'à la fin des temps. Ce changement de paradigme a entrainé le besoin d'infrastructures nouvelles, qui seraient capables, non seulement de stocker ces données, mais également d'en extraire de la valeur.

Ces infrastructures auront la capacité de gérer toute la chaîne logistique des données, de la collecte vers l'affichage. Cela semble évident, me direz-vous, car les systèmes classiques sont capables de faire cela. Qui stocke mieux les données qu'une bonne vieille base de données relationnelle? Le problème est que les données dites "Big Data" sont caractérisées par des propriétés telles que, les systèmes classiques de stockage et de traitement auraient du mal à les exploiter à leur juste valeur.

## Caractéristiques des Données Massives
Le terme "données massives", ou "Big Data", ne donne à mon avis pas entièrement justice aux données de notre domaine. En effet, il représente une seule caractéristique parmis plusieurs, qui est le Volume, qui, même si elle semble être la plus importante, est loin d'être la plus critique.

En effet, les données massives sont caractérisées par les fameux *V. Il en existe plusieurs (11 jusqu'à ce jour si je ne m'abuse), mais pourraient à mon avis être résumés en trois caractéristiques primordiales, autours de la combinaison desquelles tournent toutes les décisions prises dans ce domaine.

  - **Volume** : C'est évidemment le V le plus manifeste, qui caractérise le fait que les données ont un volume énorme qui peut atteindre des valeurs de l'ordre de Exa-, Zetta- ou Yottaoctet (allant jusqu'à  $2^{80}$ octets!). Mais ceci n'est pas tout. Un volume énorme, s'il reste constant, est gérable: il suffit de trouver une machine suffisamment puissante pour le gérer. Le problème avec la propriété du volume, c'est qu'il augmente de façon continue, ce qui rend sa gestion beaucoup plus ardue. Une citation bien connue, et qui se re-confirme chaque année, atteste que _"Over the last two years alone 90 percent of the data in the world was generated."_ Il est donc primordial de trouver un moyen de gérer ce volume toujours croissant des données.
  - **Vélocité** : Cette propriété est, à mon avis, la plus problématique des trois, car, couplée avec le volume, elle rend les système actuels obsolètes. En effet, la vélocité est, littéralement, "La vitesse avec laquelle quelque chose se déplace dans une direction particulière". Dans notre cas, la vélocité des données est la responsable directe du volume croissant des données dans le système. Elle est provoquée par une arrivée des données dans le système sous la forme d'un flux constant qui demande à être stocké et traité immédiatement, ainsi que le besoin croissant des utilisateurs d'avoir une représentation récente et fidèle de l'état des données. D'ailleurs, cette propriété a engendré une autre préoccupation des analystes des données, qui est de fournir une introspection en temps réel sur les données, les qualifiant ainsi de "Fast Data".
  - **Variété** :

## Principes de base du Domaine des Big Data

### Stocker d'abord, réfléchir ensuite

### Absolument TOUTES les données sont importantes!

### Ce sont les données qui pilotent le traitement

### La redondance, c'est bien

### Sois Polyglotte!

## Infrastructure Big Data: Besoins

## Théorème CAP

## Technologies et Paradigmes
### Technologies d'Ingestion de Données

### Technologies de Stockage de Données

### Technologies de Traitement de Données
