<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Lilia Sfaxi">
        <link rel="canonical" href="http://liliasfaxi.github.io/Atelier-Spark/p4-batch/">
        <link rel="shortcut icon" href="../img/favicon.ico">
        <title>P4 - RDD et Batch Processing avec Spark - Atelier Apache Spark</title>
        <link href="../css/bootstrap.min.css" rel="stylesheet">
        <link href="../css/font-awesome.min.css" rel="stylesheet">
        <link href="../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/styles/github.min.css">

        <script src="../js/jquery-1.10.2.min.js" defer></script>
        <script src="../js/bootstrap.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.5.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>
        <div class="navbar fixed-top navbar-expand-lg navbar-dark bg-primary">
            <div class="container">
                <a class="navbar-brand" href="..">Atelier Apache Spark</a>
                <!-- Expander button -->
                <button type="button" class="navbar-toggler" data-toggle="collapse" data-target="#navbar-collapse">
                    <span class="navbar-toggler-icon"></span>
                </button>

                <!-- Expanded navigation -->
                <div id="navbar-collapse" class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="navitem">
                                <a href=".." class="nav-link">Atelier Apache Spark</a>
                            </li>
                            <li class="navitem">
                                <a href="../p1-big-data/" class="nav-link">P1 - Introduction au Big Data</a>
                            </li>
                            <li class="navitem">
                                <a href="../p2-spark/" class="nav-link">P2 - Introduction à Apache Spark</a>
                            </li>
                            <li class="navitem">
                                <a href="../p3-install/" class="nav-link">P3 - Installation de Spark</a>
                            </li>
                            <li class="navitem active">
                                <a href="./" class="nav-link">P4 - RDD et Batch Processing avec Spark</a>
                            </li>
                            <li class="navitem">
                                <a href="../p5-sql/" class="nav-link">P5 - Spark SQL</a>
                            </li>
                            <li class="navitem">
                                <a href="../p6-stream/" class="nav-link">P6 - Spark Streaming</a>
                            </li>
                            <li class="navitem">
                                <a href="../p7-ml/" class="nav-link">P7 - Spark MLLib</a>
                            </li>
                            <li class="navitem">
                                <a href="../p8-graphx/" class="nav-link">P8 - Spark GraphX</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav ml-auto">
                        <li class="nav-item">
                            <a href="#" class="nav-link" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li class="nav-item">
                                <a rel="prev" href="../p3-install/" class="nav-link">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li class="nav-item">
                                <a rel="next" href="../p5-sql/" class="nav-link">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                            <li class="nav-item">
                                <a href="https://github.com/liliasfaxi/Atelier-Spark/edit/master/docs/p4-batch.md" class="nav-link">Edit on liliasfaxi/Atelier-Spark</a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
            <div class="row">
                    <div class="col-md-3"><div class="navbar-light navbar-expand-md bs-sidebar hidden-print affix" role="complementary">
    <div class="navbar-header">
        <button type="button" class="navbar-toggler collapsed" data-toggle="collapse" data-target="#toc-collapse" title="Table of Contents">
            <span class="fa fa-angle-down"></span>
        </button>
    </div>

    
    <div id="toc-collapse" class="navbar-collapse collapse card bg-secondary">
        <ul class="nav flex-column">
            
            <li class="nav-item" data-level="1"><a href="#partie-4-spark-rdd-et-traitement-par-lots" class="nav-link">Partie 4 - Spark RDD et Traitement par Lots</a>
              <ul class="nav flex-column">
            <li class="nav-item" data-level="2"><a href="#rdd-resilient-distributed-dataset" class="nav-link">RDD: Resilient Distributed Dataset</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#operations-sur-les-rdds" class="nav-link">Opérations sur les RDDs</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#caracteristiques-des-rdds" class="nav-link">Caractéristiques des RDDs</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#exemple" class="nav-link">Exemple</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#test-de-spark-avec-spark-shell" class="nav-link">Test de Spark avec Spark-Shell</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#spark-batch-en-java" class="nav-link">Spark Batch en Java</a>
              <ul class="nav flex-column">
              </ul>
            </li>
            <li class="nav-item" data-level="2"><a href="#references" class="nav-link">Références</a>
              <ul class="nav flex-column">
              </ul>
            </li>
              </ul>
            </li>
        </ul>
    </div>
</div></div>
                    <div class="col-md-9" role="main">

<h1 id="partie-4-spark-rdd-et-traitement-par-lots">Partie 4 - Spark RDD et Traitement par Lots</h1>
<p><center><img src="../img/p4/rdd.png" width="500pt">[^sparkforbeginners]</center></p>
<h2 id="rdd-resilient-distributed-dataset">RDD: Resilient Distributed Dataset</h2>
<p>Spark gravite autour du concept de "Resilient Distributed Dataset" ou RDD, qui est une collection d'éléments tolérante aux fautes qui peut être gérée en parallèle. Les RDDs utilisent la mémoire et l'espace disque selon les besoins.</p>
<ul>
<li><em>R pour Résilient</em>: capable de récupérer rapidement en cas de problèmes ou de conditions difficiles,</li>
<li><em>D pour Distribué</em>: partage les données sur les différents noeuds participants pour une exécution parallèle,</li>
<li><em>D pour Dataset</em>: une collection de données composée d'éléments séparés mais qui sont manipulés comme une unité compacte.</li>
</ul>
<p>Il existe deux moyens de créer les RDDs [^spark-official]:</p>
<ul>
<li>Paralléliser une collection existante en mémoire dans le programme Driver.</li>
<li>Le générer à partir d'un fichier enregistré sur un support de stockage externe.</li>
</ul>
<h3 id="parallelisation-de-collections">Parallélisation de Collections</h3>
<p>Les collections parallélisées sont créées en appelant la méthode <code>parallelize</code> du <code>JavaSparkContext</code> sur une collection existante dans votre programme Driver. Les éléments de la collection sont copiés pour former une structure distribuée qui peut être traitée en parallèle.</p>
<p>Par exemple, nous pouvons créer une collection parallélisée à partir d'une liste contenant les chiffres de 1 à 5:</p>
<pre><code class="language-Java">  List&lt;Integer&gt; data = Arrays.asList(1, 2, 3, 4, 5);
  JavaRDD&lt;Integer&gt; distData = sc.parallelize(data);
</code></pre>
<p>Une fois créée, la structure distribuée <code>distData</code> peut être traitée en parallèle. Par exemple, il est possible d'appeler <code>distData.reduce((a,b)-&gt; a + b)</code> pour faire la somme de tous les éléments de la liste.</p>
<p>Un paramètre important à définir dans une collection parallélisée, c'est le nombre de partitions à utiliser pour diviser la collection. Spark exécute une tâche pour chaque partition du cluster. En temps normal, Spark essaiera de définir le nombre de partitions automatiquement selon votre cluster, cependant, il est possible de le définir manuellement en le passant comme second paramètre de la fonction <code>parallelize</code>:</p>
<pre><code class="language-Java">  sc.parallelize(data, 10)
</code></pre>
<h3 id="generation-a-partir-dun-fichier-externe">Génération à partir d'un fichier externe</h3>
<p>Spark peut créer une collection distribuée à partir de n'importe quelle source de stockage supportée par Hadoop, incluant votre propre système de stockage, HDFS, Cassandra, HBase, Amazon S3, etc.</p>
<p>Il est possible de créer des RDDs à partir de fichiers texte en utilisant la méthode <code>textfile</code> du <code>SparkContext</code>. Cette méthode prend l'URI du fichier (chemin du fichier local, ou bien en utilisant <code>hdfs://</code> ou <code>s3://</code>), et le lit comme une collection de lignes. Par exemple:</p>
<pre><code class="language-Java">JavaRDD&lt;String&gt; distFile = sc.textFile(&quot;data.txt&quot;);
</code></pre>
<h2 id="operations-sur-les-rdds">Opérations sur les RDDs</h2>
<p>Les RDDs supportent deux types d'opérations:</p>
<ul>
<li>les <em>transformations</em>, qui permettent de créer une nouvelle collection à partir d'un RDD existant</li>
<li>les <em>actions</em>, qui retournent une valeur au programme <em>driver</em> après avoir exécuté un calcul sur le RDD.</li>
</ul>
<p>Par exemple, un <em>map</em> est une transformation qui passe chaque élément du dataset via une fonction, et retourne un nouvel RDD représentant les résultats. Un <em>reduce</em> est une action qui agrège tous les éléments du RDD en utilisant une certaine fonction et retourne le résultat final au programme.</p>
<p><center><img src="../img/p4/operations.png" width="500pt">[^devopedia]</center></p>
<h2 id="caracteristiques-des-rdds">Caractéristiques des RDDs</h2>
<h3 id="immutabilite-et-lignee">Immutabilité et Lignée</h3>
<p>Les RDDs dans Spark sont des collection <em>immuables</em>, c'est à dire qu'elle ne sont jamais modifiées: toute transformation va créer un nouvel RDD au lieu de modifier le RDD initial. Quand un nouvel RDD a été créé à partir d'un RDD existant, ce nouvel RDD contient un pointeur vers le RDD parent. De même, toutes les dépendances entre les RDDs sont loggées dans un graphe, plutôt que directement sur les données. Ce graphe s'appelle <em>Graphe de Lignée</em> ou <em>Lineage Graph</em>.</p>
<p>Par exemple, si on considère les opérations suivantes:</p>
<ol>
<li>Créer un nouvel RDD à partir d'un fichier texte -&gt; RDD1</li>
<li>Appliquer des opérations de Map sur RDD1 -&gt; RDD2</li>
<li>Appliquer une opération de filtrage sur RDD2 -&gt; RDD3</li>
<li>Appliquer une opération de comptage sur RDD3 -&gt; RDD4</li>
</ol>
<p>Le graphe de lignée associé à ces opérations ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/dag.png" width="300pt"></center></p>
<p>Ce graphe peut être utile au cas où certaines partitions sont perdues. Spark peut rejouer la transformation sur cette partition en utilisant le graphe de lignée existant pour réaliser le même calcul, plutôt que de répliquer les données à partir de des différents noeuds du cluster.</p>
<p>Il est également utile en cas de réutilisation d'un graphe existant. Si par exemple on désire appliquer une opération tri sur RDD2, il est inutile de recharger le fichier une deuxième fois à partir du disque. Il suffit de modifier le graphe pour qu'il devienne comme suit:</p>
<p><center><img src="../img/p4/dag2.png" width="300pt"></center></p>
<h3 id="lazy-evaluation">Lazy Evaluation</h3>
<p>Toutes les transformations dans Spark sont <em>lazy</em> (fainéantes), car elles ne calculent pas le résultat immédiatement. Elles se souviennent des transformations appliquées à un dataset de base (par ex. un fichier). Les transformations ne sont calculées que quand une action nécessite qu'un résultat soit retourné au programme principal. Cela permet à Spark de s'exécuter plus efficacement.</p>
<h2 id="exemple">Exemple</h2>
<p>L'exemple que nous allons présenter ici par étapes permet de relever les mots les plus fréquents dans un fichier. Pour cela, le code suivant est utilisé:</p>
<pre><code class="language-Scala">  //Etape 1 - Créer un RDD à partir d'un fichier texte
  val docs = spark.textFile(&quot;/docs&quot;)
</code></pre>
<p><center><img src="../img/p4/ex1.png" width="500"></center></p>
<pre><code class="language-Scala">  //Etape 2 - Convertir les lignes en minuscule
  val lower = docs.map(line =&gt; line.toLowerCase)
</code></pre>
<p><center><img src="../img/p4/ex2.png" width="500"></center></p>
<pre><code class="language-Scala">  //Etape 3 - Séparer les lignes en mots
  val words = lower.flatMap(line =&gt; line.split(&quot;\\s+&quot;))
</code></pre>
<p><center><img src="../img/p4/ex3.png" width="500"></center></p>
<pre><code class="language-Scala">  //Etape 4 - produire les tuples (mot, 1)
  val counts = words.map(word =&gt; (word,1))
</code></pre>
<p><center><img src="../img/p4/ex4.png" width="500"></center></p>
<pre><code class="language-Scala">  //Etape 5 - Compter tous les mots
  val freq = counts.reduceByKey(_ + _)
</code></pre>
<p><center><img src="../img/p4/ex5.png" width="500"></center></p>
<pre><code class="language-Scala">  //Etape 6 - Inverser les tuples (transformation avec swap)
  freq.map(_.swap)
</code></pre>
<p><center><img src="../img/p4/ex6.png" width="400"></center></p>
<pre><code class="language-Scala">  //Etape 7 - Inverser les tuples (action de sélection des n premiers)
  val top = freq.map(_swap).top(N)
</code></pre>
<p><center><img src="../img/p4/ex7.png" width="500"></center></p>
<h2 id="test-de-spark-avec-spark-shell">Test de Spark avec Spark-Shell</h2>
<p>Nous allons tester le comportement de Spark et des RDD en utilisant l'exemple type pour l'analyse des données: le Wordcount, qui permet de compter le nombre de mots dans un fichier donné en entrée.</p>
<p>Commençons par lancer le cluster Spark installé dans la partie <a href="../p3-install/index.html">P3</a>.</p>
<pre><code class="language-bash">  docker start spark-master spark-slave1 spark-slave2
</code></pre>
<p>Entrer dans le noeud Master comme suit:</p>
<pre><code class="language-bash">  docker exec -it spark-master bash
</code></pre>
<p>Dans le but de tester l'exécution de spark, commencer par créer un fichier <em>input/file1.txt</em> dans le répertoire <code>/root</code>:</p>
<pre><code class="language-bash">  mkdir /root/input
  vim /root/input/file1.txt
</code></pre>
<p>Remplir le fichier avec le texte suivant, ou tout texte de votre choix (vous devez taper <code>i</code> pour passer en mode édition):</p>
<pre><code class="language-text">  Hello Spark Wordcount!
  Hello everybody else!
</code></pre>
<p>Lancer Spark Shell en utilisant la commande suivante:</p>
<pre><code class="language-Bash">  spark-shell
</code></pre>
<p>Vous devriez avoir un résultat semblable au suivant:
<img alt="Spark Shell" src="../img/p3/spark-shell.png" /></p>
<p>Vous pourrez tester spark avec un code scala simple comme suit (à exécuter ligne par ligne):</p>
<pre><code class="language-Scala">  val lines = sc.textFile(&quot;/root/input/file1.txt&quot;)
  val words = lines.flatMap(_.split(&quot;\\s+&quot;))
  val wc = words.map(w =&gt; (w, 1)).reduceByKey(_ + _)
  wc.saveAsTextFile(&quot;/root/file1.count&quot;)
</code></pre>
<p>Ce code vient de (1) charger le fichier <em>file1.txt</em> du système de fichier courant, (2) séparer les mots selon les caractères d'espacement, (3) appliquer un <em>map</em> sur les mots obtenus qui produit le couple (<em>&lt;mot></em>, 1), puis un <em>reduce</em> qui permet de faire la somme des 1 des mots identiques.</p>
<p>Pour afficher le résultat, sortir de spark-shell en cliquant sur <em>Ctrl-C</em>. Afficher ensuite le contenu du fichier <em>part-00000</em> du répertoire <em>file1.count</em> créé, comme suit:</p>
<pre><code class="language-Bash">  cat /root/file1.count/part-00000
</code></pre>
<p>Le contenu des deux fichiers <em>part-00000</em> et <em>part-00001</em> ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/result.png" width=400px></center></p>
<h2 id="spark-batch-en-java">Spark Batch en Java</h2>
<h3 id="preparation-de-lenvironnement-et-code">Préparation de l'environnement et Code</h3>
<p>Nous allons dans cette partie créer un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.</p>
<ol>
<li>Sur votre propre machine, créer un projet Maven avec IntelliJ IDEA (ou tout IDE de votre choix), en utilisant la config suivante:
  <code>xml
    &lt;groupId&gt;spark.batch&lt;/groupId&gt;
    &lt;artifactId&gt;wordcount&lt;/artifactId&gt;
    &lt;version&gt;1&lt;/version&gt;</code>
<center><img src="../img/p4/proj1.png" width=400px></center></li>
<li>Rajouter dans le fichier pom les dépendances nécessaires, et indiquer la version du compilateur Java:
  <code>xml
  &lt;properties&gt;
      &lt;maven.compiler.source&gt;1.8&lt;/maven.compiler.source&gt;
      &lt;maven.compiler.target&gt;1.8&lt;/maven.compiler.target&gt;
  &lt;/properties&gt;
  &lt;dependencies&gt;
      &lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;
      &lt;dependency&gt;
          &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
          &lt;artifactId&gt;spark-core_2.12&lt;/artifactId&gt;
          &lt;version&gt;2.4.5&lt;/version&gt;
      &lt;/dependency&gt;
  &lt;/dependencies&gt;</code></li>
<li>Sous le répertoire java, créer un package que vous appellerez <em>tn.spark.batch</em>, et dedans, une classe appelée <em>WordCountTask</em>.</li>
<li>Écrire le code suivant dans <em>WordCountTask</em>:</li>
</ol>
<p>```java
  package tn.spark.batch;</p>
<p>import org.apache.spark.SparkConf;
  import org.apache.spark.api.java.JavaPairRDD;
  import org.apache.spark.api.java.JavaRDD;
  import org.apache.spark.api.java.JavaSparkContext;
  import org.slf4j.Logger;
  import org.slf4j.LoggerFactory;
  import scala.Tuple2;</p>
<p>import java.util.Arrays;</p>
<p>import static jersey.repackaged.com.google.common.base.Preconditions.checkArgument;
  public class WordCountTask {
      private static final Logger LOGGER = LoggerFactory.getLogger(WordCountTask.class);</p>
<pre><code>  public static void main(String[] args) {
      checkArgument(args.length &gt; 1, "Please provide the path of input file and output dir as parameters.");
      new WordCountTask().run(args[0], args[1]);
  }

  public void run(String inputFilePath, String outputDir) {
      String master = "local[*]";
      SparkConf conf = new SparkConf()
              .setAppName(WordCountTask.class.getName())
              .setMaster(master);
      JavaSparkContext sc = new JavaSparkContext(conf);

      JavaRDD&lt;String&gt; textFile = sc.textFile(inputFilePath);
      JavaPairRDD&lt;String, Integer&gt; counts = textFile
              .flatMap(s -&gt; Arrays.asList(s.split(" ")).iterator())
              .mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1))
              .reduceByKey((a, b) -&gt; a + b);
      counts.saveAsTextFile(outputDir);
  }
</code></pre>
<p>}
  ```</p>
<p>La première chose à faire dans un programme Spark est de créer un objet <em>JavaSparkContext</em>, qui indique à Spark comment accéder à un cluster. Pour créer ce contexte, vous aurez besoin de construire un objet <em>SparkConf</em> qui contient toutes les informations sur l'application.</p>
<ul>
<li><em>appName</em> est le nom de l'application</li>
<li><em>master</em> est une URL d'un cluster Spark, Mesos ou YARN, ou bien une chaîne spéciale <em>local</em> pour lancer le job en mode local.</li>
</ul>
<p>!!! warning
      Nous avons indiqué ici que notre master est <em>local</em> pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet déconseillé de la hard-coder dans le programme, il faudrait plutôt l'indiquer comme option de commande à chaque fois que nous lançons le job.</p>
<p>Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.</p>
<h3 id="test-du-code-en-local">Test du code en local</h3>
<p>Pour tester le code sur votre machine, procéder aux étapes suivantes:</p>
<ol>
<li>Créer un fichier texte de votre choix (par exemple le fameux loremipsum.txt, que vous pourrez générer <a href="https://generator.lorem-ipsum.info/_latin">ici</a>) dans le répertoire src/main/resources.</li>
<li>Créer une nouvelle configuration de type "Application" (<em>Run-&gt;Edit Configurations</em>):
<center><img src="../img/p4/proj2.png" width=600px></center>
<center><img src="../img/p4/proj3.png" width=600px></center></li>
<li>La nommer <em>WordCountTask</em>, et définir les arguments suivants (fichier de départ et répertoire d'arrivée) comme <em>Program arguments</em>:
  <code>src/main/resources/loremipsum.txt src/main/resources/out</code>
<center><img src="../img/p4/proj4.png" width=600px></center></li>
<li>Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un répertoire <em>out</em> sera créé sous <em>resources</em>, qui contient deux fichiers: part-00000, part-00001.</li>
</ol>
<p><center><img src="../img/p4/proj5.png" width=600px></center></p>
<h3 id="lancement-du-code-sur-le-cluster">Lancement du code sur le cluster</h3>
<p>Pour exécuter le code sur le cluster, modifier comme indiqué les lignes en jaune dans ce qui suit:</p>
<pre><code class="language-java">public class WordCountTask {
  private static final Logger LOGGER = LoggerFactory.getLogger(WordCountTask.class);

  public static void main(String[] args) {
      checkArgument(args.length &gt; 1, &quot;Please provide the path of input file and output dir as parameters.&quot;);
      new WordCountTask().run(args[0], args[1]);
  }

  public void run(String inputFilePath, String outputDir) {

      SparkConf conf = new SparkConf()
              .setAppName(WordCountTask.class.getName());

      JavaSparkContext sc = new JavaSparkContext(conf);

      JavaRDD&lt;String&gt; textFile = sc.textFile(inputFilePath);
      JavaPairRDD&lt;String, Integer&gt; counts = textFile
              .flatMap(s -&gt; Arrays.asList(s.split(&quot; &quot;)).iterator())
              .mapToPair(word -&gt; new Tuple2&lt;&gt;(word, 1))
              .reduceByKey((a, b) -&gt; a + b);
      counts.saveAsTextFile(outputDir);
  }
}
</code></pre>
<p>Lancer ensuite une configuration de type Maven, avec les commandes <em>package install</em>.
<center><img src="../img/p4/proj6.png" width=600px></center></p>
<p>Un fichier intitulé <em>worcount-1.jar</em> sera créé sous le répertoire <em>target</em>.</p>
<p>Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le répertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:</p>
<pre><code class="language-Bash">  docker cp target/wordcount-1.jar spark-master:/root/wordcount-1.jar
</code></pre>
<p>Copier également le fichier <em>loremipsum.txt</em> que vous avez créé dans votre projet:</p>
<pre><code class="language-Bash">  docker cp src/main/resources/loremipsum.txt spark-master:/root/input/loremipsum.txt
</code></pre>
<p>Aller à votre contenaire spark-master (en utilisant la commande <code>docker exec ...</code>), et lancer un job Spark en utilisant ce fichier jar généré, avec la commande <code>spark-submit</code>, un script utilisé pour lancer des applications spark sur un cluster.</p>
<pre><code class="language-Bash">  cd /root
  spark-submit  --class tn.spark.batch.WordCountTask --master local wordcount-1.jar input/loremipsum.txt output
</code></pre>
<ul>
<li>Nous avons lancé le job en mode local, pour commencer.</li>
<li>Le fichier en entrée est le fichier loremipsum.txt, et le résultat sera stocké dans un répertoire <em>output</em>.</li>
</ul>
<p>Si tout se passe bien, vous devriez trouver, dans le répertoire <em>output</em>, un fichier part-00000, qui ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/proj7.png" width="600"></center></p>
<h2 id="references">Références</h2>
<p>[^sparkforbeginners]:
  Spark for beginners, <em>RDD in Spark</em>, <a href="http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html">http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html</a>, consulté le 03/2020</p>
<p>[^spark-official]:
  Spark Documentation, <em>Resilient Distributed Datasets(RDDs)</em>, <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds">https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a>, consulté le 03/2020</p>
<p>[^devopedia]:
  Devopedia, <em>Apache Spark</em>, <a href="https://devopedia.org/apache-spark">https://devopedia.org/apache-spark</a>, consulté le 03/2020</p></div>
            </div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyright &copy; 2019 - 2020 Lilia Sfaxi</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../js/base.js" defer></script>
        <script src="../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="searchModalLabel" aria-hidden="true">
    <div class="modal-dialog modal-lg">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="searchModalLabel">Search</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
                <p>From here you can search these documents. Enter your search terms below.</p>
                <form>
                    <div class="form-group">
                        <input type="search" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results" data-no-results-text="No results found"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="keyboardModalLabel" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <h4 class="modal-title" id="keyboardModalLabel">Keyboard Shortcuts</h4>
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
