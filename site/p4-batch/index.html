



<!doctype html>
<html lang="fr" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      <meta http-equiv="x-ua-compatible" content="ie=edge">
      
        <meta name="description" content="Cours et Travaux Pratiques pour se familiariser avec Apache Spark">
      
      
        <link rel="canonical" href="http://liliasfaxi.github.io/Atelier-Spark/p4-batch/">
      
      
        <meta name="author" content="Lilia Sfaxi">
      
      
        <meta name="lang:clipboard.copy" content="Copier dans le presse-papier">
      
        <meta name="lang:clipboard.copied" content="Copié dans le presse-papier">
      
        <meta name="lang:search.language" content="fr">
      
        <meta name="lang:search.pipeline.stopwords" content="True">
      
        <meta name="lang:search.pipeline.trimmer" content="True">
      
        <meta name="lang:search.result.none" content="Aucun document trouvé">
      
        <meta name="lang:search.result.one" content="1 document trouvé">
      
        <meta name="lang:search.result.other" content="# documents trouvés">
      
        <meta name="lang:search.tokenizer" content="[\s\-]+">
      
      <link rel="shortcut icon" href="../img/favicon.ico">
      <meta name="generator" content="mkdocs-1.0.4, mkdocs-material-4.6.0">
    
    
      
        <title>P4 - RDD et Batch Processing avec Spark - Atelier Apache Spark</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/application.1b62728e.css">
      
        <link rel="stylesheet" href="../assets/stylesheets/application-palette.a8b3c06d.css">
      
      
        
        
        <meta name="theme-color" content="#546e7a">
      
    
    
      <script src="../assets/javascripts/modernizr.268332fc.js"></script>
    
    
      
        <link href="https://fonts.gstatic.com" rel="preconnect" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto","Helvetica Neue",Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono","Courier New",Courier,monospace}</style>
      
    
    <link rel="stylesheet" href="../assets/fonts/material-icons.css">
    
    
      <link rel="stylesheet" href="../extra.css">
    
    
      
        
<script>
  window.ga = window.ga || function() {
    (ga.q = ga.q || []).push(arguments)
  }
  ga.l = +new Date
  /* Setup integration and send page view */
  ga("create", "None", "auto")
  ga("set", "anonymizeIp", true)
  ga("send", "pageview")
  /* Register handler to log search on blur */
  document.addEventListener("DOMContentLoaded", () => {
    if (document.forms.search) {
      var query = document.forms.search.query
      query.addEventListener("blur", function() {
        if (this.value) {
          var path = document.location.pathname;
          ga("send", "pageview", path + "?q=" + this.value)
        }
      })
    }
  })
</script>
<script async src="https://www.google-analytics.com/analytics.js"></script>
      
    
    
  </head>
  
    
    
    <body dir="ltr" data-md-color-primary="blue-grey" data-md-color-accent="amber">
  
    <svg class="md-svg">
      <defs>
        
        
          <svg xmlns="http://www.w3.org/2000/svg" width="416" height="448" viewBox="0 0 416 448" id="__github"><path fill="currentColor" d="M160 304q0 10-3.125 20.5t-10.75 19T128 352t-18.125-8.5-10.75-19T96 304t3.125-20.5 10.75-19T128 256t18.125 8.5 10.75 19T160 304zm160 0q0 10-3.125 20.5t-10.75 19T288 352t-18.125-8.5-10.75-19T256 304t3.125-20.5 10.75-19T288 256t18.125 8.5 10.75 19T320 304zm40 0q0-30-17.25-51T296 232q-10.25 0-48.75 5.25Q229.5 240 208 240t-39.25-2.75Q130.75 232 120 232q-29.5 0-46.75 21T56 304q0 22 8 38.375t20.25 25.75 30.5 15 35 7.375 37.25 1.75h42q20.5 0 37.25-1.75t35-7.375 30.5-15 20.25-25.75T360 304zm56-44q0 51.75-15.25 82.75-9.5 19.25-26.375 33.25t-35.25 21.5-42.5 11.875-42.875 5.5T212 416q-19.5 0-35.5-.75t-36.875-3.125-38.125-7.5-34.25-12.875T37 371.5t-21.5-28.75Q0 312 0 260q0-59.25 34-99-6.75-20.5-6.75-42.5 0-29 12.75-54.5 27 0 47.5 9.875t47.25 30.875Q171.5 96 212 96q37 0 70 8 26.25-20.5 46.75-30.25T376 64q12.75 25.5 12.75 54.5 0 21.75-6.75 42 34 40 34 99.5z"/></svg>
        
      </defs>
    </svg>
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" data-md-component="overlay" for="__drawer"></label>
    
      <a href="#partie-4-spark-rdd-et-traitement-par-lots" tabindex="1" class="md-skip">
        Skip to content
      </a>
    
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid">
    <div class="md-flex">
      <div class="md-flex__cell md-flex__cell--shrink">
        <a href="http://liliasfaxi.github.io/Atelier-Spark/" title="Atelier Apache Spark" class="md-header-nav__button md-logo">
          
            <img src="../img/logo.png" width="24" height="24">
          
        </a>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        <label class="md-icon md-icon--menu md-header-nav__button" for="__drawer"></label>
      </div>
      <div class="md-flex__cell md-flex__cell--stretch">
        <div class="md-flex__ellipsis md-header-nav__title" data-md-component="title">
          
            <span class="md-header-nav__topic">
              Atelier Apache Spark
            </span>
            <span class="md-header-nav__topic">
              
                P4 - RDD et Batch Processing avec Spark
              
            </span>
          
        </div>
      </div>
      <div class="md-flex__cell md-flex__cell--shrink">
        
          <label class="md-icon md-icon--search md-header-nav__button" for="__search"></label>
          
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" placeholder="Rechercher" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="query" data-md-state="active">
      <label class="md-icon md-search__icon" for="__search"></label>
      <button type="reset" class="md-icon md-search__icon" data-md-component="reset" tabindex="-1">
        &#xE5CD;
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="result">
          <div class="md-search-result__meta">
            Taper pour démarrer la recherche
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
        
      </div>
      
        <div class="md-flex__cell md-flex__cell--shrink">
          <div class="md-header-nav__source">
            


  

<a href="https://github.com/liliasfaxi/Atelier-Spark/" title="Aller au dépôt" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    liliasfaxi/Atelier-Spark
  </div>
</a>
          </div>
        </div>
      
    </div>
  </nav>
</header>
    
    <div class="md-container">
      
        
      
      
      <main class="md-main" role="main">
        <div class="md-main__inner md-grid" data-md-component="container">
          
            
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    <nav class="md-nav md-nav--primary" data-md-level="0">
  <label class="md-nav__title md-nav__title--site" for="__drawer">
    <a href="http://liliasfaxi.github.io/Atelier-Spark/" title="Atelier Apache Spark" class="md-nav__button md-logo">
      
        <img src="../img/logo.png" width="48" height="48">
      
    </a>
    Atelier Apache Spark
  </label>
  
    <div class="md-nav__source">
      


  

<a href="https://github.com/liliasfaxi/Atelier-Spark/" title="Aller au dépôt" class="md-source" data-md-source="github">
  
    <div class="md-source__icon">
      <svg viewBox="0 0 24 24" width="24" height="24">
        <use xlink:href="#__github" width="24" height="24"></use>
      </svg>
    </div>
  
  <div class="md-source__repository">
    liliasfaxi/Atelier-Spark
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      


  <li class="md-nav__item">
    <a href=".." title="Atelier Apache Spark" class="md-nav__link">
      Atelier Apache Spark
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../p1-big-data/" title="P1 - Introduction au Big Data" class="md-nav__link">
      P1 - Introduction au Big Data
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../p2-spark/" title="P2 - Introduction à Apache Spark" class="md-nav__link">
      P2 - Introduction à Apache Spark
    </a>
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../p3-install/" title="P3 - Installation de Spark" class="md-nav__link">
      P3 - Installation de Spark
    </a>
  </li>

    
      
      
      

  


  <li class="md-nav__item md-nav__item--active">
    
    <input class="md-toggle md-nav__toggle" data-md-toggle="toc" type="checkbox" id="__toc">
    
      
    
    
      <label class="md-nav__link md-nav__link--active" for="__toc">
        P4 - RDD et Batch Processing avec Spark
      </label>
    
    <a href="./" title="P4 - RDD et Batch Processing avec Spark" class="md-nav__link md-nav__link--active">
      P4 - RDD et Batch Processing avec Spark
    </a>
    
      
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table des matières</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rdd-resilient-distributed-dataset" class="md-nav__link">
    RDD: Resilient Distributed Dataset
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parallelisation-de-collections" class="md-nav__link">
    Parallélisation de Collections
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generation-a-partir-dun-fichier-externe" class="md-nav__link">
    Génération à partir d'un fichier externe
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations-sur-les-rdds" class="md-nav__link">
    Opérations sur les RDDs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caracteristiques-des-rdds" class="md-nav__link">
    Caractéristiques des RDDs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#immutabilite-et-lignee" class="md-nav__link">
    Immutabilité et Lignée
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lazy-evaluation" class="md-nav__link">
    Lazy Evaluation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exemple" class="md-nav__link">
    Exemple
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-de-spark-avec-spark-shell" class="md-nav__link">
    Test de Spark avec Spark-Shell
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-batch-en-java" class="md-nav__link">
    Spark Batch en Java
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation-de-lenvironnement-et-code" class="md-nav__link">
    Préparation de l'environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local" class="md-nav__link">
    Test du code en local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    Références
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
    
  </li>

    
      
      
      


  <li class="md-nav__item">
    <a href="../p5-sql/" title="P5 - Spark SQL" class="md-nav__link">
      P5 - Spark SQL
    </a>
  </li>

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc">
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">Table des matières</label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#rdd-resilient-distributed-dataset" class="md-nav__link">
    RDD: Resilient Distributed Dataset
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parallelisation-de-collections" class="md-nav__link">
    Parallélisation de Collections
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#generation-a-partir-dun-fichier-externe" class="md-nav__link">
    Génération à partir d'un fichier externe
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#operations-sur-les-rdds" class="md-nav__link">
    Opérations sur les RDDs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#caracteristiques-des-rdds" class="md-nav__link">
    Caractéristiques des RDDs
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#immutabilite-et-lignee" class="md-nav__link">
    Immutabilité et Lignée
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lazy-evaluation" class="md-nav__link">
    Lazy Evaluation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#exemple" class="md-nav__link">
    Exemple
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#test-de-spark-avec-spark-shell" class="md-nav__link">
    Test de Spark avec Spark-Shell
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#spark-batch-en-java" class="md-nav__link">
    Spark Batch en Java
  </a>
  
    <nav class="md-nav">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#preparation-de-lenvironnement-et-code" class="md-nav__link">
    Préparation de l'environnement et Code
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#test-du-code-en-local" class="md-nav__link">
    Test du code en local
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lancement-du-code-sur-le-cluster" class="md-nav__link">
    Lancement du code sur le cluster
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#references" class="md-nav__link">
    Références
  </a>
  
</li>
      
      
      
      
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/liliasfaxi/Atelier-Spark/edit/master/docs/p4-batch.md" title="Editer cette page" class="md-icon md-content__icon">&#xE3C9;</a>
                
                
                <h1 id="partie-4-spark-rdd-et-traitement-par-lots">Partie 4 - Spark RDD et Traitement par Lots</h1>
<p><center><img src="../img/p4/rdd.png" width="500pt"><sup id="fnref:sparkforbeginners"><a class="footnote-ref" href="#fn:sparkforbeginners">1</a></sup></center></p>
<h2 id="rdd-resilient-distributed-dataset">RDD: Resilient Distributed Dataset</h2>
<p>Spark gravite autour du concept de "Resilient Distributed Dataset" ou RDD, qui est une collection d'éléments tolérante aux fautes qui peut être gérée en parallèle. Les RDDs utilisent la mémoire et l'espace disque selon les besoins.</p>
<ul>
<li><em>R pour Résilient</em>: capable de récupérer rapidement en cas de problèmes ou de conditions difficiles,</li>
<li><em>D pour Distribué</em>: partage les données sur les différents noeuds participants pour une exécution parallèle,</li>
<li><em>D pour Dataset</em>: une collection de données composée d'éléments séparés mais qui sont manipulés comme une unité compacte.</li>
</ul>
<p>Il existe deux moyens de créer les RDDs <sup id="fnref:spark-official"><a class="footnote-ref" href="#fn:spark-official">2</a></sup>:</p>
<ul>
<li>Paralléliser une collection existante en mémoire dans le programme Driver.</li>
<li>Le générer à partir d'un fichier enregistré sur un support de stockage externe.</li>
</ul>
<h3 id="parallelisation-de-collections">Parallélisation de Collections</h3>
<p>Les collections parallélisées sont créées en appelant la méthode <code class="codehilite"><span class="n">parallelize</span></code> du <code class="codehilite"><span class="n">JavaSparkContext</span></code> sur une collection existante dans votre programme Driver. Les éléments de la collection sont copiés pour former une structure distribuée qui peut être traitée en parallèle.</p>
<p>Par exemple, nous pouvons créer une collection parallélisée à partir d'une liste contenant les chiffres de 1 à 5:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="kt">List</span><span class="o">&lt;</span><span class="kt">Integer</span><span class="o">&gt;</span> <span class="kd">data</span> <span class="o">=</span> <span class="nx">Arrays.asList</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">);</span>
  <span class="nx">JavaRDD</span><span class="o">&lt;</span><span class="kt">Integer</span><span class="o">&gt;</span> <span class="n">distData</span><span class="o"> =</span> <span class="nx">sc.parallelize</span><span class="p">(</span><span class="nx">data</span><span class="p">);</span>
</pre></div>
</td></tr></table>

<p>Une fois créée, la structure distribuée <code class="codehilite"><span class="n">distData</span></code> peut être traitée en parallèle. Par exemple, il est possible d'appeler <code class="codehilite"><span class="n">distData</span><span class="p">.</span><span class="n">reduce</span><span class="p">((</span><span class="n">a</span><span class="p">,</span><span class="n">b</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span></code> pour faire la somme de tous les éléments de la liste.</p>
<p>Un paramètre important à définir dans une collection parallélisée, c'est le nombre de partitions à utiliser pour diviser la collection. Spark exécute une tâche pour chaque partition du cluster. En temps normal, Spark essaiera de définir le nombre de partitions automatiquement selon votre cluster, cependant, il est possible de le définir manuellement en le passant comme second paramètre de la fonction <code class="codehilite"><span class="n">parallelize</span></code>:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<h3 id="generation-a-partir-dun-fichier-externe">Génération à partir d'un fichier externe</h3>
<p>Spark peut créer une collection distribuée à partir de n'importe quelle source de stockage supportée par Hadoop, incluant votre propre système de stockage, HDFS, Cassandra, HBase, Amazon S3, etc.</p>
<p>Il est possible de créer des RDDs à partir de fichiers texte en utilisant la méthode <code class="codehilite"><span class="n">textfile</span></code> du <code class="codehilite"><span class="n">SparkContext</span></code>. Cette méthode prend l'URI du fichier (chemin du fichier local, ou bien en utilisant <code class="codehilite"><span class="nl">hdfs:</span><span class="c1">//</span></code> ou <code class="codehilite"><span class="nl">s3:</span><span class="c1">//</span></code>), et le lit comme une collection de lignes. Par exemple:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="nx">JavaRDD</span><span class="o">&lt;</span><span class="kt">String</span><span class="o">&gt;</span> <span class="n">distFile</span><span class="o"> =</span> <span class="nx">sc.textFile</span><span class="p">(</span><span class="s2">&quot;data.txt&quot;</span><span class="p">);</span>
</pre></div>
</td></tr></table>

<h2 id="operations-sur-les-rdds">Opérations sur les RDDs</h2>
<p>Les RDDs supportent deux types d'opérations:</p>
<ul>
<li>les <em>transformations</em>, qui permettent de créer une nouvelle collection à partir d'un RDD existant</li>
<li>les <em>actions</em>, qui retournent une valeur au programme <em>driver</em> après avoir exécuté un calcul sur le RDD.</li>
</ul>
<p>Par exemple, un <em>map</em> est une transformation qui passe chaque élément du dataset via une fonction, et retourne un nouvel RDD représentant les résultats. Un <em>reduce</em> est une action qui agrège tous les éléments du RDD en utilisant une certaine fonction et retourne le résultat final au programme.</p>
<p><center><img src="../img/p4/operations.png" width="500pt"><sup id="fnref:devopedia"><a class="footnote-ref" href="#fn:devopedia">3</a></sup></center></p>
<h2 id="caracteristiques-des-rdds">Caractéristiques des RDDs</h2>
<h3 id="immutabilite-et-lignee">Immutabilité et Lignée</h3>
<p>Les RDDs dans Spark sont des collection <em>immuables</em>, c'est à dire qu'elle ne sont jamais modifiées: toute transformation va créer un nouvel RDD au lieu de modifier le RDD initial. Quand un nouvel RDD a été créé à partir d'un RDD existant, ce nouvel RDD contient un pointeur vers le RDD parent. De même, toutes les dépendances entre les RDDs sont loggées dans un graphe, plutôt que directement sur les données. Ce graphe s'appelle <em>Graphe de Lignée</em> ou <em>Lineage Graph</em>.</p>
<p>Par exemple, si on considère les opérations suivantes:</p>
<ol>
<li>Créer un nouvel RDD à partir d'un fichier texte -&gt; RDD1</li>
<li>Appliquer des opérations de Map sur RDD1 -&gt; RDD2</li>
<li>Appliquer une opération de filtrage sur RDD2 -&gt; RDD3</li>
<li>Appliquer une opération de comptage sur RDD3 -&gt; RDD4</li>
</ol>
<p>Le graphe de lignée associé à ces opérations ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/dag.png" width="300pt"></center></p>
<p>Ce graphe peut être utile au cas où certaines partitions sont perdues. Spark peut rejouer la transformation sur cette partition en utilisant le graphe de lignée existant pour réaliser le même calcul, plutôt que de répliquer les données à partir de des différents noeuds du cluster.</p>
<p>Il est également utile en cas de réutilisation d'un graphe existant. Si par exemple on désire appliquer une opération tri sur RDD2, il est inutile de recharger le fichier une deuxième fois à partir du disque. Il suffit de modifier le graphe pour qu'il devienne comme suit:</p>
<p><center><img src="../img/p4/dag2.png" width="300pt"></center></p>
<h3 id="lazy-evaluation">Lazy Evaluation</h3>
<p>Toutes les transformations dans Spark sont <em>lazy</em> (fainéantes), car elles ne calculent pas le résultat immédiatement. Elles se souviennent des transformations appliquées à un dataset de base (par ex. un fichier). Les transformations ne sont calculées que quand une action nécessite qu'un résultat soit retourné au programme principal. Cela permet à Spark de s'exécuter plus efficacement.</p>
<h2 id="exemple">Exemple</h2>
<p>L'exemple que nous allons présenter ici par étapes permet de relever les mots les plus fréquents dans un fichier. Pour cela, le code suivant est utilisé:</p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 1 - Créer un RDD à partir d&#39;un fichier texte</span>
  <span class="n">val</span> <span class="n">docs</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;/docs&quot;</span><span class="p">)</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex1.png" width="500"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 2 - Convertir les lignes en minuscule</span>
  <span class="n">val</span> <span class="n">lower</span> <span class="o">=</span> <span class="n">docs</span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">line</span> <span class="o">=&gt;</span> <span class="n">line</span><span class="p">.</span><span class="n">toLowerCase</span><span class="p">)</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex2.png" width="500"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 3 - Séparer les lignes en mots</span>
  <span class="n">val</span> <span class="n">words</span> <span class="o">=</span> <span class="n">lower</span><span class="p">.</span><span class="n">flatMap</span><span class="p">(</span><span class="n">line</span> <span class="o">=&gt;</span> <span class="n">line</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\\</span><span class="s">s+&quot;</span><span class="p">))</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex3.png" width="500"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 4 - produire les tuples (mot, 1)</span>
  <span class="n">val</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">words</span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">word</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">word</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex4.png" width="500"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 5 - Compter tous les mots</span>
  <span class="n">val</span> <span class="n">freq</span> <span class="o">=</span> <span class="n">counts</span><span class="p">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">_</span> <span class="o">+</span> <span class="n">_</span><span class="p">)</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex5.png" width="500"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 6 - Inverser les tuples (transformation avec swap)</span>
  <span class="n">freq</span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">_</span><span class="p">.</span><span class="n">swap</span><span class="p">)</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex6.png" width="400"></center></p>
<p><table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="c1">//Etape 7 - Inverser les tuples (action de sélection des n premiers)</span>
  <span class="n">val</span> <span class="n">top</span> <span class="o">=</span> <span class="n">freq</span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">_swap</span><span class="p">).</span><span class="n">top</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/ex7.png" width="500"></center></p>
<h2 id="test-de-spark-avec-spark-shell">Test de Spark avec Spark-Shell</h2>
<p>Nous allons tester le comportement de Spark et des RDD en utilisant l'exemple type pour l'analyse des données: le Wordcount, qui permet de compter le nombre de mots dans un fichier donné en entrée.</p>
<p>Commençons par lancer le cluster Spark installé dans la partie <a href="../p3-install/index.html">P3</a>.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  docker start spark-master spark-slave1 spark-slave2
</pre></div>
</td></tr></table>

<p>Entrer dans le noeud Master comme suit:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  docker <span class="nb">exec</span> -it spark-master bash
</pre></div>
</td></tr></table>

<p>Dans le but de tester l'exécution de spark, commencer par créer un fichier <em>input/file1.txt</em> dans le répertoire <code class="codehilite"><span class="o">/</span><span class="n">root</span></code>:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  mkdir /root/input
  vim /root/input/file1.txt
</pre></div>
</td></tr></table>

<p>Remplir le fichier avec le texte suivant, ou tout texte de votre choix (vous devez taper <code class="codehilite"><span class="n">i</span></code> pour passer en mode édition):</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  Hello Spark Wordcount!
  Hello everybody else!
</pre></div>
</td></tr></table>

<p>Lancer Spark Shell en utilisant la commande suivante:<br />
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">spark</span><span class="o">-</span><span class="n">shell</span>
</pre></div>
</td></tr></table></p>
<p>Vous devriez avoir un résultat semblable au suivant:<br />
<img alt="Spark Shell" src="../img/p3/spark-shell.png" /></p>
<p>Vous pourrez tester spark avec un code scala simple comme suit (à exécuter ligne par ligne):</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3
4</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">val</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;/root/input/file1.txt&quot;</span><span class="p">)</span>
  <span class="n">val</span> <span class="n">words</span> <span class="o">=</span> <span class="n">lines</span><span class="p">.</span><span class="n">flatMap</span><span class="p">(</span><span class="n">_</span><span class="p">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot;</span><span class="se">\\</span><span class="s">s+&quot;</span><span class="p">))</span>
  <span class="n">val</span> <span class="n">wc</span> <span class="o">=</span> <span class="n">words</span><span class="p">.</span><span class="n">map</span><span class="p">(</span><span class="n">w</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="p">)).</span><span class="n">reduceByKey</span><span class="p">(</span><span class="n">_</span> <span class="o">+</span> <span class="n">_</span><span class="p">)</span>
  <span class="n">wc</span><span class="p">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="s">&quot;/root/file1.count&quot;</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>Ce code vient de (1) charger le fichier <em>file1.txt</em> du système de fichier courant, (2) séparer les mots selon les caractères d'espacement, (3) appliquer un <em>map</em> sur les mots obtenus qui produit le couple (<em>&lt;mot></em>, 1), puis un <em>reduce</em> qui permet de faire la somme des 1 des mots identiques.</p>
<p>Pour afficher le résultat, sortir de spark-shell en cliquant sur <em>Ctrl-C</em>. Afficher ensuite le contenu du fichier <em>part-00000</em> du répertoire <em>file1.count</em> créé, comme suit:<br />
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">cat</span> <span class="o">/</span><span class="n">root</span><span class="o">/</span><span class="n">file1</span><span class="p">.</span><span class="n">count</span><span class="o">/</span><span class="n">part</span><span class="o">-</span><span class="mo">00000</span>
</pre></div>
</td></tr></table><br />
Le contenu des deux fichiers <em>part-00000</em> et <em>part-00001</em> ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/result.png" width=400px></center></p>
<h2 id="spark-batch-en-java">Spark Batch en Java</h2>
<h3 id="preparation-de-lenvironnement-et-code">Préparation de l'environnement et Code</h3>
<p>Nous allons dans cette partie créer un projet Spark Batch en Java (un simple WordCount), le charger sur le cluster et lancer le job.</p>
<ol>
<li>Sur votre propre machine, créer un projet Maven avec IntelliJ IDEA (ou tout IDE de votre choix), en utilisant la config suivante:<br />
  <table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2
3</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="nt">&lt;groupId&gt;</span>spark.batch<span class="nt">&lt;/groupId&gt;</span>
  <span class="nt">&lt;artifactId&gt;</span>wordcount<span class="nt">&lt;/artifactId&gt;</span>
  <span class="nt">&lt;version&gt;</span>1<span class="nt">&lt;/version&gt;</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/proj1.png" width=400px></center></li>
<li>Rajouter dans le fichier pom les dépendances nécessaires, et indiquer la version du compilateur Java:<br />
  <table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="nt">&lt;properties&gt;</span>
    <span class="nt">&lt;maven.compiler.source&gt;</span>1.8<span class="nt">&lt;/maven.compiler.source&gt;</span>
    <span class="nt">&lt;maven.compiler.target&gt;</span>1.8<span class="nt">&lt;/maven.compiler.target&gt;</span>
<span class="nt">&lt;/properties&gt;</span>
<span class="nt">&lt;dependencies&gt;</span>
    <span class="c">&lt;!-- https://mvnrepository.com/artifact/org.apache.spark/spark-core --&gt;</span>
    <span class="nt">&lt;dependency&gt;</span>
        <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
        <span class="nt">&lt;artifactId&gt;</span>spark-core_2.12<span class="nt">&lt;/artifactId&gt;</span>
        <span class="nt">&lt;version&gt;</span>2.4.5<span class="nt">&lt;/version&gt;</span>
    <span class="nt">&lt;/dependency&gt;</span>
<span class="nt">&lt;/dependencies&gt;</span>
</pre></div>
</td></tr></table></li>
<li>Sous le répertoire java, créer un package que vous appellerez <em>tn.spark.batch</em>, et dedans, une classe appelée <em>WordCountTask</em>.</li>
<li>Écrire le code suivant dans <em>WordCountTask</em>:</li>
</ol>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kn">package</span> <span class="n">tn</span><span class="o">.</span><span class="na">spark</span><span class="o">.</span><span class="na">batch</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.spark.SparkConf</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaPairRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaRDD</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.spark.api.java.JavaSparkContext</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.slf4j.Logger</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.slf4j.LoggerFactory</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">scala.Tuple2</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">java.util.Arrays</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">static</span> <span class="n">jersey</span><span class="o">.</span><span class="na">repackaged</span><span class="o">.</span><span class="na">com</span><span class="o">.</span><span class="na">google</span><span class="o">.</span><span class="na">common</span><span class="o">.</span><span class="na">base</span><span class="o">.</span><span class="na">Preconditions</span><span class="o">.</span><span class="na">checkArgument</span><span class="o">;</span>
<span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountTask</span> <span class="o">{</span>
    <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">Logger</span> <span class="n">LOGGER</span> <span class="o">=</span> <span class="n">LoggerFactory</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

    <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">checkArgument</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">,</span> <span class="s">&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class="o">);</span>
        <span class="k">new</span> <span class="n">WordCountTask</span><span class="o">().</span><span class="na">run</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
    <span class="o">}</span>

    <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="n">String</span> <span class="n">inputFilePath</span><span class="o">,</span> <span class="n">String</span> <span class="n">outputDir</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">String</span> <span class="n">master</span> <span class="o">=</span> <span class="s">&quot;local[*]&quot;</span><span class="o">;</span>
        <span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">()</span>
                <span class="o">.</span><span class="na">setAppName</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">())</span>
                <span class="o">.</span><span class="na">setMaster</span><span class="o">(</span><span class="n">master</span><span class="o">);</span>
        <span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

        <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">textFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="n">inputFilePath</span><span class="o">);</span>
        <span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">textFile</span>
                <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">())</span>
                <span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
                <span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
        <span class="n">counts</span><span class="o">.</span><span class="na">saveAsTextFile</span><span class="o">(</span><span class="n">outputDir</span><span class="o">);</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>

<p>La première chose à faire dans un programme Spark est de créer un objet <em>JavaSparkContext</em>, qui indique à Spark comment accéder à un cluster. Pour créer ce contexte, vous aurez besoin de construire un objet <em>SparkConf</em> qui contient toutes les informations sur l'application.</p>
<ul>
<li><em>appName</em> est le nom de l'application</li>
<li><em>master</em> est une URL d'un cluster Spark, Mesos ou YARN, ou bien une chaîne spéciale <em>local</em> pour lancer le job en mode local.</li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Nous avons indiqué ici que notre master est <em>local</em> pour les besoins du test, mais plus tard, en le packageant pour le cluster, nous allons enlever cette indication. Il est en effet déconseillé de la hard-coder dans le programme, il faudrait plutôt l'indiquer comme option de commande à chaque fois que nous lançons le job.</p>
</div>
<p>Le reste du code de l'application est la version en Java de l'exemple en scala que nous avions fait avec spark-shell.</p>
<h3 id="test-du-code-en-local">Test du code en local</h3>
<p>Pour tester le code sur votre machine, procéder aux étapes suivantes:</p>
<ol>
<li>Créer un fichier texte de votre choix (par exemple le fameux loremipsum.txt, que vous pourrez générer <a href="https://generator.lorem-ipsum.info/_latin">ici</a>) dans le répertoire src/main/resources.</li>
<li>Créer une nouvelle configuration de type "Application" (<em>Run-&gt;Edit Configurations</em>):<br />
<center><img src="../img/p4/proj2.png" width=600px></center><br />
<center><img src="../img/p4/proj3.png" width=600px></center></li>
<li>La nommer <em>WordCountTask</em>, et définir les arguments suivants (fichier de départ et répertoire d'arrivée) comme <em>Program arguments</em>:<br />
  <table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">src</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">resources</span><span class="o">/</span><span class="n">loremipsum</span><span class="p">.</span><span class="n">txt</span> <span class="n">src</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">resources</span><span class="o">/</span><span class="n">out</span>
</pre></div>
</td></tr></table><br />
<center><img src="../img/p4/proj4.png" width=600px></center></li>
<li>Cliquer sur OK, et lancer la configuration. Si tout se passe bien, un répertoire <em>out</em> sera créé sous <em>resources</em>, qui contient deux fichiers: part-00000, part-00001.</li>
</ol>
<p><center><img src="../img/p4/proj5.png" width=600px></center></p>
<h3 id="lancement-du-code-sur-le-cluster">Lancement du code sur le cluster</h3>
<p>Pour exécuter le code sur le cluster, modifier comme indiqué les lignes en jaune dans ce qui suit:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23</pre></div></td><td class="code"><div class="codehilite"><pre><span></span><span class="kd">public</span> <span class="kd">class</span> <span class="nc">WordCountTask</span> <span class="o">{</span>
  <span class="kd">private</span> <span class="kd">static</span> <span class="kd">final</span> <span class="n">Logger</span> <span class="n">LOGGER</span> <span class="o">=</span> <span class="n">LoggerFactory</span><span class="o">.</span><span class="na">getLogger</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

  <span class="kd">public</span> <span class="kd">static</span> <span class="kt">void</span> <span class="nf">main</span><span class="o">(</span><span class="n">String</span><span class="o">[]</span> <span class="n">args</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">checkArgument</span><span class="o">(</span><span class="n">args</span><span class="o">.</span><span class="na">length</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="o">,</span> <span class="s">&quot;Please provide the path of input file and output dir as parameters.&quot;</span><span class="o">);</span>
      <span class="k">new</span> <span class="n">WordCountTask</span><span class="o">().</span><span class="na">run</span><span class="o">(</span><span class="n">args</span><span class="o">[</span><span class="mi">0</span><span class="o">],</span> <span class="n">args</span><span class="o">[</span><span class="mi">1</span><span class="o">]);</span>
  <span class="o">}</span>

  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">run</span><span class="o">(</span><span class="n">String</span> <span class="n">inputFilePath</span><span class="o">,</span> <span class="n">String</span> <span class="n">outputDir</span><span class="o">)</span> <span class="o">{</span>

<span class="hll">      <span class="n">SparkConf</span> <span class="n">conf</span> <span class="o">=</span> <span class="k">new</span> <span class="n">SparkConf</span><span class="o">()</span>
</span><span class="hll">              <span class="o">.</span><span class="na">setAppName</span><span class="o">(</span><span class="n">WordCountTask</span><span class="o">.</span><span class="na">class</span><span class="o">.</span><span class="na">getName</span><span class="o">());</span>
</span>
      <span class="n">JavaSparkContext</span> <span class="n">sc</span> <span class="o">=</span> <span class="k">new</span> <span class="n">JavaSparkContext</span><span class="o">(</span><span class="n">conf</span><span class="o">);</span>

      <span class="n">JavaRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">textFile</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="na">textFile</span><span class="o">(</span><span class="n">inputFilePath</span><span class="o">);</span>
      <span class="n">JavaPairRDD</span><span class="o">&lt;</span><span class="n">String</span><span class="o">,</span> <span class="n">Integer</span><span class="o">&gt;</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">textFile</span>
              <span class="o">.</span><span class="na">flatMap</span><span class="o">(</span><span class="n">s</span> <span class="o">-&gt;</span> <span class="n">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span><span class="n">s</span><span class="o">.</span><span class="na">split</span><span class="o">(</span><span class="s">&quot; &quot;</span><span class="o">)).</span><span class="na">iterator</span><span class="o">())</span>
              <span class="o">.</span><span class="na">mapToPair</span><span class="o">(</span><span class="n">word</span> <span class="o">-&gt;</span> <span class="k">new</span> <span class="n">Tuple2</span><span class="o">&lt;&gt;(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1</span><span class="o">))</span>
              <span class="o">.</span><span class="na">reduceByKey</span><span class="o">((</span><span class="n">a</span><span class="o">,</span> <span class="n">b</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">a</span> <span class="o">+</span> <span class="n">b</span><span class="o">);</span>
      <span class="n">counts</span><span class="o">.</span><span class="na">saveAsTextFile</span><span class="o">(</span><span class="n">outputDir</span><span class="o">);</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>
</td></tr></table>

<p>Lancer ensuite une configuration de type Maven, avec les commandes <em>package install</em>.<br />
<center><img src="../img/p4/proj6.png" width=600px></center></p>
<p>Un fichier intitulé <em>worcount-1.jar</em> sera créé sous le répertoire <em>target</em>.</p>
<p>Nous allons maintenant copier ce fichier dans docker. Pour cela, naviguer vers le répertoire du projet avec votre terminal (ou plus simplement utiliser le terminal dans IntelliJ), et taper la commande suivante:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">docker</span> <span class="n">cp</span> <span class="n">target</span><span class="o">/</span><span class="n">wordcount</span><span class="o">-</span><span class="mf">1.</span><span class="n">jar</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span><span class="o">:/</span><span class="n">root</span><span class="o">/</span><span class="n">wordcount</span><span class="o">-</span><span class="mf">1.</span><span class="n">jar</span>
</pre></div>
</td></tr></table>

<p>Copier également le fichier <em>loremipsum.txt</em> que vous avez créé dans votre projet:</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">docker</span> <span class="n">cp</span> <span class="n">src</span><span class="o">/</span><span class="n">main</span><span class="o">/</span><span class="n">resources</span><span class="o">/</span><span class="n">loremipsum</span><span class="p">.</span><span class="n">txt</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span><span class="o">:/</span><span class="n">root</span><span class="o">/</span><span class="n">input</span><span class="o">/</span><span class="n">loremipsum</span><span class="p">.</span><span class="n">txt</span>
</pre></div>
</td></tr></table>

<p>Aller à votre contenaire spark-master (en utilisant la commande <code class="codehilite"><span class="n">docker</span> <span class="n">exec</span> <span class="p">...</span></code>), et lancer un job Spark en utilisant ce fichier jar généré, avec la commande <code class="codehilite"><span class="n">spark</span><span class="o">-</span><span class="n">submit</span></code>, un script utilisé pour lancer des applications spark sur un cluster.</p>
<table class="codehilitetable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span>1
2</pre></div></td><td class="code"><div class="codehilite"><pre><span></span>  <span class="n">cd</span> <span class="o">/</span><span class="n">root</span>
  <span class="n">spark</span><span class="o">-</span><span class="n">submit</span>  <span class="o">--</span><span class="n">class</span> <span class="n">tn</span><span class="p">.</span><span class="n">spark</span><span class="p">.</span><span class="n">batch</span><span class="p">.</span><span class="n">WordCountTask</span> <span class="o">--</span><span class="n">master</span> <span class="n">local</span> <span class="n">wordcount</span><span class="o">-</span><span class="mf">1.</span><span class="n">jar</span> <span class="n">input</span><span class="o">/</span><span class="n">loremipsum</span><span class="p">.</span><span class="n">txt</span> <span class="n">output</span>
</pre></div>
</td></tr></table>

<ul>
<li>Nous avons lancé le job en mode local, pour commencer.</li>
<li>Le fichier en entrée est le fichier loremipsum.txt, et le résultat sera stocké dans un répertoire <em>output</em>.</li>
</ul>
<p>Si tout se passe bien, vous devriez trouver, dans le répertoire <em>output</em>, un fichier part-00000, qui ressemble à ce qui suit:</p>
<p><center><img src="../img/p4/proj7.png" width="600"></center></p>
<h2 id="references">Références</h2>
<div class="footnote">
<hr />
<ol>
<li id="fn:sparkforbeginners">
<p>Spark for beginners, <em>RDD in Spark</em>, <a href="http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html">http://sparkforbeginners.blogspot.com/2016/05/rdd-in-spark.html</a>, consulté le 03/2020&#160;<a class="footnote-backref" href="#fnref:sparkforbeginners" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:spark-official">
<p>Spark Documentation, <em>Resilient Distributed Datasets(RDDs)</em>, <a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds">https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds</a>, consulté le 03/2020&#160;<a class="footnote-backref" href="#fnref:spark-official" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:devopedia">
<p>Devopedia, <em>Apache Spark</em>, <a href="https://devopedia.org/apache-spark">https://devopedia.org/apache-spark</a>, consulté le 03/2020&#160;<a class="footnote-backref" href="#fnref:devopedia" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
                  
                
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../p3-install/" title="P3 - Installation de Spark" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Précédent
                </span>
                P3 - Installation de Spark
              </span>
            </div>
          </a>
        
        
          <a href="../p5-sql/" title="P5 - Spark SQL" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Suivant
                </span>
                P5 - Spark SQL
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2019 - 2020 Lilia Sfaxi
          </div>
        
        powered by
        <a href="https://www.mkdocs.org">MkDocs</a>
        and
        <a href="https://squidfunk.github.io/mkdocs-material/">
          Material for MkDocs</a>
      </div>
      
  <div class="md-footer-social">
    <link rel="stylesheet" href="../assets/fonts/font-awesome.css">
    
      <a href="http://liliasfaxi.wix.com/liliasfaxi" class="md-footer-social__link fa fa-globe"></a>
    
      <a href="https://github.com/liliasfaxi" class="md-footer-social__link fa fa-github-alt"></a>
    
      <a href="https://twitter.com/lillitou" class="md-footer-social__link fa fa-twitter"></a>
    
      <a href="https://www.linkedin.com/in/liliasfaxi/" class="md-footer-social__link fa fa-linkedin"></a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../assets/javascripts/application.808e90bb.js"></script>
      
        
        
          
          <script src="../assets/javascripts/lunr/lunr.stemmer.support.js"></script>
          
            
              
              
                <script src="../assets/javascripts/lunr/lunr.fr.js"></script>
              
            
          
          
        
      
      <script>app.initialize({version:"1.0.4",url:{base:".."}})</script>
      
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
      
    
  </body>
</html>