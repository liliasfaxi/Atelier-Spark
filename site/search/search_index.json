{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Atelier Apache Spark Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International . Objectif L'objectif de ce cours est d'initier les apprenants aux concepts de base de Apache Spark, et de faire le tours des composants qui le constituent et des cas d'utilisation. Ce cours est renforc\u00e9 par des travaux pratiques. Table des Mati\u00e8res Le cours est divis\u00e9 comme suit: Introduction aux Big Data Pr\u00e9sentation de Apache Spark, installation et manipulation avec les commandes Shell Spark Batch Spark SQL Spark Streaming Spark ML-Lib Spark GraphX","title":"Atelier Apache Spark"},{"location":"#atelier-apache-spark","text":"Ce(tte) \u0153uvre est mise \u00e0 disposition selon les termes de la Licence Creative Commons Attribution - Pas d\u2019Utilisation Commerciale - Partage dans les M\u00eames Conditions 4.0 International .","title":"Atelier Apache Spark"},{"location":"#objectif","text":"L'objectif de ce cours est d'initier les apprenants aux concepts de base de Apache Spark, et de faire le tours des composants qui le constituent et des cas d'utilisation. Ce cours est renforc\u00e9 par des travaux pratiques.","title":"Objectif"},{"location":"#table-des-matieres","text":"Le cours est divis\u00e9 comme suit: Introduction aux Big Data Pr\u00e9sentation de Apache Spark, installation et manipulation avec les commandes Shell Spark Batch Spark SQL Spark Streaming Spark ML-Lib Spark GraphX","title":"Table des Mati\u00e8res"},{"location":"p1-big-data/","text":"Partie 1 - Introduction au Big Data Les \"Big Data\", Pourquoi? L'\u00eatre humain, \u00e0 travers l'humanit\u00e9, a toujours cherch\u00e9 trois choses : Savoir (qu'est-ce qui s'est pass\u00e9?), Comprendre (pourquoi cela s'est-il pass\u00e9?) et Pr\u00e9dire (qu'est-ce que qui se passera?). Plusieurs cultures ont clam\u00e9 l'omniscience en ayant recours \u00e0 des subterfuges, tels que les oracles, l'astrologie, le tarot, ou les boules de cristal. Cela dit, ces moyens ne sont gu\u00e8res satisfaisants \u00e0 l'esprit m\u00e9ticuleux du scientifique, qui cherche toujours une explication logique et rationnelle \u00e0 tout \u00e9v\u00e8nement, et une justification convainquante \u00e0 tout comportement. Le scientifique se base sur des faits. Il veut arriver \u00e0 faire de la magie gr\u00e2ce \u00e0 la technologie. Pour arriver \u00e0 ces fins, le scientifique a besoin de donn\u00e9es. L'int\u00e9r\u00eat de collecter des donn\u00e9es et de les exploiter a longtemps \u00e9t\u00e9 n\u00e9glig\u00e9, et a \u00e9t\u00e9 limit\u00e9 au peu de donn\u00e9es, jug\u00e9es \"utiles\", qui semblaient suffisantes pour atteindre un objectif imm\u00e9diat. Cependant, adopter le chemin \u00e9vident et peu risqu\u00e9 n'aurait jamais permis de r\u00e9aliser les miracles auxquelles on s'attendait. Il fallait trouver un autre moyen.. Le terme Big Data est apparu peu de temps apr\u00e8s l'apparition du terme Web 2.0, qui montre la transition de l'internet d'une \u00e8re o\u00f9 l'ajout des donn\u00e9es \u00e9tait exclusivement r\u00e9serv\u00e9 \u00e0 une \u00e9lite experte, o\u00f9 le volume des donn\u00e9es disponible \u00e9tait petit mais o\u00f9 les donn\u00e9es \u00e9taient pr\u00e9cieuses et pertinentes, vers une \u00e8re o\u00f9 tout un chacun \u00e9tait capable d'introduire des connaissances, v\u00e9ridiques ou pas, qui seraient sauvegard\u00e9es dans une m\u00e9moire collective jusqu'\u00e0 la fin des temps. Ce changement de paradigme a entrain\u00e9 le besoin d'infrastructures nouvelles, qui seraient capables, non seulement de stocker ces donn\u00e9es, mais \u00e9galement d'en extraire de la valeur. Ces infrastructures auront la capacit\u00e9 de g\u00e9rer toute la cha\u00eene logistique des donn\u00e9es, de la collecte vers l'affichage. Cela semble \u00e9vident, me direz-vous, car les syst\u00e8mes classiques sont capables de faire cela. Qui stocke mieux les donn\u00e9es qu'une bonne vieille base de donn\u00e9es relationnelle? Le probl\u00e8me est que les donn\u00e9es dites \"Big Data\" sont caract\u00e9ris\u00e9es par des propri\u00e9t\u00e9s telles que, les syst\u00e8mes classiques de stockage et de traitement auraient du mal \u00e0 les exploiter \u00e0 leur juste valeur. Caract\u00e9ristiques des Donn\u00e9es Massives Le terme \"donn\u00e9es massives\", ou \"Big Data\", ne donne \u00e0 mon avis pas enti\u00e8rement justice aux donn\u00e9es de notre domaine. En effet, il repr\u00e9sente une seule caract\u00e9ristique parmis plusieurs, le Volume, qui, m\u00eame si elle semble \u00eatre la plus importante, est loin d'\u00eatre la plus critique. En effet, les donn\u00e9es massives sont caract\u00e9ris\u00e9es par les fameux *V . Il en existe plusieurs (10 jusqu'\u00e0 ce jour si je ne m'abuse, certains en citent m\u00eame 42!!!), mais pourraient \u00e0 mon avis \u00eatre r\u00e9sum\u00e9s en trois caract\u00e9ristiques primordiales, autours de la combinaison desquelles tournent toutes les d\u00e9cisions prises dans ce domaine. Volume C'est \u00e9videmment le V le plus manifeste, qui caract\u00e9rise le fait que les donn\u00e9es ont un volume \u00e9norme qui peut atteindre des valeurs de l'ordre de Exa-, Zetta- ou Yottaoctet (allant jusqu'\u00e0 2^{80} 2^{80} octets!). Mais ceci n'est pas tout. Un volume \u00e9norme, s'il reste constant, est g\u00e9rable : il suffit de trouver une machine suffisamment puissante pour le sauvegarder. Le probl\u00e8me avec la propri\u00e9t\u00e9 du volume, c'est qu'il augmente de fa\u00e7on continue, ce qui rend sa gestion beaucoup plus ardue. Une citation bien connue, et qui se re-confirme chaque ann\u00e9e, atteste que \"Over the last two years alone 90 percent of the data in the world was generated.\" Il est donc primordial de trouver un moyen de g\u00e9rer ce volume toujours croissant des donn\u00e9es. V\u00e9locit\u00e9 Cette propri\u00e9t\u00e9 est, \u00e0 mon avis, la plus probl\u00e9matique des trois, car, coupl\u00e9e avec le volume, elle rend les syst\u00e8me actuels obsol\u00e8tes. En effet, la v\u00e9locit\u00e9 est, litt\u00e9ralement, \"La vitesse avec laquelle quelque chose se d\u00e9place dans une direction particuli\u00e8re\". Dans notre cas, la v\u00e9locit\u00e9 des donn\u00e9es est la responsable directe du volume croissant des donn\u00e9es dans le syst\u00e8me. Elle est provoqu\u00e9e par une arriv\u00e9e des donn\u00e9es dans le syst\u00e8me sous la forme d'un flux constant qui demande \u00e0 \u00eatre stock\u00e9 et trait\u00e9 imm\u00e9diatement, ainsi que le besoin croissant des utilisateurs d'avoir une repr\u00e9sentation r\u00e9cente et fid\u00e8le de l'\u00e9tat des donn\u00e9es. D'ailleurs, cette propri\u00e9t\u00e9 a engendr\u00e9 une autre pr\u00e9occupation des analystes des donn\u00e9es, qui est de fournir une introspection en temps r\u00e9el sur les donn\u00e9es, les qualifiant ainsi de \" Fast Data \". Vari\u00e9t\u00e9 Ce qui distingue vraiment les donn\u00e9es massives des donn\u00e9es g\u00e9r\u00e9es classiquement dans des bases de donn\u00e9es op\u00e9rationnelles, c'est le support des donn\u00e9es semi- et non structur\u00e9es. En effet, les donn\u00e9es non structur\u00e9es sont des donn\u00e9es qu'on stocke dans un format qui n'est pas d\u00e9fini \u00e0 la cr\u00e9ation, telles que les donn\u00e9es textuelles, images ou sons. Les donn\u00e9es semi-structur\u00e9es sont des donn\u00e9es qui contiennent une structure, mais une structure qui n'est pas rigide, et dont on ne d\u00e9finit pas les contraintes \u00e0 l'insertion de la donn\u00e9e, contrairement aux donn\u00e9es structur\u00e9es (se trouvant typiquement dans des bases de donn\u00e9es relationnelles) qui, si elles ne respectent pas la structure d\u00e9finie, sont consid\u00e9r\u00e9es fausses et ne sont pas autoris\u00e9es \u00e0 \u00eatre enregistr\u00e9es. On estime que seules 15% des donn\u00e9es dans une entreprise sont des donn\u00e9es structur\u00e9es, contre 85% qui ne le sont pas! Dans une optique centr\u00e9e sur les donn\u00e9es, dont le but est de gagner le maximum de vision \u00e0 partir des donn\u00e9es, perdre autant de sources d'information est un vrai probl\u00e8me. Il est donc important que les syst\u00e8mes Big Data sachent interpr\u00e9ter ces donn\u00e9es et en extraire le maximum de valeur. Toutes les d\u00e9cisions, choix et propri\u00e9t\u00e9s prises au niveau des architectures et infrastructures Big Data sont r\u00e9gies par ces trois caract\u00e9ristiques, ce qui va compl\u00e8tement changer la vision \"relationnelle\" que tout informaticien qui se respecte a acquis tout au long de ses ann\u00e9es d'\u00e9tude et de travail. Cela dit, ce ne sont pas les seules propri\u00e9t\u00e9s. D'autres V ont vu le jour, mais sans jamais avoir autant d'impact sur l'infrastructre, plut\u00f4t dans la fa\u00e7on de d\u00e9finir les processus, la gouvernance et les approches m\u00e9tier \u00e0 adopter. Nous citons par exemple : V\u00e9racit\u00e9 : c'est la confiance que nous devons avoir en nos donn\u00e9es. Cette propri\u00e9t\u00e9 est inversement proportionnelle au volume et \u00e0 la vari\u00e9t\u00e9 : plus nos donn\u00e9es sont fiables, moins elles sont diversifi\u00e9es et volumineuses! Valeur : c'est la capacit\u00e9 d'extraire de la valeur m\u00e9tier \u00e0 partir des donn\u00e9es. Variabilit\u00e9 : une extension de la vari\u00e9t\u00e9, qui indique \u00e0 quel point nos donn\u00e9es peuvent avoir des dimensions diff\u00e9rentes \u00e0 partir des sources de donn\u00e9es disparates. Visualisation : c'est la capacit\u00e9 de nos donn\u00e9es \u00e0 \u00eatre repr\u00e9sent\u00e9es par les outils de visualisation classiques. etc. Infrastructure Big Data : Besoins Les caract\u00e9ristiques des donn\u00e9es Big Data cit\u00e9es ci-dessus, entra\u00eenent des besoins particuliers en termes d'infrastructure et d'architecture. Volume La caract\u00e9ristique de volume, qui implique que la taille des donn\u00e9es augmente de fa\u00e7on r\u00e9guli\u00e8re, fait qu'on ne peut plus se contenter d'un syst\u00e8me centralis\u00e9 classique. Car dans un syst\u00e8me centralis\u00e9 (donc bas\u00e9 sur une seule machine), augmenter les ressources de stockage au besoin implique ce que nous appelons une scalabilit\u00e9 verticale ou un scale up , qui veut dire une augmentation des capacit\u00e9s du serveur de stockage en rajoutant des processeurs, de la RAM ou des disques. Cependant, cette solution, bien qu'elle soit intuitive, rapide et ne requiert pas de changement architecturaux cons\u00e9quents, implique en g\u00e9n\u00e9ral un temps d'arr\u00eat pendant l'installation, ainsi qu'une d\u00e9pense assez cons\u00e9quente pour faire l'acquisition d'un serveur puissant. De plus, une machine unique atteindra rapidement une limite mat\u00e9rielle, car il vous est impossible d'augmenter ses ressources ind\u00e9finiment. En contrepartie, il est possible de penser que, face \u00e0 un volume de donn\u00e9es toujours en augmentation, il serait plus judicieux de rajouter des machines au besoin, cr\u00e9ant ainsi un cluster de machines interconnect\u00e9es, ou syst\u00e8me r\u00e9parti , dont la taille et la capacit\u00e9 sont virtuellement illimit\u00e9es. Nous sommes donc face \u00e0 un autre type de scalabilit\u00e9 : la scalabilit\u00e9 horizontale ou le scale out . Donc Volume => Scalabilit\u00e9 Horizontale V\u00e9locit\u00e9 La v\u00e9locit\u00e9 est une propri\u00e9t\u00e9 qui, coupl\u00e9e au volume, rend la gestion de l'infrastructure un vrai cauchemar. En effet, g\u00e9rer des donn\u00e9es en continuelle arriv\u00e9e implique qu'il y'a un risque \u00e9norme de perte de donn\u00e9es, si elles ne sont pas manipul\u00e9es \u00e0 temps. C'est pour cette raison qu'un syst\u00e8me Big Data se doit d'\u00eatre continuellement disponible : toute requ\u00eate de lecture ou d'\u00e9criture doit \u00eatre trait\u00e9e en un temps raisonnable, et le syst\u00e8me doit \u00eatre continuellement alerte pour saisir toutes les donn\u00e9es, sans risquer de les perdre. Ainsi V\u00e9locit\u00e9 => Disponibilit\u00e9 Vari\u00e9t\u00e9 La vari\u00e9t\u00e9 de donn\u00e9es implique non seulement que nous sommes en pr\u00e9sence de donn\u00e9es structur\u00e9es, semi-structur\u00e9es et non structur\u00e9es, mais \u00e9galement que ces donn\u00e9es peuvent parvenir de sources diff\u00e9rentes, avec des formats diff\u00e9rents, et que m\u00eame \u00e0 partir d'une m\u00eame source, ce format peut changer d'un moment \u00e0 un autre. Dans les syst\u00e8mes classiques, tout ce qui est variable doit passer par une couche d'homog\u00e9n\u00e9isation qui transformera chaque entr\u00e9e ou enregistrement dans la forme souhait\u00e9e, en remplissant par des valeurs NULL les donn\u00e9es manquantes. Rajouter cette couche d'homog\u00e9n\u00e9isation aura un double impact n\u00e9gatif sur notre syst\u00e8me : (1) \u00e0 cause de la v\u00e9locit\u00e9, cette op\u00e9ration risquera de ralentir la collecte et saisie des donn\u00e9es entrantes, et (2) on pourra subir une perte de donn\u00e9es suite \u00e0 ces transformations. C'est pour ces raisons qu'un syst\u00e8me Big Data se doit de supporter des types de donn\u00e9es changeants, sans pour autant requ\u00e9rir \u00e0 des subterfuges qui alourdissent ou contournent le syst\u00e8me de stockage. D'o\u00f9 Vari\u00e9t\u00e9 => Flexibilit\u00e9 Th\u00e9or\u00e8me CAP Les besoins de scalabilit\u00e9, disponibilit\u00e9 et flexibilit\u00e9, obligatoires pour avoir un syst\u00e8me Big Data en bonne et due forme, se trouvent confront\u00e9s \u00e0 une contrainte de taille... et qu'en est-il de la coh\u00e9rence (commun\u00e9ment appel\u00e9e aussi consistence, par anglicisme)? La coh\u00e9rence repr\u00e9sente en effet un must pour les syst\u00e8mes relationnels classiques, et une base sur laquelle sont prises toutes les d\u00e9cisions conceptuelles et techniques. Elle repr\u00e9sente le fait que les donn\u00e9es stables doivent respecter toutes les contraintes d'int\u00e9grit\u00e9 d\u00e9finies \u00e0 la cr\u00e9ation de la base de donn\u00e9e. Par exemple, si un champ est d\u00e9cr\u00e9t\u00e9 \"Not Null\", il doit le rester quelque soit la situation, et \u00e0 aucun moment une requ\u00eate ne doit surprendre ce champs avec une valeur nulle, m\u00eame si c'est juste une valeur interm\u00e9diaire. La coh\u00e9rence est un principe tr\u00e8s rigide dans les bases de donn\u00e9es relationnelles, et repr\u00e9sente le crit\u00e8re de base pour la gestion des transactions : le C de ACID . Cela dit, dans les syst\u00e8mes Big Data, nous nous trouvons confront\u00e9s \u00e0 un probl\u00e8me de taille : nous devons \u00eatre en pr\u00e9sence d'une infrastructure r\u00e9partie et hautement disponible. Or, il existe un th\u00e9or\u00e8me appel\u00e9 CAP pour Consistency / Availability / Partition tolerance , qui stipule que ces trois propri\u00e9t\u00e9s (notamment la coh\u00e9rence, la disponibilit\u00e9 et la tol\u00e9rance au partitionnement), ne peuvent jamais avoir lieu en m\u00eame temps. Seules deux d'entre elles peuvent \u00eatre respect\u00e9es \u00e0 la fois. Essayons d'expliquer pourquoi. Un syst\u00e8me r\u00e9parti est dit coh\u00e9rent si tous ses noeuds voient les m\u00eames donn\u00e9es en m\u00eame temps. C'est \u00e0 dire que, si nous r\u00e9alisons une op\u00e9ration de lecture sur un syst\u00e8me consistant, il devrait toujours retourner la valeur la plus r\u00e9cente qui ait \u00e9t\u00e9 \u00e9crite, quel que soit l'endroit \u00e0 partir duquel la lecture est effectu\u00e9e. Ainsi, si une donn\u00e9e est modifi\u00e9e sur un noeud particulier, pour conserver la coh\u00e9rence demand\u00e9e, aucune op\u00e9ration de lecture ne doit \u00eatre permise avant d'avoir mis \u00e0 jour toutes les r\u00e9pliques (copies) de cette donn\u00e9es. Or, les diff\u00e9rents noeuds d'un cluster sont en g\u00e9n\u00e9ral distants, parfois m\u00eame g\u00e9ographiquement, il est donc n\u00e9cessaire d'attendre que la propagation de la modification se fasse sur le r\u00e9seau, pour effectuer n'importe quelle op\u00e9ration, m\u00eame une lecture. Ceci va rendre nos donn\u00e9es indisponibles \u00e0 la lecture pendant tout le temps que durera l'op\u00e9ration de synchronisation, qui est un temps incertain puisque... r\u00e9seau. Assurer donc une coh\u00e9rence forte dans un syst\u00e8me distribu\u00e9 est en contradiction avec le besoin de disponibilit\u00e9 du syst\u00e8me et des donn\u00e9es. D'ailleurs, c'est ce que font les bases de donn\u00e9es relationnelles r\u00e9parties, qui conservent les propri\u00e9t\u00e9s ACID tout en distribuant les donn\u00e9es, mais qui souffrent d'un manque notoire de performance. Les syst\u00e8mes Big Data, subissant les contraintes des V pr\u00e9c\u00e9demment cit\u00e9s, doivent donc faire un choix. Or ce choix est loin d'\u00eatre facile : qui voudra acheter un syst\u00e8me qui pr\u00f4ne haut et fort qu'il est incoh\u00e9rent ? L'id\u00e9e serait donc de partir sur le principe de coh\u00e9rence \u00e9ventuelle ou parfois de coh\u00e9rence ajustable . Ainsi, un syst\u00e8me Big Data est un syst\u00e8me principalement disponible, fondamentalement r\u00e9parti, et qui assure une coh\u00e9rence \u00e9ventuelle au bout d'un temps g\u00e9n\u00e9ralement n\u00e9gligeable, avec la possibilit\u00e9 de configurer les niveau de coh\u00e9rence parfois m\u00eame dynamiquement. Les experts les appellent donc les syst\u00e8mes BASE (admirez le jeux de mot.. ACID, BASE \ud83d\ude0e): B asically A vailable S oft-state E ventual consistency La propri\u00e9t\u00e9 de Soft State ou d'\u00e9tat \"mou\" veut dire que l'\u00e9tat du syst\u00e8me peut changer dans le temps, m\u00eame sans qu'il y ait une nouvelle entr\u00e9e, \u00e0 cause du principe de coh\u00e9rence \u00e9ventuelle expliqu\u00e9 pr\u00e9c\u00e9demment. Maintenant que vous \u00eates plus familiaris\u00e9s avec les caract\u00e9ristiques d'un syst\u00e8me Big Data, listons quelques principes, appel\u00e9s ici MOTTOS , qui vont r\u00e9gir nos futures d\u00e9cisions dans ce domaine. Principes de base du Domaine des Big Data Il est important, avant d'entamer n'importe quel travail sur les syst\u00e8mes Big Data, de consid\u00e9rer certains principes, qui sont parfois en enti\u00e8re contradiction avec les principes classiques de d\u00e9veloppement d'application. Ce n'est pas si \u00e9tonnant : le domaine des Big Data n'est pas cens\u00e9 prendre la place des domaines relationnel et d\u00e9cisionnel, mais plut\u00f4t les enrichir et les agr\u00e9menter. MOTTO 1 : Stocker d'abord, r\u00e9fl\u00e9chir ensuite \u00c0 cause de la v\u00e9locit\u00e9, il est important de consid\u00e9rer qu'il nous sera parfois difficile, voire impossible, de nettoyer les donn\u00e9es ou de faire un traitement quelconque dessus, avant de les stocker. Cela risque dans bien des cas de nous faire perdre des donn\u00e9es, le cauchemar de tout scientifique des donn\u00e9es! Nous devons donc envisager la possibilit\u00e9 de d\u00e9finir des syst\u00e8mes de stockage qui contiennent des donn\u00e9es non nettoy\u00e9es, en vrac (appel\u00e9es raw data ), pour ensuite lancer des traitements dessus.. l'horreur pour un gestionnaire de bases des donn\u00e9es! \ud83d\ude31 Bien entendu, ces \"bases\" ne sont pas con\u00e7ues pour \u00eatre directement exploit\u00e9es par des applications externes, mais plut\u00f4t pour conserver le plus longtemps possibles les donn\u00e9es brutes, sans perte, qui pourraient eventuellement \u00eatre r\u00e9utilis\u00e9es pour d'autres fins. MOTTO 2 : Absolument TOUTES les donn\u00e9es sont importantes! D'o\u00f9 l'int\u00e9r\u00eat du MOTTO 1 . Il nous est parfois difficile, au tout d\u00e9but de la conception des syst\u00e8mes Big Data, de cerner toutes les possibilit\u00e9s offertes par ces syst\u00e8mes et par les donn\u00e9es que nous avons \u00e0 notre disposition. Nous sommes donc en g\u00e9n\u00e9ral tent\u00e9s de supprimer les donn\u00e9es dont nous n'avons pas besoin une fois extraite l'information imm\u00e9diatement utile. Cela dit, gr\u00e2ce \u00e0 l'accessibilit\u00e9 des syst\u00e8mes de stockage magn\u00e9tiques et leur prix de plus en plus bas, nous consid\u00e9rons qu'il est largement plus b\u00e9n\u00e9fique de stocker des donn\u00e9es qu'on n'utilisera peut-\u00eatre jamais, plut\u00f4t que de gagner de la place et perdre un potentiel pouvoir concurrentiel. MOTTO 3 : Ce sont les donn\u00e9es qui pilotent le traitement Dans un syst\u00e8me op\u00e9rationnel classique, ainsi que dans la plupart des syst\u00e8mes d\u00e9cisionnels, ce sont les besoins m\u00e9tier qui pr\u00e9valoient : le responsable m\u00e9tier commence par d\u00e9finir les besoins (ou les KPIs : Key Performance Indicators dans le cas d'un syst\u00e8me d\u00e9cisionnel), puis le responsable technique con\u00e7oit les structures de donn\u00e9es pour r\u00e9pondre \u00e0 ces besoins. Par essence, un syst\u00e8me Big Data fonctionne diff\u00e9remment : les donn\u00e9es sont collect\u00e9es tout d'abord \u00e0 partir de toutes les sources possibles; des traitements de fouille et d'exploration de ces donn\u00e9es sont lanc\u00e9s ensuite, pour extraire de la valeur \u00e0 partir de ces donn\u00e9es. L'objectif est toujours le m\u00eame : chercher l'effet WOW! D'o\u00f9 l'int\u00e9r\u00eat de ce MOTTO : d\u00e9finir le traitement \u00e0 r\u00e9aliser d\u00e9pend des donn\u00e9es que nous avons r\u00e9ussi \u00e0 collecter, et pas le contraire. Cela implique donc l'utilisation d'autres types de syst\u00e8mes de traitement et d'algorithmes d'analyse. MOTTO 4 : Co-localisation des donn\u00e9es et du traitement Un syst\u00e8me classique \u00e0 plusieurs couches, tel que le syst\u00e8me trois tiers par exemple, se base sur le principe de s\u00e9paration des donn\u00e9es et du traitement. On trouve en g\u00e9n\u00e9ral des donn\u00e9es sur un serveur de bases de donn\u00e9es s\u00e9par\u00e9, et les traitement complexes sur un serveur d'application qui se charge de l'aggr\u00e9gation et de l'affichage de ces donn\u00e9es. Ceci est agr\u00e9ment\u00e9 d'un langage de requ\u00eatage d\u00e9claratif (typiquement SQL) pour r\u00e9aliser des op\u00e9rations de filtrage, parfois assez lourdes et complexes, au niveau de la base de donn\u00e9es. Cela dit, dans un contexte Big Data, le volume des donn\u00e9es peut s'av\u00e9rer assez cons\u00e9quent, trop m\u00eame pour envisager de le d\u00e9placer \u00e0 chaque fois vers un autre syst\u00e8me pour en extraire une vraie valeur. De plus, compter sur un langage comme SQL pour diminuer le volume ou faire de simples agr\u00e9gations au niveau de la base de donn\u00e9es pourra la rendre indisponible pendant un moment (car n'oublions pas que nous parlons d'un syst\u00e8me r\u00e9parti), ce qui va \u00e0 l'encontre du principe de v\u00e9locit\u00e9, qui exige une disponibilit\u00e9 \u00e0 toute \u00e9preuve du syst\u00e8me de stockage. C'est pour cette raison que, pour r\u00e9aliser les traitements voulus en un temps raisonnable et sans avoir \u00e0 trimballer les donn\u00e9es sur le r\u00e9seau, il est question dans les syst\u00e8mes Big Data de d\u00e9placer le traitement vers les donn\u00e9es massives, au lieu de d\u00e9placer les donn\u00e9es vers le traitement. MOTTO 5 : La redondance, c'est bien Dans les bases de donn\u00e9es relationnelles, le plus grand ennemi \u00e0 combattre dans la conception de la structure de donn\u00e9es est la redondance, et ce pour deux raisons. La premi\u00e8re, \u00e9vidente, est le gain d'espace : notre espace de stockage est pr\u00e9cieux, et nous devons \u00e9viter de le gaspiller sans raison pr\u00e9cise. La deuxi\u00e8me est un besoin de coh\u00e9rence : si nous dupliquons une m\u00eame information \u00e0 plusieurs endroits dans la base, nous devrons par la suite faire attention, parfois par des m\u00e9canismes compliqu\u00e9s et co\u00fbteux, \u00e0 ce que cette information soit mise \u00e0 jour instantan\u00e9ment sur la totalit\u00e9 de ses copies. Ce besoin d'\u00e9viter la redondance a cr\u00e9\u00e9 la n\u00e9cessit\u00e9 d'utiliser plusieurs techniques, telles que les jointures et clefs \u00e9trang\u00e8res, et entra\u00eene parfois la cr\u00e9ation d'un tr\u00e8s grand nombre de tables. Ceci rajoute une complexit\u00e9 pour le requ\u00eatage, et une lourdeur d'ex\u00e9cution des t\u00e2ches sur la base. Un syst\u00e8me Big Data qui, non seulement est caract\u00e9ris\u00e9 par un gros volume de donn\u00e9es, mais \u00e9galement une grande v\u00e9locit\u00e9, et qui doit donc \u00eatre imm\u00e9diatement disponible, ne peut pas se permettre de gaspiller ses ressources en requ\u00eates inutiles. On tol\u00e8re donc \u00e0 un certain point les risques dus \u00e0 la redondance, pour gagner en disponibilit\u00e9, primordiale dans ce type de syst\u00e8mes. D'autre part, un syst\u00e8me Big Data est un syst\u00e8me r\u00e9parti par excellence, et dans un syst\u00e8me r\u00e9parti, il est primordial d'assurer une bonne tol\u00e9rance aux fautes en cr\u00e9ant des r\u00e9pliques des donn\u00e9es, diss\u00e9min\u00e9es partout sur le cluster. Ces r\u00e9pliques assurent qu'aucune machine n'est compl\u00e8tement indispensable, et diminue le risque d'indisponibilit\u00e9 des donn\u00e9es. Un autre signe de redondance. MOTTO 6 : Vive le Polyglottisme! \u00catre polyglotte, c'est \u00eatre capable de parler plusieurs langues. Et les syst\u00e8mes Big Data encouragent le polyglottisme. En effet, ce sont des syst\u00e8mes complexes qui impliquent en g\u00e9n\u00e9ral plusieurs traitements et plusieurs types de donn\u00e9es diff\u00e9rentes (donn\u00e9es brutes, donn\u00e9es nettoy\u00e9es, donn\u00e9es trait\u00e9es), ce qui fait qu'il existe deux principes importants \u00e0 encourager : Polyglot Programming : Une application peut comporter plusieurs langages et paradigmes de programmation, chacun assurant un besoin particulier, de fa\u00e7on \u00e0 profiter des avantages de chacun \u00e0 sa juste valeur. Polyglot Persistence : Dans une m\u00eame application, il est possible d'utiliser plusieurs syst\u00e8mes de stockage diff\u00e9rents (relationnels, NOSQL, syst\u00e8mes de fichiers, etc.). Gr\u00e2ce \u00e0 ces deux principes, on pourra cr\u00e9er des applications complexes mais compl\u00e8tes, qui permettent d'assurer tous les besoins en terme de stockage et de traitement. Technologies et Paradigmes Les op\u00e9rations \u00e0 r\u00e9aliser sur les syst\u00e8mes Big Data consistent principalement en : Ingestion des donn\u00e9es : repr\u00e9sente les phases de collecte et d'importation des donn\u00e9es pour \u00eatre stock\u00e9es ou trait\u00e9es \u00e0 la vol\u00e9e. Cela peut se faire en \"temps r\u00e9el\", c'est \u00e0 dire que les donn\u00e9es sont import\u00e9es au moment o\u00f9 elles sont \u00e9mises par leur source, ou bien \"par lots\", ce qui veut dire que les donn\u00e9es sont import\u00e9es par portions \u00e0 intervalles r\u00e9gulier. Exemples de technologies Apache Kafka , Amazon Kinesis , Apache Flume , Sqoop , etc. Stockage des donn\u00e9es : Les syst\u00e8mes de stockage de donn\u00e9es respectant les propri\u00e9t\u00e9s le Big Data se distinguent principalement en syst\u00e8mes de fichiers distribu\u00e9s, tel que Hadoop HDFS ou Google GFS , ou bases de donn\u00e9es NOSQL, tel que MongoDB , Cassandra , Redis ou Neo4J . Traitement des donn\u00e9es : Plusieurs types de traitement de donn\u00e9es sont possibles, nous citons : Traitement par lot (Batch Processing) : c'est le traitement des donn\u00e9es au repos (data at rest) qui se fait sur l'ensemble des donn\u00e9es stock\u00e9es, sans avoir besoin d'une interaction avec l'utilisateur. Le traitement par lot est adapt\u00e9 principalement aux op\u00e9rations ayant lieu \u00e0 la fin d'un cycle, permettant d'avoir une vision globale sur la totalit\u00e9 des donn\u00e9es, par exemple pour avoir un rapport global ou une analyse mensuelle. Les op\u00e9rations de traitement par lots sont en g\u00e9n\u00e9ral lanc\u00e9es \u00e0 des p\u00e9riodes r\u00e9guli\u00e8res, car elles sont connues pour avoir une grande latence (temps total de traitement). Exemples de technologies Hadoop Map Reduce et Spark Batch . Traitement en Streaming (Stream Processing) : c'est le traitement des donn\u00e9es en transit (data in motion) , ou en d'autres termes, le traitement des donn\u00e9es pendant qu'elles sont produites ou re\u00e7ues. Les donn\u00e9es \u00e9tant en g\u00e9n\u00e9ral cr\u00e9\u00e9es en tant que flux continu (\u00e9v\u00e8nements de capteurs, activit\u00e9 des utilisateurs sur un site web, flux vid\u00e9o, etc.), elles sont captur\u00e9es comme une s\u00e9rie d'\u00e9v\u00e8nements continus dans le temps. Avant la cr\u00e9ation des traitements en streaming, ces donn\u00e9es \u00e9taient stock\u00e9es dans une base de donn\u00e9es, un syst\u00e8me de fichier ou tout autre forme de stockage en masse. Les applications appelleront ensuite les donn\u00e9es au besoin. Gr\u00e2ce \u00e0 ce nouveau paradigme, les donn\u00e9es peuvent maintenant \u00eatre trait\u00e9es \u00e0 la vol\u00e9e, ce qui permet \u00e0 la couche applicative d'\u00eatre toujours sur \u00e9coute et \u00e0 jour. Exemples de technologies Apache Flink et Apache Storm . Traitement par Micro-Lot (Micro-Batch Processing) : c'est la pratique de collecter les donn\u00e9es en petits groupes (appel\u00e9s des micro-lots ou des micro-batchs ) pour les traiter. Contrairement au traditionnel traitement par lot, cette variante fait en sorte que le traitement des donn\u00e9es soit plus fr\u00e9quent, et que les r\u00e9sultats soient produits avec une latence beaucoup plus petite. Les donn\u00e9es sont collect\u00e9es par intervalles selon un seuil pr\u00e9d\u00e9fini, limit\u00e9 par un temps (par exemple toutes les secondes), ou par un nombre (tous les 20 \u00e9l\u00e9ments). Ce traitement est en g\u00e9n\u00e9ral une alternative au traitement en streaming, o\u00f9 les donn\u00e9es sont trait\u00e9es \u00e0 la vol\u00e9e, mais risquent d'\u00eatre perdues si le temps de traitement est sup\u00e9rieur \u00e0 la fr\u00e9quence de g\u00e9n\u00e9ration des donn\u00e9es. Le micro-batching permet, par contraste, de sauvegarder les donn\u00e9es dans un buffer, ralentissant ainsi le flux g\u00e9n\u00e9r\u00e9. D'autre part, les donn\u00e9es \u00e9tant trait\u00e9es par micro-lots, il est possible d'avoir une visibilit\u00e9 sur ce petit lot de donn\u00e9es, contrairement au traitement en streaming qui n'a de visibilit\u00e9 que sur la derni\u00e8re donn\u00e9e g\u00e9n\u00e9r\u00e9e, \u00e0 moins de proc\u00e9der \u00e0 des m\u00e9canismes parfois co\u00fbteux. En contrepartie, le traitement en micro-batch donne des r\u00e9sultats moins r\u00e9cents que le \"vrai\" streaming, et s'ex\u00e9cute sous forme de bursts r\u00e9guliers, qui peuvent parfois \u00eatre g\u00eanants pour le syst\u00e8me sous-jacent. Exemples de technologies Spark Streaming et Logstash . Traitement Interactif (Interactive Processing) : Dans les syst\u00e8mes Big Data, la notion de transaction n'est plus exactement la m\u00eame que pour les syst\u00e8mes classiques: finies les sacro-saintes propri\u00e9t\u00e9s ACID dont le premier objectif est d'avoir des donn\u00e9es correctes et coh\u00e9rentes, et bonjour les propri\u00e9t\u00e9s BASE, qui favorisent un acc\u00e8s moins rigide aux donn\u00e9es. On parle donc rarement de traitement transactionnel en Big Data, mais de traitements plut\u00f4t interactifs : une requ\u00eate est envoy\u00e9e par le client, trait\u00e9e imm\u00e9diatement par le syst\u00e8me qui renverra un r\u00e9sultat dans un temps raisonnable. On parle alors d' interaction entre l'utilisateur et le syst\u00e8me. Les traitements en batch et en streaming ne sont pas cens\u00e9s communiquer avec un utilisateur de l'autre c\u00f4t\u00e9. En g\u00e9n\u00e9ral, les r\u00e9sultats de ces traitements sont enregistr\u00e9s dans un syst\u00e8me de stockage, qui sera, lui, par la suite interrog\u00e9 par l'utilisateur. Le traitement interactif est donc le r\u00e9sultat d'une requ\u00eate de l'utilisateur, faite en g\u00e9n\u00e9ral sur une base de donn\u00e9es (relationnelle ou NOSQL). Exemples de technologies Apache Drill , Cloudera Impala ou Apache Zeppelin .","title":"P1 - Introduction au Big Data"},{"location":"p1-big-data/#partie-1-introduction-au-big-data","text":"","title":"Partie 1 - Introduction au Big Data"},{"location":"p1-big-data/#les-big-data-pourquoi","text":"L'\u00eatre humain, \u00e0 travers l'humanit\u00e9, a toujours cherch\u00e9 trois choses : Savoir (qu'est-ce qui s'est pass\u00e9?), Comprendre (pourquoi cela s'est-il pass\u00e9?) et Pr\u00e9dire (qu'est-ce que qui se passera?). Plusieurs cultures ont clam\u00e9 l'omniscience en ayant recours \u00e0 des subterfuges, tels que les oracles, l'astrologie, le tarot, ou les boules de cristal. Cela dit, ces moyens ne sont gu\u00e8res satisfaisants \u00e0 l'esprit m\u00e9ticuleux du scientifique, qui cherche toujours une explication logique et rationnelle \u00e0 tout \u00e9v\u00e8nement, et une justification convainquante \u00e0 tout comportement. Le scientifique se base sur des faits. Il veut arriver \u00e0 faire de la magie gr\u00e2ce \u00e0 la technologie. Pour arriver \u00e0 ces fins, le scientifique a besoin de donn\u00e9es. L'int\u00e9r\u00eat de collecter des donn\u00e9es et de les exploiter a longtemps \u00e9t\u00e9 n\u00e9glig\u00e9, et a \u00e9t\u00e9 limit\u00e9 au peu de donn\u00e9es, jug\u00e9es \"utiles\", qui semblaient suffisantes pour atteindre un objectif imm\u00e9diat. Cependant, adopter le chemin \u00e9vident et peu risqu\u00e9 n'aurait jamais permis de r\u00e9aliser les miracles auxquelles on s'attendait. Il fallait trouver un autre moyen.. Le terme Big Data est apparu peu de temps apr\u00e8s l'apparition du terme Web 2.0, qui montre la transition de l'internet d'une \u00e8re o\u00f9 l'ajout des donn\u00e9es \u00e9tait exclusivement r\u00e9serv\u00e9 \u00e0 une \u00e9lite experte, o\u00f9 le volume des donn\u00e9es disponible \u00e9tait petit mais o\u00f9 les donn\u00e9es \u00e9taient pr\u00e9cieuses et pertinentes, vers une \u00e8re o\u00f9 tout un chacun \u00e9tait capable d'introduire des connaissances, v\u00e9ridiques ou pas, qui seraient sauvegard\u00e9es dans une m\u00e9moire collective jusqu'\u00e0 la fin des temps. Ce changement de paradigme a entrain\u00e9 le besoin d'infrastructures nouvelles, qui seraient capables, non seulement de stocker ces donn\u00e9es, mais \u00e9galement d'en extraire de la valeur. Ces infrastructures auront la capacit\u00e9 de g\u00e9rer toute la cha\u00eene logistique des donn\u00e9es, de la collecte vers l'affichage. Cela semble \u00e9vident, me direz-vous, car les syst\u00e8mes classiques sont capables de faire cela. Qui stocke mieux les donn\u00e9es qu'une bonne vieille base de donn\u00e9es relationnelle? Le probl\u00e8me est que les donn\u00e9es dites \"Big Data\" sont caract\u00e9ris\u00e9es par des propri\u00e9t\u00e9s telles que, les syst\u00e8mes classiques de stockage et de traitement auraient du mal \u00e0 les exploiter \u00e0 leur juste valeur.","title":"Les \"Big Data\", Pourquoi?"},{"location":"p1-big-data/#caracteristiques-des-donnees-massives","text":"Le terme \"donn\u00e9es massives\", ou \"Big Data\", ne donne \u00e0 mon avis pas enti\u00e8rement justice aux donn\u00e9es de notre domaine. En effet, il repr\u00e9sente une seule caract\u00e9ristique parmis plusieurs, le Volume, qui, m\u00eame si elle semble \u00eatre la plus importante, est loin d'\u00eatre la plus critique. En effet, les donn\u00e9es massives sont caract\u00e9ris\u00e9es par les fameux *V . Il en existe plusieurs (10 jusqu'\u00e0 ce jour si je ne m'abuse, certains en citent m\u00eame 42!!!), mais pourraient \u00e0 mon avis \u00eatre r\u00e9sum\u00e9s en trois caract\u00e9ristiques primordiales, autours de la combinaison desquelles tournent toutes les d\u00e9cisions prises dans ce domaine. Volume C'est \u00e9videmment le V le plus manifeste, qui caract\u00e9rise le fait que les donn\u00e9es ont un volume \u00e9norme qui peut atteindre des valeurs de l'ordre de Exa-, Zetta- ou Yottaoctet (allant jusqu'\u00e0 2^{80} 2^{80} octets!). Mais ceci n'est pas tout. Un volume \u00e9norme, s'il reste constant, est g\u00e9rable : il suffit de trouver une machine suffisamment puissante pour le sauvegarder. Le probl\u00e8me avec la propri\u00e9t\u00e9 du volume, c'est qu'il augmente de fa\u00e7on continue, ce qui rend sa gestion beaucoup plus ardue. Une citation bien connue, et qui se re-confirme chaque ann\u00e9e, atteste que \"Over the last two years alone 90 percent of the data in the world was generated.\" Il est donc primordial de trouver un moyen de g\u00e9rer ce volume toujours croissant des donn\u00e9es. V\u00e9locit\u00e9 Cette propri\u00e9t\u00e9 est, \u00e0 mon avis, la plus probl\u00e9matique des trois, car, coupl\u00e9e avec le volume, elle rend les syst\u00e8me actuels obsol\u00e8tes. En effet, la v\u00e9locit\u00e9 est, litt\u00e9ralement, \"La vitesse avec laquelle quelque chose se d\u00e9place dans une direction particuli\u00e8re\". Dans notre cas, la v\u00e9locit\u00e9 des donn\u00e9es est la responsable directe du volume croissant des donn\u00e9es dans le syst\u00e8me. Elle est provoqu\u00e9e par une arriv\u00e9e des donn\u00e9es dans le syst\u00e8me sous la forme d'un flux constant qui demande \u00e0 \u00eatre stock\u00e9 et trait\u00e9 imm\u00e9diatement, ainsi que le besoin croissant des utilisateurs d'avoir une repr\u00e9sentation r\u00e9cente et fid\u00e8le de l'\u00e9tat des donn\u00e9es. D'ailleurs, cette propri\u00e9t\u00e9 a engendr\u00e9 une autre pr\u00e9occupation des analystes des donn\u00e9es, qui est de fournir une introspection en temps r\u00e9el sur les donn\u00e9es, les qualifiant ainsi de \" Fast Data \". Vari\u00e9t\u00e9 Ce qui distingue vraiment les donn\u00e9es massives des donn\u00e9es g\u00e9r\u00e9es classiquement dans des bases de donn\u00e9es op\u00e9rationnelles, c'est le support des donn\u00e9es semi- et non structur\u00e9es. En effet, les donn\u00e9es non structur\u00e9es sont des donn\u00e9es qu'on stocke dans un format qui n'est pas d\u00e9fini \u00e0 la cr\u00e9ation, telles que les donn\u00e9es textuelles, images ou sons. Les donn\u00e9es semi-structur\u00e9es sont des donn\u00e9es qui contiennent une structure, mais une structure qui n'est pas rigide, et dont on ne d\u00e9finit pas les contraintes \u00e0 l'insertion de la donn\u00e9e, contrairement aux donn\u00e9es structur\u00e9es (se trouvant typiquement dans des bases de donn\u00e9es relationnelles) qui, si elles ne respectent pas la structure d\u00e9finie, sont consid\u00e9r\u00e9es fausses et ne sont pas autoris\u00e9es \u00e0 \u00eatre enregistr\u00e9es. On estime que seules 15% des donn\u00e9es dans une entreprise sont des donn\u00e9es structur\u00e9es, contre 85% qui ne le sont pas! Dans une optique centr\u00e9e sur les donn\u00e9es, dont le but est de gagner le maximum de vision \u00e0 partir des donn\u00e9es, perdre autant de sources d'information est un vrai probl\u00e8me. Il est donc important que les syst\u00e8mes Big Data sachent interpr\u00e9ter ces donn\u00e9es et en extraire le maximum de valeur. Toutes les d\u00e9cisions, choix et propri\u00e9t\u00e9s prises au niveau des architectures et infrastructures Big Data sont r\u00e9gies par ces trois caract\u00e9ristiques, ce qui va compl\u00e8tement changer la vision \"relationnelle\" que tout informaticien qui se respecte a acquis tout au long de ses ann\u00e9es d'\u00e9tude et de travail. Cela dit, ce ne sont pas les seules propri\u00e9t\u00e9s. D'autres V ont vu le jour, mais sans jamais avoir autant d'impact sur l'infrastructre, plut\u00f4t dans la fa\u00e7on de d\u00e9finir les processus, la gouvernance et les approches m\u00e9tier \u00e0 adopter. Nous citons par exemple : V\u00e9racit\u00e9 : c'est la confiance que nous devons avoir en nos donn\u00e9es. Cette propri\u00e9t\u00e9 est inversement proportionnelle au volume et \u00e0 la vari\u00e9t\u00e9 : plus nos donn\u00e9es sont fiables, moins elles sont diversifi\u00e9es et volumineuses! Valeur : c'est la capacit\u00e9 d'extraire de la valeur m\u00e9tier \u00e0 partir des donn\u00e9es. Variabilit\u00e9 : une extension de la vari\u00e9t\u00e9, qui indique \u00e0 quel point nos donn\u00e9es peuvent avoir des dimensions diff\u00e9rentes \u00e0 partir des sources de donn\u00e9es disparates. Visualisation : c'est la capacit\u00e9 de nos donn\u00e9es \u00e0 \u00eatre repr\u00e9sent\u00e9es par les outils de visualisation classiques. etc.","title":"Caract\u00e9ristiques des Donn\u00e9es Massives"},{"location":"p1-big-data/#infrastructure-big-data-besoins","text":"Les caract\u00e9ristiques des donn\u00e9es Big Data cit\u00e9es ci-dessus, entra\u00eenent des besoins particuliers en termes d'infrastructure et d'architecture. Volume La caract\u00e9ristique de volume, qui implique que la taille des donn\u00e9es augmente de fa\u00e7on r\u00e9guli\u00e8re, fait qu'on ne peut plus se contenter d'un syst\u00e8me centralis\u00e9 classique. Car dans un syst\u00e8me centralis\u00e9 (donc bas\u00e9 sur une seule machine), augmenter les ressources de stockage au besoin implique ce que nous appelons une scalabilit\u00e9 verticale ou un scale up , qui veut dire une augmentation des capacit\u00e9s du serveur de stockage en rajoutant des processeurs, de la RAM ou des disques. Cependant, cette solution, bien qu'elle soit intuitive, rapide et ne requiert pas de changement architecturaux cons\u00e9quents, implique en g\u00e9n\u00e9ral un temps d'arr\u00eat pendant l'installation, ainsi qu'une d\u00e9pense assez cons\u00e9quente pour faire l'acquisition d'un serveur puissant. De plus, une machine unique atteindra rapidement une limite mat\u00e9rielle, car il vous est impossible d'augmenter ses ressources ind\u00e9finiment. En contrepartie, il est possible de penser que, face \u00e0 un volume de donn\u00e9es toujours en augmentation, il serait plus judicieux de rajouter des machines au besoin, cr\u00e9ant ainsi un cluster de machines interconnect\u00e9es, ou syst\u00e8me r\u00e9parti , dont la taille et la capacit\u00e9 sont virtuellement illimit\u00e9es. Nous sommes donc face \u00e0 un autre type de scalabilit\u00e9 : la scalabilit\u00e9 horizontale ou le scale out . Donc Volume => Scalabilit\u00e9 Horizontale V\u00e9locit\u00e9 La v\u00e9locit\u00e9 est une propri\u00e9t\u00e9 qui, coupl\u00e9e au volume, rend la gestion de l'infrastructure un vrai cauchemar. En effet, g\u00e9rer des donn\u00e9es en continuelle arriv\u00e9e implique qu'il y'a un risque \u00e9norme de perte de donn\u00e9es, si elles ne sont pas manipul\u00e9es \u00e0 temps. C'est pour cette raison qu'un syst\u00e8me Big Data se doit d'\u00eatre continuellement disponible : toute requ\u00eate de lecture ou d'\u00e9criture doit \u00eatre trait\u00e9e en un temps raisonnable, et le syst\u00e8me doit \u00eatre continuellement alerte pour saisir toutes les donn\u00e9es, sans risquer de les perdre. Ainsi V\u00e9locit\u00e9 => Disponibilit\u00e9 Vari\u00e9t\u00e9 La vari\u00e9t\u00e9 de donn\u00e9es implique non seulement que nous sommes en pr\u00e9sence de donn\u00e9es structur\u00e9es, semi-structur\u00e9es et non structur\u00e9es, mais \u00e9galement que ces donn\u00e9es peuvent parvenir de sources diff\u00e9rentes, avec des formats diff\u00e9rents, et que m\u00eame \u00e0 partir d'une m\u00eame source, ce format peut changer d'un moment \u00e0 un autre. Dans les syst\u00e8mes classiques, tout ce qui est variable doit passer par une couche d'homog\u00e9n\u00e9isation qui transformera chaque entr\u00e9e ou enregistrement dans la forme souhait\u00e9e, en remplissant par des valeurs NULL les donn\u00e9es manquantes. Rajouter cette couche d'homog\u00e9n\u00e9isation aura un double impact n\u00e9gatif sur notre syst\u00e8me : (1) \u00e0 cause de la v\u00e9locit\u00e9, cette op\u00e9ration risquera de ralentir la collecte et saisie des donn\u00e9es entrantes, et (2) on pourra subir une perte de donn\u00e9es suite \u00e0 ces transformations. C'est pour ces raisons qu'un syst\u00e8me Big Data se doit de supporter des types de donn\u00e9es changeants, sans pour autant requ\u00e9rir \u00e0 des subterfuges qui alourdissent ou contournent le syst\u00e8me de stockage. D'o\u00f9 Vari\u00e9t\u00e9 => Flexibilit\u00e9","title":"Infrastructure Big Data : Besoins"},{"location":"p1-big-data/#theoreme-cap","text":"Les besoins de scalabilit\u00e9, disponibilit\u00e9 et flexibilit\u00e9, obligatoires pour avoir un syst\u00e8me Big Data en bonne et due forme, se trouvent confront\u00e9s \u00e0 une contrainte de taille... et qu'en est-il de la coh\u00e9rence (commun\u00e9ment appel\u00e9e aussi consistence, par anglicisme)? La coh\u00e9rence repr\u00e9sente en effet un must pour les syst\u00e8mes relationnels classiques, et une base sur laquelle sont prises toutes les d\u00e9cisions conceptuelles et techniques. Elle repr\u00e9sente le fait que les donn\u00e9es stables doivent respecter toutes les contraintes d'int\u00e9grit\u00e9 d\u00e9finies \u00e0 la cr\u00e9ation de la base de donn\u00e9e. Par exemple, si un champ est d\u00e9cr\u00e9t\u00e9 \"Not Null\", il doit le rester quelque soit la situation, et \u00e0 aucun moment une requ\u00eate ne doit surprendre ce champs avec une valeur nulle, m\u00eame si c'est juste une valeur interm\u00e9diaire. La coh\u00e9rence est un principe tr\u00e8s rigide dans les bases de donn\u00e9es relationnelles, et repr\u00e9sente le crit\u00e8re de base pour la gestion des transactions : le C de ACID . Cela dit, dans les syst\u00e8mes Big Data, nous nous trouvons confront\u00e9s \u00e0 un probl\u00e8me de taille : nous devons \u00eatre en pr\u00e9sence d'une infrastructure r\u00e9partie et hautement disponible. Or, il existe un th\u00e9or\u00e8me appel\u00e9 CAP pour Consistency / Availability / Partition tolerance , qui stipule que ces trois propri\u00e9t\u00e9s (notamment la coh\u00e9rence, la disponibilit\u00e9 et la tol\u00e9rance au partitionnement), ne peuvent jamais avoir lieu en m\u00eame temps. Seules deux d'entre elles peuvent \u00eatre respect\u00e9es \u00e0 la fois. Essayons d'expliquer pourquoi. Un syst\u00e8me r\u00e9parti est dit coh\u00e9rent si tous ses noeuds voient les m\u00eames donn\u00e9es en m\u00eame temps. C'est \u00e0 dire que, si nous r\u00e9alisons une op\u00e9ration de lecture sur un syst\u00e8me consistant, il devrait toujours retourner la valeur la plus r\u00e9cente qui ait \u00e9t\u00e9 \u00e9crite, quel que soit l'endroit \u00e0 partir duquel la lecture est effectu\u00e9e. Ainsi, si une donn\u00e9e est modifi\u00e9e sur un noeud particulier, pour conserver la coh\u00e9rence demand\u00e9e, aucune op\u00e9ration de lecture ne doit \u00eatre permise avant d'avoir mis \u00e0 jour toutes les r\u00e9pliques (copies) de cette donn\u00e9es. Or, les diff\u00e9rents noeuds d'un cluster sont en g\u00e9n\u00e9ral distants, parfois m\u00eame g\u00e9ographiquement, il est donc n\u00e9cessaire d'attendre que la propagation de la modification se fasse sur le r\u00e9seau, pour effectuer n'importe quelle op\u00e9ration, m\u00eame une lecture. Ceci va rendre nos donn\u00e9es indisponibles \u00e0 la lecture pendant tout le temps que durera l'op\u00e9ration de synchronisation, qui est un temps incertain puisque... r\u00e9seau. Assurer donc une coh\u00e9rence forte dans un syst\u00e8me distribu\u00e9 est en contradiction avec le besoin de disponibilit\u00e9 du syst\u00e8me et des donn\u00e9es. D'ailleurs, c'est ce que font les bases de donn\u00e9es relationnelles r\u00e9parties, qui conservent les propri\u00e9t\u00e9s ACID tout en distribuant les donn\u00e9es, mais qui souffrent d'un manque notoire de performance. Les syst\u00e8mes Big Data, subissant les contraintes des V pr\u00e9c\u00e9demment cit\u00e9s, doivent donc faire un choix. Or ce choix est loin d'\u00eatre facile : qui voudra acheter un syst\u00e8me qui pr\u00f4ne haut et fort qu'il est incoh\u00e9rent ? L'id\u00e9e serait donc de partir sur le principe de coh\u00e9rence \u00e9ventuelle ou parfois de coh\u00e9rence ajustable . Ainsi, un syst\u00e8me Big Data est un syst\u00e8me principalement disponible, fondamentalement r\u00e9parti, et qui assure une coh\u00e9rence \u00e9ventuelle au bout d'un temps g\u00e9n\u00e9ralement n\u00e9gligeable, avec la possibilit\u00e9 de configurer les niveau de coh\u00e9rence parfois m\u00eame dynamiquement. Les experts les appellent donc les syst\u00e8mes BASE (admirez le jeux de mot.. ACID, BASE \ud83d\ude0e): B asically A vailable S oft-state E ventual consistency La propri\u00e9t\u00e9 de Soft State ou d'\u00e9tat \"mou\" veut dire que l'\u00e9tat du syst\u00e8me peut changer dans le temps, m\u00eame sans qu'il y ait une nouvelle entr\u00e9e, \u00e0 cause du principe de coh\u00e9rence \u00e9ventuelle expliqu\u00e9 pr\u00e9c\u00e9demment. Maintenant que vous \u00eates plus familiaris\u00e9s avec les caract\u00e9ristiques d'un syst\u00e8me Big Data, listons quelques principes, appel\u00e9s ici MOTTOS , qui vont r\u00e9gir nos futures d\u00e9cisions dans ce domaine.","title":"Th\u00e9or\u00e8me CAP"},{"location":"p1-big-data/#principes-de-base-du-domaine-des-big-data","text":"Il est important, avant d'entamer n'importe quel travail sur les syst\u00e8mes Big Data, de consid\u00e9rer certains principes, qui sont parfois en enti\u00e8re contradiction avec les principes classiques de d\u00e9veloppement d'application. Ce n'est pas si \u00e9tonnant : le domaine des Big Data n'est pas cens\u00e9 prendre la place des domaines relationnel et d\u00e9cisionnel, mais plut\u00f4t les enrichir et les agr\u00e9menter. MOTTO 1 : Stocker d'abord, r\u00e9fl\u00e9chir ensuite \u00c0 cause de la v\u00e9locit\u00e9, il est important de consid\u00e9rer qu'il nous sera parfois difficile, voire impossible, de nettoyer les donn\u00e9es ou de faire un traitement quelconque dessus, avant de les stocker. Cela risque dans bien des cas de nous faire perdre des donn\u00e9es, le cauchemar de tout scientifique des donn\u00e9es! Nous devons donc envisager la possibilit\u00e9 de d\u00e9finir des syst\u00e8mes de stockage qui contiennent des donn\u00e9es non nettoy\u00e9es, en vrac (appel\u00e9es raw data ), pour ensuite lancer des traitements dessus.. l'horreur pour un gestionnaire de bases des donn\u00e9es! \ud83d\ude31 Bien entendu, ces \"bases\" ne sont pas con\u00e7ues pour \u00eatre directement exploit\u00e9es par des applications externes, mais plut\u00f4t pour conserver le plus longtemps possibles les donn\u00e9es brutes, sans perte, qui pourraient eventuellement \u00eatre r\u00e9utilis\u00e9es pour d'autres fins. MOTTO 2 : Absolument TOUTES les donn\u00e9es sont importantes! D'o\u00f9 l'int\u00e9r\u00eat du MOTTO 1 . Il nous est parfois difficile, au tout d\u00e9but de la conception des syst\u00e8mes Big Data, de cerner toutes les possibilit\u00e9s offertes par ces syst\u00e8mes et par les donn\u00e9es que nous avons \u00e0 notre disposition. Nous sommes donc en g\u00e9n\u00e9ral tent\u00e9s de supprimer les donn\u00e9es dont nous n'avons pas besoin une fois extraite l'information imm\u00e9diatement utile. Cela dit, gr\u00e2ce \u00e0 l'accessibilit\u00e9 des syst\u00e8mes de stockage magn\u00e9tiques et leur prix de plus en plus bas, nous consid\u00e9rons qu'il est largement plus b\u00e9n\u00e9fique de stocker des donn\u00e9es qu'on n'utilisera peut-\u00eatre jamais, plut\u00f4t que de gagner de la place et perdre un potentiel pouvoir concurrentiel. MOTTO 3 : Ce sont les donn\u00e9es qui pilotent le traitement Dans un syst\u00e8me op\u00e9rationnel classique, ainsi que dans la plupart des syst\u00e8mes d\u00e9cisionnels, ce sont les besoins m\u00e9tier qui pr\u00e9valoient : le responsable m\u00e9tier commence par d\u00e9finir les besoins (ou les KPIs : Key Performance Indicators dans le cas d'un syst\u00e8me d\u00e9cisionnel), puis le responsable technique con\u00e7oit les structures de donn\u00e9es pour r\u00e9pondre \u00e0 ces besoins. Par essence, un syst\u00e8me Big Data fonctionne diff\u00e9remment : les donn\u00e9es sont collect\u00e9es tout d'abord \u00e0 partir de toutes les sources possibles; des traitements de fouille et d'exploration de ces donn\u00e9es sont lanc\u00e9s ensuite, pour extraire de la valeur \u00e0 partir de ces donn\u00e9es. L'objectif est toujours le m\u00eame : chercher l'effet WOW! D'o\u00f9 l'int\u00e9r\u00eat de ce MOTTO : d\u00e9finir le traitement \u00e0 r\u00e9aliser d\u00e9pend des donn\u00e9es que nous avons r\u00e9ussi \u00e0 collecter, et pas le contraire. Cela implique donc l'utilisation d'autres types de syst\u00e8mes de traitement et d'algorithmes d'analyse. MOTTO 4 : Co-localisation des donn\u00e9es et du traitement Un syst\u00e8me classique \u00e0 plusieurs couches, tel que le syst\u00e8me trois tiers par exemple, se base sur le principe de s\u00e9paration des donn\u00e9es et du traitement. On trouve en g\u00e9n\u00e9ral des donn\u00e9es sur un serveur de bases de donn\u00e9es s\u00e9par\u00e9, et les traitement complexes sur un serveur d'application qui se charge de l'aggr\u00e9gation et de l'affichage de ces donn\u00e9es. Ceci est agr\u00e9ment\u00e9 d'un langage de requ\u00eatage d\u00e9claratif (typiquement SQL) pour r\u00e9aliser des op\u00e9rations de filtrage, parfois assez lourdes et complexes, au niveau de la base de donn\u00e9es. Cela dit, dans un contexte Big Data, le volume des donn\u00e9es peut s'av\u00e9rer assez cons\u00e9quent, trop m\u00eame pour envisager de le d\u00e9placer \u00e0 chaque fois vers un autre syst\u00e8me pour en extraire une vraie valeur. De plus, compter sur un langage comme SQL pour diminuer le volume ou faire de simples agr\u00e9gations au niveau de la base de donn\u00e9es pourra la rendre indisponible pendant un moment (car n'oublions pas que nous parlons d'un syst\u00e8me r\u00e9parti), ce qui va \u00e0 l'encontre du principe de v\u00e9locit\u00e9, qui exige une disponibilit\u00e9 \u00e0 toute \u00e9preuve du syst\u00e8me de stockage. C'est pour cette raison que, pour r\u00e9aliser les traitements voulus en un temps raisonnable et sans avoir \u00e0 trimballer les donn\u00e9es sur le r\u00e9seau, il est question dans les syst\u00e8mes Big Data de d\u00e9placer le traitement vers les donn\u00e9es massives, au lieu de d\u00e9placer les donn\u00e9es vers le traitement. MOTTO 5 : La redondance, c'est bien Dans les bases de donn\u00e9es relationnelles, le plus grand ennemi \u00e0 combattre dans la conception de la structure de donn\u00e9es est la redondance, et ce pour deux raisons. La premi\u00e8re, \u00e9vidente, est le gain d'espace : notre espace de stockage est pr\u00e9cieux, et nous devons \u00e9viter de le gaspiller sans raison pr\u00e9cise. La deuxi\u00e8me est un besoin de coh\u00e9rence : si nous dupliquons une m\u00eame information \u00e0 plusieurs endroits dans la base, nous devrons par la suite faire attention, parfois par des m\u00e9canismes compliqu\u00e9s et co\u00fbteux, \u00e0 ce que cette information soit mise \u00e0 jour instantan\u00e9ment sur la totalit\u00e9 de ses copies. Ce besoin d'\u00e9viter la redondance a cr\u00e9\u00e9 la n\u00e9cessit\u00e9 d'utiliser plusieurs techniques, telles que les jointures et clefs \u00e9trang\u00e8res, et entra\u00eene parfois la cr\u00e9ation d'un tr\u00e8s grand nombre de tables. Ceci rajoute une complexit\u00e9 pour le requ\u00eatage, et une lourdeur d'ex\u00e9cution des t\u00e2ches sur la base. Un syst\u00e8me Big Data qui, non seulement est caract\u00e9ris\u00e9 par un gros volume de donn\u00e9es, mais \u00e9galement une grande v\u00e9locit\u00e9, et qui doit donc \u00eatre imm\u00e9diatement disponible, ne peut pas se permettre de gaspiller ses ressources en requ\u00eates inutiles. On tol\u00e8re donc \u00e0 un certain point les risques dus \u00e0 la redondance, pour gagner en disponibilit\u00e9, primordiale dans ce type de syst\u00e8mes. D'autre part, un syst\u00e8me Big Data est un syst\u00e8me r\u00e9parti par excellence, et dans un syst\u00e8me r\u00e9parti, il est primordial d'assurer une bonne tol\u00e9rance aux fautes en cr\u00e9ant des r\u00e9pliques des donn\u00e9es, diss\u00e9min\u00e9es partout sur le cluster. Ces r\u00e9pliques assurent qu'aucune machine n'est compl\u00e8tement indispensable, et diminue le risque d'indisponibilit\u00e9 des donn\u00e9es. Un autre signe de redondance. MOTTO 6 : Vive le Polyglottisme! \u00catre polyglotte, c'est \u00eatre capable de parler plusieurs langues. Et les syst\u00e8mes Big Data encouragent le polyglottisme. En effet, ce sont des syst\u00e8mes complexes qui impliquent en g\u00e9n\u00e9ral plusieurs traitements et plusieurs types de donn\u00e9es diff\u00e9rentes (donn\u00e9es brutes, donn\u00e9es nettoy\u00e9es, donn\u00e9es trait\u00e9es), ce qui fait qu'il existe deux principes importants \u00e0 encourager : Polyglot Programming : Une application peut comporter plusieurs langages et paradigmes de programmation, chacun assurant un besoin particulier, de fa\u00e7on \u00e0 profiter des avantages de chacun \u00e0 sa juste valeur. Polyglot Persistence : Dans une m\u00eame application, il est possible d'utiliser plusieurs syst\u00e8mes de stockage diff\u00e9rents (relationnels, NOSQL, syst\u00e8mes de fichiers, etc.). Gr\u00e2ce \u00e0 ces deux principes, on pourra cr\u00e9er des applications complexes mais compl\u00e8tes, qui permettent d'assurer tous les besoins en terme de stockage et de traitement.","title":"Principes de base du Domaine des Big Data"},{"location":"p1-big-data/#technologies-et-paradigmes","text":"Les op\u00e9rations \u00e0 r\u00e9aliser sur les syst\u00e8mes Big Data consistent principalement en : Ingestion des donn\u00e9es : repr\u00e9sente les phases de collecte et d'importation des donn\u00e9es pour \u00eatre stock\u00e9es ou trait\u00e9es \u00e0 la vol\u00e9e. Cela peut se faire en \"temps r\u00e9el\", c'est \u00e0 dire que les donn\u00e9es sont import\u00e9es au moment o\u00f9 elles sont \u00e9mises par leur source, ou bien \"par lots\", ce qui veut dire que les donn\u00e9es sont import\u00e9es par portions \u00e0 intervalles r\u00e9gulier. Exemples de technologies Apache Kafka , Amazon Kinesis , Apache Flume , Sqoop , etc. Stockage des donn\u00e9es : Les syst\u00e8mes de stockage de donn\u00e9es respectant les propri\u00e9t\u00e9s le Big Data se distinguent principalement en syst\u00e8mes de fichiers distribu\u00e9s, tel que Hadoop HDFS ou Google GFS , ou bases de donn\u00e9es NOSQL, tel que MongoDB , Cassandra , Redis ou Neo4J . Traitement des donn\u00e9es : Plusieurs types de traitement de donn\u00e9es sont possibles, nous citons : Traitement par lot (Batch Processing) : c'est le traitement des donn\u00e9es au repos (data at rest) qui se fait sur l'ensemble des donn\u00e9es stock\u00e9es, sans avoir besoin d'une interaction avec l'utilisateur. Le traitement par lot est adapt\u00e9 principalement aux op\u00e9rations ayant lieu \u00e0 la fin d'un cycle, permettant d'avoir une vision globale sur la totalit\u00e9 des donn\u00e9es, par exemple pour avoir un rapport global ou une analyse mensuelle. Les op\u00e9rations de traitement par lots sont en g\u00e9n\u00e9ral lanc\u00e9es \u00e0 des p\u00e9riodes r\u00e9guli\u00e8res, car elles sont connues pour avoir une grande latence (temps total de traitement). Exemples de technologies Hadoop Map Reduce et Spark Batch . Traitement en Streaming (Stream Processing) : c'est le traitement des donn\u00e9es en transit (data in motion) , ou en d'autres termes, le traitement des donn\u00e9es pendant qu'elles sont produites ou re\u00e7ues. Les donn\u00e9es \u00e9tant en g\u00e9n\u00e9ral cr\u00e9\u00e9es en tant que flux continu (\u00e9v\u00e8nements de capteurs, activit\u00e9 des utilisateurs sur un site web, flux vid\u00e9o, etc.), elles sont captur\u00e9es comme une s\u00e9rie d'\u00e9v\u00e8nements continus dans le temps. Avant la cr\u00e9ation des traitements en streaming, ces donn\u00e9es \u00e9taient stock\u00e9es dans une base de donn\u00e9es, un syst\u00e8me de fichier ou tout autre forme de stockage en masse. Les applications appelleront ensuite les donn\u00e9es au besoin. Gr\u00e2ce \u00e0 ce nouveau paradigme, les donn\u00e9es peuvent maintenant \u00eatre trait\u00e9es \u00e0 la vol\u00e9e, ce qui permet \u00e0 la couche applicative d'\u00eatre toujours sur \u00e9coute et \u00e0 jour. Exemples de technologies Apache Flink et Apache Storm . Traitement par Micro-Lot (Micro-Batch Processing) : c'est la pratique de collecter les donn\u00e9es en petits groupes (appel\u00e9s des micro-lots ou des micro-batchs ) pour les traiter. Contrairement au traditionnel traitement par lot, cette variante fait en sorte que le traitement des donn\u00e9es soit plus fr\u00e9quent, et que les r\u00e9sultats soient produits avec une latence beaucoup plus petite. Les donn\u00e9es sont collect\u00e9es par intervalles selon un seuil pr\u00e9d\u00e9fini, limit\u00e9 par un temps (par exemple toutes les secondes), ou par un nombre (tous les 20 \u00e9l\u00e9ments). Ce traitement est en g\u00e9n\u00e9ral une alternative au traitement en streaming, o\u00f9 les donn\u00e9es sont trait\u00e9es \u00e0 la vol\u00e9e, mais risquent d'\u00eatre perdues si le temps de traitement est sup\u00e9rieur \u00e0 la fr\u00e9quence de g\u00e9n\u00e9ration des donn\u00e9es. Le micro-batching permet, par contraste, de sauvegarder les donn\u00e9es dans un buffer, ralentissant ainsi le flux g\u00e9n\u00e9r\u00e9. D'autre part, les donn\u00e9es \u00e9tant trait\u00e9es par micro-lots, il est possible d'avoir une visibilit\u00e9 sur ce petit lot de donn\u00e9es, contrairement au traitement en streaming qui n'a de visibilit\u00e9 que sur la derni\u00e8re donn\u00e9e g\u00e9n\u00e9r\u00e9e, \u00e0 moins de proc\u00e9der \u00e0 des m\u00e9canismes parfois co\u00fbteux. En contrepartie, le traitement en micro-batch donne des r\u00e9sultats moins r\u00e9cents que le \"vrai\" streaming, et s'ex\u00e9cute sous forme de bursts r\u00e9guliers, qui peuvent parfois \u00eatre g\u00eanants pour le syst\u00e8me sous-jacent. Exemples de technologies Spark Streaming et Logstash . Traitement Interactif (Interactive Processing) : Dans les syst\u00e8mes Big Data, la notion de transaction n'est plus exactement la m\u00eame que pour les syst\u00e8mes classiques: finies les sacro-saintes propri\u00e9t\u00e9s ACID dont le premier objectif est d'avoir des donn\u00e9es correctes et coh\u00e9rentes, et bonjour les propri\u00e9t\u00e9s BASE, qui favorisent un acc\u00e8s moins rigide aux donn\u00e9es. On parle donc rarement de traitement transactionnel en Big Data, mais de traitements plut\u00f4t interactifs : une requ\u00eate est envoy\u00e9e par le client, trait\u00e9e imm\u00e9diatement par le syst\u00e8me qui renverra un r\u00e9sultat dans un temps raisonnable. On parle alors d' interaction entre l'utilisateur et le syst\u00e8me. Les traitements en batch et en streaming ne sont pas cens\u00e9s communiquer avec un utilisateur de l'autre c\u00f4t\u00e9. En g\u00e9n\u00e9ral, les r\u00e9sultats de ces traitements sont enregistr\u00e9s dans un syst\u00e8me de stockage, qui sera, lui, par la suite interrog\u00e9 par l'utilisateur. Le traitement interactif est donc le r\u00e9sultat d'une requ\u00eate de l'utilisateur, faite en g\u00e9n\u00e9ral sur une base de donn\u00e9es (relationnelle ou NOSQL). Exemples de technologies Apache Drill , Cloudera Impala ou Apache Zeppelin .","title":"Technologies et Paradigmes"},{"location":"p2-spark/","text":"Partie 2 - Introduction \u00e0 Apache Spark Apache Spark - Pr\u00e9sentation Apache Spark est une plateforme de traitement sur cluster g\u00e9n\u00e9rique. C'est un moteur de traitement libre, assurant un traitement parall\u00e8le et distribu\u00e9 sur des donn\u00e9es massives. Il fournit une API de d\u00e9veloppement pour permettre un traitement en streaming, l'apprentissage automatique ou la gestion de requ\u00eates SQL et demandant des acc\u00e8s r\u00e9p\u00e9t\u00e9s sur un grand volume de donn\u00e9es. 1 Apache Spark permet de r\u00e9aliser des traitements par lot ( batch processing ) ou \u00e0 la vol\u00e9e ( stream processing ) et est con\u00e7u de fa\u00e7on \u00e0 pouvoir int\u00e9grer tous les outils et technologies Big Data. Par exemple, non seulement Spark peut-il acc\u00e9der aux sources de donn\u00e9es de Hadoop, il peut \u00e9galement tourner sur un cluster Hadoop. \u00c9tant donn\u00e9 que Spark n'offre pas de solution de stockage (pas encore en tout cas), il est logique qu'il puisse profiter de la puissance de HDFS (le syst\u00e8me de fichiers de Hadoop), tout en offrant lui des performances in\u00e9gal\u00e9es pour le traitement en batch, ainsi que d'autres facilit\u00e9s (non offertes par Hadoop Map Reduce) telles que le traitement it\u00e9ratif, interactif et \u00e0 la vol\u00e9e. Spark offre des APIs de haut niveau en Java, Scala, Python et R. Il utilise le traitement en m\u00e9moire ( in-memory processing ), en exploitant les ressources combin\u00e9es du cluster comme si c'\u00e9tait une machine unique. Apache Spark a \u00e9t\u00e9 cr\u00e9\u00e9 en 2009 au laboratoire UC Berkeley R&D Lab (appel\u00e9 maintenant AMPLab), et est devenu open-source en 2010 avec une licence BSD. En 2013, il a int\u00e9gr\u00e9 Apache Software Foundation, pour devenir, en 2014, un projet Apache de haut niveau. Apache Spark - Composants Apache Spark utilise une architecture en couches, comportant plusieurs composants, dont l'objectif est de permettre de r\u00e9aliser des traitements performants tout en promettant un d\u00e9veloppement et une int\u00e9gration facilit\u00e9es. Il est n\u00e9 \u00e0 la base pour pallier les probl\u00e8mes pos\u00e9s par Hadoop Map Reduce, mais est devenu une entit\u00e9 \u00e0 lui seul, offrant bien plus que le traitement par lot classique. 1 Voici les composants de Spark: Spark Core Spark Core est le point central de Spark, qui fournit une plateforme d'ex\u00e9cution pour toutes les applications Spark. De plus, il supporte un large \u00e9ventail d'applications. Spark SQL Spark SQL se situe au dessus de Spark, pour permettre aux utilisateurs d'utiliser des requ\u00eates SQL/HQL. Les donn\u00e9es structur\u00e9es et semi-structur\u00e9es peuvent ainsi \u00eatre trait\u00e9es gr\u00e2ce \u00e0 Spark SQL, avec une performance am\u00e9lior\u00e9e. Spark Streaming Spark Streaming permet de cr\u00e9er des applications d'analyse de donn\u00e9es interactives. Les flux de donn\u00e9es sont transform\u00e9s en micro-lots et trait\u00e9s par dessus Spark Core. Spark MLlib La biblioth\u00e8que de machine learning MLlib fournit des algorithmes de haute qualit\u00e9 pour l'apprentissage automatique. Ce sont des libraries riches, tr\u00e8s utiles pour les data scientists, autorisant de plus des traitements en m\u00e9moire am\u00e9liorant de fa\u00e7on drastique la performance de ces algorithmes sur des donn\u00e9es massives. Spark GraphX Spark Graphx est le moteur d'ex\u00e9cution permettant un traitement scalable utilisant les graphes, se basant sur Spark Core. Architecture de Spark Les applications Spark s'ex\u00e9cutent comme un ensemble ind\u00e9pendant de processus sur un cluster, coordonn\u00e9s par un objet SparkContext dans le programme principal, appel\u00e9 driver program . 2 Pour s'ex\u00e9cuter sur un cluster, SparkContext peut se connecter \u00e0 plusieurs types de gestionnaires de clusters ( Cluster Managers ): Sur le gestionnaire autonome de Spark , qui est inclus dans Spark, et qui pr\u00e9sente le moyen le plus rapide et simple de mettre en place un cluster. Sur Apache Mesos , un gestionnaire de cluster g\u00e9n\u00e9ral qui peut aussi tourner sur Hadoop Map Reduce. Sur Hadoop YARN , le gestionnaire de ressources de Hadoop 2. Sur Kubernetes , un syst\u00e8me open-source pour l'automatisation du d\u00e9ploiement et la gestion des applications conteneuris\u00e9es. Ces gestionnaires permettent d'allouer les ressources n\u00e9cessaires pour l'ex\u00e9cution de plusieurs applications Spark. Une fois connect\u00e9, Spark lance des ex\u00e9cuteurs sur les noeuds du clluster, qui sont des processus qui lancent des traitements et stockent des donn\u00e9es pour les applications. Il envoie ensuite le code de l'application (dans un fichier JAR ou Python) aux ex\u00e9cuteurs . Spark Context envoie finalement les t\u00e2ches \u00e0 ex\u00e9cuter aux ex\u00e9cuteurs . Il est \u00e0 noter que: Chaque application a son lot d'ex\u00e9cuteurs, qui restent actifs tout au long de l'ex\u00e9cution de l'application, et qui lancent des t\u00e2ches sur plusieurs threads. Ainsi, les applications sont isol\u00e9es les unes des autres, du point de vue de l'orchestration (chaque driver ex\u00e9cute ses propres t\u00e2ches), et des ex\u00e9cuteurs (les t\u00e2ches des diff\u00e9rentes applications tournent sur des JVM diff\u00e9rentes). Ceci implique \u00e9galement que les applications (ou Jobs) Sparks ne peuvent pas \u00e9changer des donn\u00e9es, sans les enregistrer sur un support de stockage externe. Spark est ind\u00e9pendant du gestionnaire de cluster sous-jacent. Il suffit de configurer Spark pour utiliser ce gestionnaire, il peut g\u00e9rer ses ressources en m\u00eame temps que d'autres applications, m\u00eame non-Spark. L'application principale ( driver ) doit \u00eatre \u00e0 l'\u00e9coute des connexions entrantes venant de ses ex\u00e9cuteurs. Caract\u00e9ristiques de Spark Spark est connu pour avoir plusieurs caract\u00e9ristiques qui en font l'une des plateformes les plus utilis\u00e9es dans le domaine des Big Data. Nous citons: 1 Performance de traitement : Il est possible de r\u00e9aliser une vitesse de traitement tr\u00e8s \u00e9lev\u00e9e avec Spark sur des fichiers volumineux qui peut \u00eatre jusqu'\u00e0 100x meilleur que Hadoop Map Reduce, par exemple, et ceci gr\u00e2ce \u00e0 des m\u00e9canismes tel que la r\u00e9duction du nombre de lectures \u00e9critures sur le disque, la valorisation du traitement en m\u00e9moire et l'utilisation des m\u00e9moires cache et RAM pour les donn\u00e9es interm\u00e9diaires. Dynamicit\u00e9 : Il est facile de d\u00e9velopper des applications parall\u00e8les, gr\u00e2ce aux op\u00e9rateurs haut niveau fournis par Spark (allant jusqu'\u00e0 80 op\u00e9rateurs). Tol\u00e9rance aux Fautes : Apache Spark fournit un m\u00e9canisme de tol\u00e9rance aux fautes gr\u00e2ce aux RDD. Ces structures en m\u00e9moire sont con\u00e7ues pour r\u00e9cup\u00e9rer les donn\u00e9es en cas de panne. Traitements \u00e0 la vol\u00e9e : L'un des avantages de Spark par rapport \u00e0 Hadoop Map Reduce, c'est qu'il permet de traiter les donn\u00e9es \u00e0 la vol\u00e9e, pas uniquement en batch. \u00c9valuations Paresseuses ( Lazy Evaluations ) : Toutes les transformations faites sur Spark RDD sont paresseuses de nature, ce qui veut dire qu'elles ne donnent pas de r\u00e9sultat direct apr\u00e8s leur ex\u00e9cution, mais g\u00e9n\u00e8rent un nouvel RDD \u00e0 partir de l'ancien. On n'ex\u00e9cute effectivement les transformations qu'au moment de lancer une action sur les donn\u00e9es. Nous allons d\u00e9tailler cet aspect plus tard dans le cours. Support de plusieurs langages : Plusieurs langages de programmation sont support\u00e9s par Spark, tel que Java, R, Scala et Python. Une communaut\u00e9 active et en expansion : Des d\u00e9veloppeurs de plus de 50 entreprises sont impliqu\u00e9s dans le d\u00e9veloppement et l'am\u00e9lioration de Spark. Ce projet a \u00e9t\u00e9 initi\u00e9 en 2009 et est encore en expansion. Support d'analyses sophistiqu\u00e9es : Spark est fourni avec un ensemble d'outils d\u00e9di\u00e9s pour le streaming, les requ\u00eates interactives, le machine learning, etc. Int\u00e9gration avec Hadoop : Spark peut s'ex\u00e9cuter ind\u00e9pendamment ou sur Hadoop YARN, et profiter ainsi de la puissance du syst\u00e8me de fichiers distribu\u00e9 Hadoop HDFS. Limitations de Spark Spark a plusieurs limitations, tel que : 1 Pas de support pour le traitement en temps r\u00e9el : Spark permet le traitement en temps-presque-r\u00e9el, car il utilise le traitement en micro-lot plut\u00f4t que le traitement en streaming. Probl\u00e8mes avec les fichiers de petite taille : Spark partitionne le traitement sur plusieurs ex\u00e9cuteurs, et est optimis\u00e9 principalement pour les grands volumes de donn\u00e9es. L'utiliser pour des fichiers de petite taille va rajouter un co\u00fbt suppl\u00e9mentaire, il est donc plus judicieux dans ce cas d'utiliser un traitement s\u00e9quentiel classique sur une seule machine. Pas de syst\u00e8me de gestion des fichiers : Spark est principalement un syst\u00e8me de traitement, et ne fournit pas de solution pour le stockage des donn\u00e9es. Il doit donc se baser sur d'autres syst\u00e8mes de stockage tel que Hadoop HDFS ou Amazon S3. Co\u00fbteux : En tant que syst\u00e8me de traitement en m\u00e9moire, le co\u00fbt d'ex\u00e9cuter Spark sur un cluster peut \u00eatre tr\u00e8s \u00e9lev\u00e9 en terme de consommation m\u00e9moire. Nombre d'algorithmes limit\u00e9 : Malgr\u00e9 la disponibilit\u00e9 de la biblioth\u00e8que MLlib, elle reste limit\u00e9e en termes de nombre d'algorithmes impl\u00e9ment\u00e9s. Latence : La latence de Spark pour l'ex\u00e9cution de Jobs \u00e0 la vol\u00e9e est plus \u00e9lev\u00e9e que d'autres solutions de traitement en streaming tel que Flink . R\u00e9f\u00e9rences Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 02/2020 \u21a9 \u21a9 \u21a9 \u21a9 Spark Documentation, Cluster Mode Overview , https://spark.apache.org/docs/latest/cluster-overview.html , consult\u00e9 le 02/2020 \u21a9","title":"P2 - Introduction \u00e0 Apache Spark"},{"location":"p2-spark/#partie-2-introduction-a-apache-spark","text":"","title":"Partie 2 - Introduction \u00e0 Apache Spark"},{"location":"p2-spark/#apache-spark-presentation","text":"Apache Spark est une plateforme de traitement sur cluster g\u00e9n\u00e9rique. C'est un moteur de traitement libre, assurant un traitement parall\u00e8le et distribu\u00e9 sur des donn\u00e9es massives. Il fournit une API de d\u00e9veloppement pour permettre un traitement en streaming, l'apprentissage automatique ou la gestion de requ\u00eates SQL et demandant des acc\u00e8s r\u00e9p\u00e9t\u00e9s sur un grand volume de donn\u00e9es. 1 Apache Spark permet de r\u00e9aliser des traitements par lot ( batch processing ) ou \u00e0 la vol\u00e9e ( stream processing ) et est con\u00e7u de fa\u00e7on \u00e0 pouvoir int\u00e9grer tous les outils et technologies Big Data. Par exemple, non seulement Spark peut-il acc\u00e9der aux sources de donn\u00e9es de Hadoop, il peut \u00e9galement tourner sur un cluster Hadoop. \u00c9tant donn\u00e9 que Spark n'offre pas de solution de stockage (pas encore en tout cas), il est logique qu'il puisse profiter de la puissance de HDFS (le syst\u00e8me de fichiers de Hadoop), tout en offrant lui des performances in\u00e9gal\u00e9es pour le traitement en batch, ainsi que d'autres facilit\u00e9s (non offertes par Hadoop Map Reduce) telles que le traitement it\u00e9ratif, interactif et \u00e0 la vol\u00e9e. Spark offre des APIs de haut niveau en Java, Scala, Python et R. Il utilise le traitement en m\u00e9moire ( in-memory processing ), en exploitant les ressources combin\u00e9es du cluster comme si c'\u00e9tait une machine unique. Apache Spark a \u00e9t\u00e9 cr\u00e9\u00e9 en 2009 au laboratoire UC Berkeley R&D Lab (appel\u00e9 maintenant AMPLab), et est devenu open-source en 2010 avec une licence BSD. En 2013, il a int\u00e9gr\u00e9 Apache Software Foundation, pour devenir, en 2014, un projet Apache de haut niveau.","title":"Apache Spark - Pr\u00e9sentation"},{"location":"p2-spark/#apache-spark-composants","text":"Apache Spark utilise une architecture en couches, comportant plusieurs composants, dont l'objectif est de permettre de r\u00e9aliser des traitements performants tout en promettant un d\u00e9veloppement et une int\u00e9gration facilit\u00e9es. Il est n\u00e9 \u00e0 la base pour pallier les probl\u00e8mes pos\u00e9s par Hadoop Map Reduce, mais est devenu une entit\u00e9 \u00e0 lui seul, offrant bien plus que le traitement par lot classique. 1 Voici les composants de Spark: Spark Core Spark Core est le point central de Spark, qui fournit une plateforme d'ex\u00e9cution pour toutes les applications Spark. De plus, il supporte un large \u00e9ventail d'applications. Spark SQL Spark SQL se situe au dessus de Spark, pour permettre aux utilisateurs d'utiliser des requ\u00eates SQL/HQL. Les donn\u00e9es structur\u00e9es et semi-structur\u00e9es peuvent ainsi \u00eatre trait\u00e9es gr\u00e2ce \u00e0 Spark SQL, avec une performance am\u00e9lior\u00e9e. Spark Streaming Spark Streaming permet de cr\u00e9er des applications d'analyse de donn\u00e9es interactives. Les flux de donn\u00e9es sont transform\u00e9s en micro-lots et trait\u00e9s par dessus Spark Core. Spark MLlib La biblioth\u00e8que de machine learning MLlib fournit des algorithmes de haute qualit\u00e9 pour l'apprentissage automatique. Ce sont des libraries riches, tr\u00e8s utiles pour les data scientists, autorisant de plus des traitements en m\u00e9moire am\u00e9liorant de fa\u00e7on drastique la performance de ces algorithmes sur des donn\u00e9es massives. Spark GraphX Spark Graphx est le moteur d'ex\u00e9cution permettant un traitement scalable utilisant les graphes, se basant sur Spark Core.","title":"Apache Spark - Composants"},{"location":"p2-spark/#architecture-de-spark","text":"Les applications Spark s'ex\u00e9cutent comme un ensemble ind\u00e9pendant de processus sur un cluster, coordonn\u00e9s par un objet SparkContext dans le programme principal, appel\u00e9 driver program . 2 Pour s'ex\u00e9cuter sur un cluster, SparkContext peut se connecter \u00e0 plusieurs types de gestionnaires de clusters ( Cluster Managers ): Sur le gestionnaire autonome de Spark , qui est inclus dans Spark, et qui pr\u00e9sente le moyen le plus rapide et simple de mettre en place un cluster. Sur Apache Mesos , un gestionnaire de cluster g\u00e9n\u00e9ral qui peut aussi tourner sur Hadoop Map Reduce. Sur Hadoop YARN , le gestionnaire de ressources de Hadoop 2. Sur Kubernetes , un syst\u00e8me open-source pour l'automatisation du d\u00e9ploiement et la gestion des applications conteneuris\u00e9es. Ces gestionnaires permettent d'allouer les ressources n\u00e9cessaires pour l'ex\u00e9cution de plusieurs applications Spark. Une fois connect\u00e9, Spark lance des ex\u00e9cuteurs sur les noeuds du clluster, qui sont des processus qui lancent des traitements et stockent des donn\u00e9es pour les applications. Il envoie ensuite le code de l'application (dans un fichier JAR ou Python) aux ex\u00e9cuteurs . Spark Context envoie finalement les t\u00e2ches \u00e0 ex\u00e9cuter aux ex\u00e9cuteurs . Il est \u00e0 noter que: Chaque application a son lot d'ex\u00e9cuteurs, qui restent actifs tout au long de l'ex\u00e9cution de l'application, et qui lancent des t\u00e2ches sur plusieurs threads. Ainsi, les applications sont isol\u00e9es les unes des autres, du point de vue de l'orchestration (chaque driver ex\u00e9cute ses propres t\u00e2ches), et des ex\u00e9cuteurs (les t\u00e2ches des diff\u00e9rentes applications tournent sur des JVM diff\u00e9rentes). Ceci implique \u00e9galement que les applications (ou Jobs) Sparks ne peuvent pas \u00e9changer des donn\u00e9es, sans les enregistrer sur un support de stockage externe. Spark est ind\u00e9pendant du gestionnaire de cluster sous-jacent. Il suffit de configurer Spark pour utiliser ce gestionnaire, il peut g\u00e9rer ses ressources en m\u00eame temps que d'autres applications, m\u00eame non-Spark. L'application principale ( driver ) doit \u00eatre \u00e0 l'\u00e9coute des connexions entrantes venant de ses ex\u00e9cuteurs.","title":"Architecture de Spark"},{"location":"p2-spark/#caracteristiques-de-spark","text":"Spark est connu pour avoir plusieurs caract\u00e9ristiques qui en font l'une des plateformes les plus utilis\u00e9es dans le domaine des Big Data. Nous citons: 1 Performance de traitement : Il est possible de r\u00e9aliser une vitesse de traitement tr\u00e8s \u00e9lev\u00e9e avec Spark sur des fichiers volumineux qui peut \u00eatre jusqu'\u00e0 100x meilleur que Hadoop Map Reduce, par exemple, et ceci gr\u00e2ce \u00e0 des m\u00e9canismes tel que la r\u00e9duction du nombre de lectures \u00e9critures sur le disque, la valorisation du traitement en m\u00e9moire et l'utilisation des m\u00e9moires cache et RAM pour les donn\u00e9es interm\u00e9diaires. Dynamicit\u00e9 : Il est facile de d\u00e9velopper des applications parall\u00e8les, gr\u00e2ce aux op\u00e9rateurs haut niveau fournis par Spark (allant jusqu'\u00e0 80 op\u00e9rateurs). Tol\u00e9rance aux Fautes : Apache Spark fournit un m\u00e9canisme de tol\u00e9rance aux fautes gr\u00e2ce aux RDD. Ces structures en m\u00e9moire sont con\u00e7ues pour r\u00e9cup\u00e9rer les donn\u00e9es en cas de panne. Traitements \u00e0 la vol\u00e9e : L'un des avantages de Spark par rapport \u00e0 Hadoop Map Reduce, c'est qu'il permet de traiter les donn\u00e9es \u00e0 la vol\u00e9e, pas uniquement en batch. \u00c9valuations Paresseuses ( Lazy Evaluations ) : Toutes les transformations faites sur Spark RDD sont paresseuses de nature, ce qui veut dire qu'elles ne donnent pas de r\u00e9sultat direct apr\u00e8s leur ex\u00e9cution, mais g\u00e9n\u00e8rent un nouvel RDD \u00e0 partir de l'ancien. On n'ex\u00e9cute effectivement les transformations qu'au moment de lancer une action sur les donn\u00e9es. Nous allons d\u00e9tailler cet aspect plus tard dans le cours. Support de plusieurs langages : Plusieurs langages de programmation sont support\u00e9s par Spark, tel que Java, R, Scala et Python. Une communaut\u00e9 active et en expansion : Des d\u00e9veloppeurs de plus de 50 entreprises sont impliqu\u00e9s dans le d\u00e9veloppement et l'am\u00e9lioration de Spark. Ce projet a \u00e9t\u00e9 initi\u00e9 en 2009 et est encore en expansion. Support d'analyses sophistiqu\u00e9es : Spark est fourni avec un ensemble d'outils d\u00e9di\u00e9s pour le streaming, les requ\u00eates interactives, le machine learning, etc. Int\u00e9gration avec Hadoop : Spark peut s'ex\u00e9cuter ind\u00e9pendamment ou sur Hadoop YARN, et profiter ainsi de la puissance du syst\u00e8me de fichiers distribu\u00e9 Hadoop HDFS.","title":"Caract\u00e9ristiques de Spark"},{"location":"p2-spark/#limitations-de-spark","text":"Spark a plusieurs limitations, tel que : 1 Pas de support pour le traitement en temps r\u00e9el : Spark permet le traitement en temps-presque-r\u00e9el, car il utilise le traitement en micro-lot plut\u00f4t que le traitement en streaming. Probl\u00e8mes avec les fichiers de petite taille : Spark partitionne le traitement sur plusieurs ex\u00e9cuteurs, et est optimis\u00e9 principalement pour les grands volumes de donn\u00e9es. L'utiliser pour des fichiers de petite taille va rajouter un co\u00fbt suppl\u00e9mentaire, il est donc plus judicieux dans ce cas d'utiliser un traitement s\u00e9quentiel classique sur une seule machine. Pas de syst\u00e8me de gestion des fichiers : Spark est principalement un syst\u00e8me de traitement, et ne fournit pas de solution pour le stockage des donn\u00e9es. Il doit donc se baser sur d'autres syst\u00e8mes de stockage tel que Hadoop HDFS ou Amazon S3. Co\u00fbteux : En tant que syst\u00e8me de traitement en m\u00e9moire, le co\u00fbt d'ex\u00e9cuter Spark sur un cluster peut \u00eatre tr\u00e8s \u00e9lev\u00e9 en terme de consommation m\u00e9moire. Nombre d'algorithmes limit\u00e9 : Malgr\u00e9 la disponibilit\u00e9 de la biblioth\u00e8que MLlib, elle reste limit\u00e9e en termes de nombre d'algorithmes impl\u00e9ment\u00e9s. Latence : La latence de Spark pour l'ex\u00e9cution de Jobs \u00e0 la vol\u00e9e est plus \u00e9lev\u00e9e que d'autres solutions de traitement en streaming tel que Flink .","title":"Limitations de Spark"},{"location":"p2-spark/#references","text":"Data Flair, Spark Tutorial: Learn Spark Programming , https://data-flair.training/blogs/spark-tutorial/ , consult\u00e9 le 02/2020 \u21a9 \u21a9 \u21a9 \u21a9 Spark Documentation, Cluster Mode Overview , https://spark.apache.org/docs/latest/cluster-overview.html , consult\u00e9 le 02/2020 \u21a9","title":"R\u00e9f\u00e9rences"},{"location":"p3-install/","text":"Partie 3 - Installation de Spark Installation de Spark sur un seul Noeud Pour installer Spark, nous allons utiliser des contenaires Docker . Docker nous permettra de mettre en place un environnement complet, enti\u00e8rement portable, sans rien installer sur la machine h\u00f4te, pour utiliser Spark de fa\u00e7on uniforme gr\u00e2ce aux lignes de commande. Nous allons suivre les \u00e9tapes suivantes pour installer l'environnement Spark sur une machine ubuntu. \u00c9tape 1 - T\u00e9l\u00e9charger l'image de base Avant de suivre les \u00e9tapes suivantes, il faut commencer par installer Docker. Suivre les \u00e9tapes se trouvant dans le lien suivant, suivant votre syst\u00e8me d'exploitation: https://docs.docker.com/install/ Nous avons choisi Ubuntu comme environnement cible pour notre contenaire Docker. Nous commen\u00e7ons donc par t\u00e9l\u00e9charger l'image Ubuntu \u00e0 partir de Docker Hub, avec la commande suivante: 1 docker pull ubuntu Nous allons ensuite cr\u00e9er un contenaire \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. 1 docker run - itd - p 8080 : 8080 -- name spark -- hostname spark ubuntu Nous avons lanc\u00e9 un nouveau contenaire intitul\u00e9 spark \u00e0 partir de la machine ubuntu, en exposant sur le localhost son port 8080, pour pouvoir acc\u00e9der \u00e0 sa WebURL. On pourra v\u00e9rifier que la machine est bien d\u00e9marr\u00e9e en utilisant: 1 docker ps On devrait obtenir un r\u00e9sultat semblable au suivant: Pour se connecter \u00e0 la machine et la manipuler avec les lignes de commandes, utiliser: 1 docker exec - it spark bash Le r\u00e9sultat sera comme suit: Attention Ces \u00e9tapes sont faites une seule fois, \u00e0 la premi\u00e8re cr\u00e9ation de la machine. Si vous voulez relancer une machine d\u00e9j\u00e0 cr\u00e9\u00e9e, suivre les \u00e9tapes suivantes: V\u00e9rifier que la machine n'est pas d\u00e9j\u00e0 d\u00e9marr\u00e9e. Pour cela, taper la commande suivante: 1 docker ps Si vous retrouvez le contenaire dans la liste affich\u00e9e, vous pouvez ex\u00e9cuter la commande docker exec ... pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Sinon, v\u00e9rifier que le contenaire existe bien, mais qu'il est juste stopp\u00e9, gr\u00e2ce \u00e0 la commande: 1 docker ps - a Une fois le contenaire retrouv\u00e9, le d\u00e9marrer, simplement en tapant la commande suivante: 1 docker start spark Le contenaire sera lanc\u00e9. \u00c9tape 2 - Installer Java Afin d'installer Java sur la machine, commencer par mettre \u00e0 jour les packages syst\u00e8mes de Ubuntu: 1 2 apt update apt - y upgrade Installer ensuite la version par d\u00e9faut de Java: 1 apt install default - jdk V\u00e9rifier la version de Java que vous venez d'installer: 1 java - version \u00c9tape 3 - Installer Scala Installer Scala : 1 apt install scala \u00c9tape 4 - T\u00e9l\u00e9charger Spark Pour installer Spark sur la machine docker, utiliser la commande suivante: 1 2 apt install curl curl - O https : //archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz La version stable actuelle est 2.4.5, mais vous pouvez t\u00e9l\u00e9charger la version de votre choix. Vous retrouverez les liens de t\u00e9l\u00e9chargement de toutes les versions ICI . Extraire ensuite le fichier tgz: 1 tar xvf spark - 2.4.5 - bin - hadoop2 .7 . tgz D\u00e9placer le dossier obtenu vers le r\u00e9pertoire /opt comme suit: 1 2 mv spark - 2.4.5 - bin - hadoop2 .7 / opt / spark rm spark - 2.4.5 - bin - hadoop2 .7 . tgz \u00c9tape 5 - Mise en place de l'environnement Spark Nous devons mettre en place certains param\u00e8tres d'environnement pour assurer une bonne ex\u00e9cution de Spark: Ouvrir le fichier de configuration bashrc (installer vim si n\u00e9cessaire avec apt install vim ) 1 vim ~/.bashrc 2. Ajouter les lignes suivantes \u00e0 la fin du fichier (taper G pour aller \u00e0 la fin du fichier, puis o pour ins\u00e9rer une nouvelle ligne et passer en mode \u00e9dition) 1 2 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin Quitter l'\u00e9diteur en tapant : wq Activer les changements r\u00e9alis\u00e9s en tapant ` source ~/ . bashrc \u00c9tape 6 - D\u00e9marrer un serveur master en standalone Il est d\u00e9sormais possible de d\u00e9marrer un serveur en standalone, en utilisant la commande suivante: 1 start-master.sh Vous pourrez ensuite v\u00e9rifier que votre serveur est bien d\u00e9marr\u00e9 en tapant: jps Il suffit de plus, d'aller sur le navigateur de votre machine h\u00f4te, et d'ouvrir le lien: http: //localhost:8080 (apr\u00e8s avoir v\u00e9rifi\u00e9 que rien d'autre ne tourne sur le m\u00eame port). L'interface Web de Spark s'affichera, comme suit: On remarque que la fen\u00eatre indique que le spark master se trouve sur spark: //spark:7077 \u00c9tape 7 - D\u00e9marrer un processus Worker Pour lancer un processus Worker, utiliser la commande suivante: 1 start-slave.sh spark://spark:7077 Un nouveau processus sera lanc\u00e9, qu'on pourra voir avec jps Vous pouvez maintenant lancer Spark Shell pour executer des Jobs Spark. 1 spark-shell Installation de Spark sur un cluster Nous allons maintenant proc\u00e9der \u00e0 l'installation de Spark sur un cluster, c'est \u00e0 dire un ensemble de machines interconnect\u00e9es, repr\u00e9sent\u00e9es dans notre cas par des contenaires Docker. L'objectif sera donc de cr\u00e9er un r\u00e9seau de contenaires, installer Spark dessus, et lancer les processus sur les diff\u00e9rents contenaires, de fa\u00e7on \u00e0 obtenir le cluster suivant: Pour r\u00e9aliser cela, nous allons nous baser sur le contenaire cr\u00e9\u00e9 pr\u00e9c\u00e9demment, dans lequel nous avons install\u00e9 Java et Spark. \u00c9tape 1 - Installer SSH Installer OpenSSH sur la machine : 1 apt install openssh-server openssh-client G\u00e9n\u00e9rer une paire de clefs (quand on vous le demande, valider le chemin par d\u00e9faut propos\u00e9 pour enregistrer la paire de clefs): 1 ssh-keygen -t rsa -P \"\" D\u00e9finir la clef g\u00e9n\u00e9r\u00e9e comme clef autoris\u00e9e: 1 cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys Programmer ssh pour qu'il soit lanc\u00e9 au d\u00e9marrage du contenaire. Pour cela, ajouter les lignes suivantes \u00e0 la fin du fichier ~/ . bashrc : 1 service ssh start \u00c9tape 2 - Configurer Spark Il faudrait \u00e9diter le fichier de configuration spark - env . sh (se trouvant dans le r\u00e9pertoire $ SPARK_HOME / conf ) pour ajouter les param\u00e8tres suivants: Cr\u00e9er une copie du template du fichier spark - env . sh et le renommer: 1 cp $SPARK_HOME /conf/spark-env.sh.template $SPARK_HOME /conf/spark-env.sh Ajouter les deux lignes suivantes \u00e0 la fin du fichier ~/ . bashrc (n'oubliez pas de le recharger apr\u00e8s modification avec source ~/ . bashrc ) 1 export SPARK_WORKER_CORES = 8 Cr\u00e9er le fichier de configuration slaves dans le r\u00e9pertoire $ SPARK_HOME / conf : 1 vim $SPARK_HOME /conf/slaves Ajouter dans le fichier slaves les noms des contenaires workers (que nous allons cr\u00e9er tout \u00e0 l'heure): 1 2 spark-slave1 spark-slave2 Vous avez configur\u00e9 Spark pour supporter deux esclaves ( workers ou slaves ) en plus du master. \u00c9tape 3 - Cr\u00e9er une image \u00e0 partir du contenaire Une fois le contenaire cr\u00e9\u00e9 et configur\u00e9 tel que pr\u00e9sent\u00e9 pr\u00e9c\u00e9demment, nous allons le dupliquer pour en cr\u00e9er un cluster. Mais d'abord, il faut cr\u00e9er une image du contenaire, de fa\u00e7on \u00e0 l'utiliser pour cr\u00e9er les deux autre contenaires. Commencer par quitter le noeud spark et retourner vers la machine h\u00f4te, en tapant exit . Taper la commande suivante pour cr\u00e9er une image \u00e0 partir du contenaire spark: 1 docker commit spark spark-image commit permet de cr\u00e9er une nouvelle image spark - image \u00e0 partir du contenaire spark . V\u00e9rifier que spark - image existe bien en tapant: docker images . \u00c9tape 4 - Cr\u00e9er le Cluster Pour cr\u00e9er le cluster \u00e0 partir de l'image d\u00e9j\u00e0 g\u00e9n\u00e9r\u00e9e, suivre les \u00e9tapes suivantes: Supprimer le contenaire spark pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 2 docker stop spark docker rm spark Cr\u00e9er un r\u00e9seau qui permettra de connecter les trois noeuds du cluster: 1 docker network create --driver = bridge spark-network Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): 1 2 3 4 5 6 7 8 9 10 11 docker run -itd --net = spark-network -p 8080:8080 --expose 22 \\ --name spark-master --hostname spark-master \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave1 --hostname spark-slave1 \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave2 --hostname spark-slave2 \\ spark-image V\u00e9rifier que les trois contenaires sont bien cr\u00e9\u00e9s: 1 docker ps Vous devriez retrouver la liste des trois contenaires: \u00c9tape 5 - D\u00e9marrer les services Spark Pour d\u00e9marrer les services spark sur tous les noeuds, utiliser la commande suivante: 1 start-all.sh Vous obtiendrez le r\u00e9sultat suivant: Pour v\u00e9rifier que les services sont bien d\u00e9marr\u00e9s, aller sur le noeud Master et taper la commande jps , vous trouverez le r\u00e9sultat suivant: Si on fait la m\u00eame chose sur un des slaves, on obtiendra le r\u00e9sultat suivant:","title":"P3 - Installation de Spark"},{"location":"p3-install/#partie-3-installation-de-spark","text":"","title":"Partie 3 - Installation de Spark"},{"location":"p3-install/#installation-de-spark-sur-un-seul-noeud","text":"Pour installer Spark, nous allons utiliser des contenaires Docker . Docker nous permettra de mettre en place un environnement complet, enti\u00e8rement portable, sans rien installer sur la machine h\u00f4te, pour utiliser Spark de fa\u00e7on uniforme gr\u00e2ce aux lignes de commande. Nous allons suivre les \u00e9tapes suivantes pour installer l'environnement Spark sur une machine ubuntu.","title":"Installation de Spark sur un seul Noeud"},{"location":"p3-install/#etape-1-telecharger-limage-de-base","text":"Avant de suivre les \u00e9tapes suivantes, il faut commencer par installer Docker. Suivre les \u00e9tapes se trouvant dans le lien suivant, suivant votre syst\u00e8me d'exploitation: https://docs.docker.com/install/ Nous avons choisi Ubuntu comme environnement cible pour notre contenaire Docker. Nous commen\u00e7ons donc par t\u00e9l\u00e9charger l'image Ubuntu \u00e0 partir de Docker Hub, avec la commande suivante: 1 docker pull ubuntu Nous allons ensuite cr\u00e9er un contenaire \u00e0 partir de l'image t\u00e9l\u00e9charg\u00e9e. 1 docker run - itd - p 8080 : 8080 -- name spark -- hostname spark ubuntu Nous avons lanc\u00e9 un nouveau contenaire intitul\u00e9 spark \u00e0 partir de la machine ubuntu, en exposant sur le localhost son port 8080, pour pouvoir acc\u00e9der \u00e0 sa WebURL. On pourra v\u00e9rifier que la machine est bien d\u00e9marr\u00e9e en utilisant: 1 docker ps On devrait obtenir un r\u00e9sultat semblable au suivant: Pour se connecter \u00e0 la machine et la manipuler avec les lignes de commandes, utiliser: 1 docker exec - it spark bash Le r\u00e9sultat sera comme suit: Attention Ces \u00e9tapes sont faites une seule fois, \u00e0 la premi\u00e8re cr\u00e9ation de la machine. Si vous voulez relancer une machine d\u00e9j\u00e0 cr\u00e9\u00e9e, suivre les \u00e9tapes suivantes: V\u00e9rifier que la machine n'est pas d\u00e9j\u00e0 d\u00e9marr\u00e9e. Pour cela, taper la commande suivante: 1 docker ps Si vous retrouvez le contenaire dans la liste affich\u00e9e, vous pouvez ex\u00e9cuter la commande docker exec ... pr\u00e9sent\u00e9e pr\u00e9c\u00e9demment. Sinon, v\u00e9rifier que le contenaire existe bien, mais qu'il est juste stopp\u00e9, gr\u00e2ce \u00e0 la commande: 1 docker ps - a Une fois le contenaire retrouv\u00e9, le d\u00e9marrer, simplement en tapant la commande suivante: 1 docker start spark Le contenaire sera lanc\u00e9.","title":"\u00c9tape 1 - T\u00e9l\u00e9charger l'image de base"},{"location":"p3-install/#etape-2-installer-java","text":"Afin d'installer Java sur la machine, commencer par mettre \u00e0 jour les packages syst\u00e8mes de Ubuntu: 1 2 apt update apt - y upgrade Installer ensuite la version par d\u00e9faut de Java: 1 apt install default - jdk V\u00e9rifier la version de Java que vous venez d'installer: 1 java - version","title":"\u00c9tape 2 - Installer Java"},{"location":"p3-install/#etape-3-installer-scala","text":"Installer Scala : 1 apt install scala","title":"\u00c9tape 3 - Installer Scala"},{"location":"p3-install/#etape-4-telecharger-spark","text":"Pour installer Spark sur la machine docker, utiliser la commande suivante: 1 2 apt install curl curl - O https : //archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz La version stable actuelle est 2.4.5, mais vous pouvez t\u00e9l\u00e9charger la version de votre choix. Vous retrouverez les liens de t\u00e9l\u00e9chargement de toutes les versions ICI . Extraire ensuite le fichier tgz: 1 tar xvf spark - 2.4.5 - bin - hadoop2 .7 . tgz D\u00e9placer le dossier obtenu vers le r\u00e9pertoire /opt comme suit: 1 2 mv spark - 2.4.5 - bin - hadoop2 .7 / opt / spark rm spark - 2.4.5 - bin - hadoop2 .7 . tgz","title":"\u00c9tape 4 - T\u00e9l\u00e9charger Spark"},{"location":"p3-install/#etape-5-mise-en-place-de-lenvironnement-spark","text":"Nous devons mettre en place certains param\u00e8tres d'environnement pour assurer une bonne ex\u00e9cution de Spark: Ouvrir le fichier de configuration bashrc (installer vim si n\u00e9cessaire avec apt install vim ) 1 vim ~/.bashrc 2. Ajouter les lignes suivantes \u00e0 la fin du fichier (taper G pour aller \u00e0 la fin du fichier, puis o pour ins\u00e9rer une nouvelle ligne et passer en mode \u00e9dition) 1 2 export SPARK_HOME = /opt/spark export PATH = $PATH : $SPARK_HOME /bin: $SPARK_HOME /sbin Quitter l'\u00e9diteur en tapant : wq Activer les changements r\u00e9alis\u00e9s en tapant ` source ~/ . bashrc","title":"\u00c9tape 5 - Mise en place de l'environnement Spark"},{"location":"p3-install/#etape-6-demarrer-un-serveur-master-en-standalone","text":"Il est d\u00e9sormais possible de d\u00e9marrer un serveur en standalone, en utilisant la commande suivante: 1 start-master.sh Vous pourrez ensuite v\u00e9rifier que votre serveur est bien d\u00e9marr\u00e9 en tapant: jps Il suffit de plus, d'aller sur le navigateur de votre machine h\u00f4te, et d'ouvrir le lien: http: //localhost:8080 (apr\u00e8s avoir v\u00e9rifi\u00e9 que rien d'autre ne tourne sur le m\u00eame port). L'interface Web de Spark s'affichera, comme suit: On remarque que la fen\u00eatre indique que le spark master se trouve sur spark: //spark:7077","title":"\u00c9tape 6 - D\u00e9marrer un serveur master en standalone"},{"location":"p3-install/#etape-7-demarrer-un-processus-worker","text":"Pour lancer un processus Worker, utiliser la commande suivante: 1 start-slave.sh spark://spark:7077 Un nouveau processus sera lanc\u00e9, qu'on pourra voir avec jps Vous pouvez maintenant lancer Spark Shell pour executer des Jobs Spark. 1 spark-shell","title":"\u00c9tape 7 - D\u00e9marrer un processus Worker"},{"location":"p3-install/#installation-de-spark-sur-un-cluster","text":"Nous allons maintenant proc\u00e9der \u00e0 l'installation de Spark sur un cluster, c'est \u00e0 dire un ensemble de machines interconnect\u00e9es, repr\u00e9sent\u00e9es dans notre cas par des contenaires Docker. L'objectif sera donc de cr\u00e9er un r\u00e9seau de contenaires, installer Spark dessus, et lancer les processus sur les diff\u00e9rents contenaires, de fa\u00e7on \u00e0 obtenir le cluster suivant: Pour r\u00e9aliser cela, nous allons nous baser sur le contenaire cr\u00e9\u00e9 pr\u00e9c\u00e9demment, dans lequel nous avons install\u00e9 Java et Spark.","title":"Installation de Spark sur un cluster"},{"location":"p3-install/#etape-1-installer-ssh","text":"Installer OpenSSH sur la machine : 1 apt install openssh-server openssh-client G\u00e9n\u00e9rer une paire de clefs (quand on vous le demande, valider le chemin par d\u00e9faut propos\u00e9 pour enregistrer la paire de clefs): 1 ssh-keygen -t rsa -P \"\" D\u00e9finir la clef g\u00e9n\u00e9r\u00e9e comme clef autoris\u00e9e: 1 cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys Programmer ssh pour qu'il soit lanc\u00e9 au d\u00e9marrage du contenaire. Pour cela, ajouter les lignes suivantes \u00e0 la fin du fichier ~/ . bashrc : 1 service ssh start","title":"\u00c9tape 1 - Installer SSH"},{"location":"p3-install/#etape-2-configurer-spark","text":"Il faudrait \u00e9diter le fichier de configuration spark - env . sh (se trouvant dans le r\u00e9pertoire $ SPARK_HOME / conf ) pour ajouter les param\u00e8tres suivants: Cr\u00e9er une copie du template du fichier spark - env . sh et le renommer: 1 cp $SPARK_HOME /conf/spark-env.sh.template $SPARK_HOME /conf/spark-env.sh Ajouter les deux lignes suivantes \u00e0 la fin du fichier ~/ . bashrc (n'oubliez pas de le recharger apr\u00e8s modification avec source ~/ . bashrc ) 1 export SPARK_WORKER_CORES = 8 Cr\u00e9er le fichier de configuration slaves dans le r\u00e9pertoire $ SPARK_HOME / conf : 1 vim $SPARK_HOME /conf/slaves Ajouter dans le fichier slaves les noms des contenaires workers (que nous allons cr\u00e9er tout \u00e0 l'heure): 1 2 spark-slave1 spark-slave2 Vous avez configur\u00e9 Spark pour supporter deux esclaves ( workers ou slaves ) en plus du master.","title":"\u00c9tape 2 - Configurer Spark"},{"location":"p3-install/#etape-3-creer-une-image-a-partir-du-contenaire","text":"Une fois le contenaire cr\u00e9\u00e9 et configur\u00e9 tel que pr\u00e9sent\u00e9 pr\u00e9c\u00e9demment, nous allons le dupliquer pour en cr\u00e9er un cluster. Mais d'abord, il faut cr\u00e9er une image du contenaire, de fa\u00e7on \u00e0 l'utiliser pour cr\u00e9er les deux autre contenaires. Commencer par quitter le noeud spark et retourner vers la machine h\u00f4te, en tapant exit . Taper la commande suivante pour cr\u00e9er une image \u00e0 partir du contenaire spark: 1 docker commit spark spark-image commit permet de cr\u00e9er une nouvelle image spark - image \u00e0 partir du contenaire spark . V\u00e9rifier que spark - image existe bien en tapant: docker images .","title":"\u00c9tape 3 - Cr\u00e9er une image \u00e0 partir du contenaire"},{"location":"p3-install/#etape-4-creer-le-cluster","text":"Pour cr\u00e9er le cluster \u00e0 partir de l'image d\u00e9j\u00e0 g\u00e9n\u00e9r\u00e9e, suivre les \u00e9tapes suivantes: Supprimer le contenaire spark pr\u00e9c\u00e9demment cr\u00e9\u00e9: 1 2 docker stop spark docker rm spark Cr\u00e9er un r\u00e9seau qui permettra de connecter les trois noeuds du cluster: 1 docker network create --driver = bridge spark-network Cr\u00e9er et lancer les trois contenaires (les instructions -p permettent de faire un mapping entre les ports de la machine h\u00f4te et ceux du contenaire): 1 2 3 4 5 6 7 8 9 10 11 docker run -itd --net = spark-network -p 8080:8080 --expose 22 \\ --name spark-master --hostname spark-master \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave1 --hostname spark-slave1 \\ spark-image docker run -itd --net = spark-network --expose 22 \\ --name spark-slave2 --hostname spark-slave2 \\ spark-image V\u00e9rifier que les trois contenaires sont bien cr\u00e9\u00e9s: 1 docker ps Vous devriez retrouver la liste des trois contenaires:","title":"\u00c9tape 4 - Cr\u00e9er le Cluster"},{"location":"p3-install/#etape-5-demarrer-les-services-spark","text":"Pour d\u00e9marrer les services spark sur tous les noeuds, utiliser la commande suivante: 1 start-all.sh Vous obtiendrez le r\u00e9sultat suivant: Pour v\u00e9rifier que les services sont bien d\u00e9marr\u00e9s, aller sur le noeud Master et taper la commande jps , vous trouverez le r\u00e9sultat suivant: Si on fait la m\u00eame chose sur un des slaves, on obtiendra le r\u00e9sultat suivant:","title":"\u00c9tape 5 - D\u00e9marrer les services Spark"},{"location":"tips/","text":"See mkdocs Cheat Sheet italique input gras 50070 image ou lien Apache Hadoop code 1 git clone https : //github.com/liliasfaxi/hadoop-cluster-docker inline code hadoop fs - mkdir - p / user / root Attention blablabla Erreur blablabla Activit\u00e9 Modifier tables |Instruction|Fonctionnalit\u00e9| |---------|-------------------------------------------------------------| | hadoop fs \u2013 ls | Afficher le contenu du re\u0301pertoire racine | | hadoop fs \u2013 put file . txt | Upload un fichier dans hadoop (a\u0300 partir du re\u0301pertoire courant linux) | | hadoop fs \u2013 get file . txt | Download un fichier a\u0300 partir de hadoop sur votre disque local | | hadoop fs \u2013 tail file . txt | Lire les dernie\u0300res lignes du fichier | | hadoop fs \u2013 cat file . txt | Affiche tout le contenu du fichier | | hadoop fs \u2013 mv file . txt newfile . txt | Renommer le fichier | | hadoop fs \u2013 rm newfile . txt | Supprimer le fichier | | hadoop fs \u2013 mkdir myinput | Cre\u0301er un re\u0301pertoire | | hadoop fs \u2013 cat file . txt \\ | less | Lire le fichier page par page| Footnotes 1 2 3 4 5 6 7 8 Footnotes [ ^ 1 ] are added in - text like so ... #end of the document [ ^ 1 ] : Footnotes are the mind killer . Footnotes are the little - death that brings total obliteration . I will face my footnotes . get the IP address of a docker container docker inspect | grep IPAddress","title":"Tips"}]}